#!/usr/bin/env python3

import argparse, cv2, numpy as np, pathlib, textwrap
import struct
import concurrent.futures
from sklearn.cluster import MiniBatchKMeans
from scipy.spatial.distance import cdist
from collections import defaultdict
import statistics
from numba import jit, njit, types
from numba.typed import List

from dither_opt import apply_dither_optimized

WIDTH, HEIGHT = 240, 160
DEFAULT_STRIP_COUNT = 4
DEFAULT_UNIFIED_CODEBOOK_SIZE = 256   # ç»Ÿä¸€ç æœ¬å¤§å°
EFFECTIVE_UNIFIED_CODEBOOK_SIZE = 254  # æœ‰æ•ˆç æœ¬å¤§å°ï¼ˆ0xFFä¿ç•™ï¼‰
DEFAULT_BIG_BLOCK_CODEBOOK_SIZE = 256  # 4x4å¤§å—ç è¡¨å¤§å°

# æ ‡è®°å¸¸é‡
# COLOR_BLOCK_MARKER = 0xFF
BIG_BLOCK_MARKER = 0xFE

Y_COEFF  = np.array([0.28571429,  0.57142857,  0.14285714])
CB_COEFF = np.array([-0.14285714, -0.28571429,  0.42857143])
CR_COEFF = np.array([ 0.35714286, -0.28571429, -0.07142857])
BLOCK_W, BLOCK_H = 2, 2
BYTES_PER_BLOCK  = 7  # 4Y + d_r + d_g + d_b
BYTES_PER_BIG_BLOCK = 28  # 16Y + 4*(d_r + d_g + d_b)

# æ–°å¢å¸¸é‡
ZONE_HEIGHT_PIXELS = 16  # æ¯ä¸ªåŒºåŸŸçš„åƒç´ é«˜åº¦
ZONE_HEIGHT_BIG_BLOCKS = ZONE_HEIGHT_PIXELS // (BLOCK_H * 2)  # æ¯ä¸ªåŒºåŸŸçš„4x4å¤§å—è¡Œæ•° (16åƒç´  = 4è¡Œ4x4å¤§å—)

# å¸§ç±»å‹æ ‡è¯†
FRAME_TYPE_I = 0x00  # Iå¸§ï¼ˆå…³é”®å¸§ï¼‰
FRAME_TYPE_P = 0x01  # På¸§ï¼ˆå·®åˆ†å¸§ï¼‰

def calculate_strip_heights(height: int, strip_count: int) -> list:
    """è®¡ç®—æ¯ä¸ªæ¡å¸¦çš„é«˜åº¦ï¼Œç¡®ä¿æ¯ä¸ªæ¡å¸¦é«˜åº¦éƒ½æ˜¯4çš„å€æ•°"""
    if height % 4 != 0:
        raise ValueError(f"è§†é¢‘é«˜åº¦ {height} å¿…é¡»æ˜¯4çš„å€æ•°")
    
    base_height = (height // strip_count // 4) * 4
    remaining_height = height - (base_height * strip_count)
    
    strip_heights = []
    for i in range(strip_count):
        current_height = base_height
        if remaining_height >= 4:
            current_height += 4
            remaining_height -= 4
        strip_heights.append(current_height)
    
    if sum(strip_heights) != height:
        raise ValueError(f"æ¡å¸¦é«˜åº¦åˆ†é…é”™è¯¯: {strip_heights} æ€»å’Œ {sum(strip_heights)} != {height}")
    
    for i, h in enumerate(strip_heights):
        if h % 4 != 0:
            raise ValueError(f"æ¡å¸¦ {i} é«˜åº¦ {h} ä¸æ˜¯4çš„å€æ•°")
    
    return strip_heights

@njit
def clip_value(value, min_val, max_val):
    """Numbaå…¼å®¹çš„clipå‡½æ•°"""
    if value < min_val:
        return min_val
    elif value > max_val:
        return max_val
    else:
        return value

@njit
def pack_yuv420_strip_numba(bgr_strip, strip_height, width):
    """NumbaåŠ é€Ÿçš„YUV420è½¬æ¢"""
    blocks_h = strip_height // BLOCK_H
    blocks_w = width // BLOCK_W
    
    block_array = np.zeros((blocks_h, blocks_w, BYTES_PER_BLOCK), dtype=np.uint8)
    
    for by in range(blocks_h):
        for bx in range(blocks_w):
            # æå–2x2å—
            y_start = by * BLOCK_H
            x_start = bx * BLOCK_W
            
            # BGR to YUV conversion for 2x2 block
            cb_sum = 0.0
            cr_sum = 0.0
            y_values = np.zeros(4, dtype=np.uint8)
            
            idx = 0
            for dy in range(BLOCK_H):
                for dx in range(BLOCK_W):
                    if y_start + dy < strip_height and x_start + dx < width:
                        b = float(bgr_strip[y_start + dy, x_start + dx, 0])
                        g = float(bgr_strip[y_start + dy, x_start + dx, 1])  
                        r = float(bgr_strip[y_start + dy, x_start + dx, 2])
                        
                        y = r * 0.28571429 + g * 0.57142857 + b * 0.14285714
                        cb = r * (-0.14285714) + g * (-0.28571429) + b * 0.42857143
                        cr = r * 0.35714286 + g * (-0.28571429) + b * (-0.07142857)
                        
                        y_values[idx] = np.uint8(clip_value(y / 2.0, 0.0, 255.0))
                        cb_sum += cb
                        cr_sum += cr
                        idx += 1
            
            # Store Y values
            block_array[by, bx, 0:4] = y_values
            
            # Compute and store chroma
            cb_mean = cb_sum / 4.0
            cr_mean = cr_sum / 4.0
            
            d_r = clip_value(cr_mean, -128.0, 127.0)
            d_g = clip_value((-(cb_mean/2.0) - cr_mean) / 2.0, -128.0, 127.0)  
            d_b = clip_value(cb_mean, -128.0, 127.0)
            
            # å°†æœ‰ç¬¦å·å€¼è½¬æ¢ä¸ºæ— ç¬¦å·å­—èŠ‚å­˜å‚¨
            block_array[by, bx, 4] = np.uint8(np.int8(d_r).view(np.uint8))
            block_array[by, bx, 5] = np.uint8(np.int8(d_g).view(np.uint8))
            block_array[by, bx, 6] = np.uint8(np.int8(d_b).view(np.uint8))
    
    return block_array

def pack_yuv420_strip(frame_bgr: np.ndarray, strip_y: int, strip_height: int) -> np.ndarray:
    """ä½¿ç”¨NumbaåŠ é€Ÿçš„YUVè½¬æ¢åŒ…è£…å‡½æ•°"""
    strip_bgr = frame_bgr[strip_y:strip_y + strip_height, :, :]
    return pack_yuv420_strip_numba(strip_bgr, strip_height, WIDTH)

def generate_codebook(blocks_data: np.ndarray, codebook_size: int, max_iter: int = 100) -> tuple:
    """ä½¿ç”¨K-Meansèšç±»ç”Ÿæˆç è¡¨"""
    if len(blocks_data) == 0:
        return np.zeros((codebook_size, BYTES_PER_BLOCK), dtype=np.uint8), 0
    
    if blocks_data.ndim > 2:
        blocks_data = blocks_data.reshape(-1, BYTES_PER_BLOCK)
    
    # ç§»é™¤å»é‡æ“ä½œï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®è¿›è¡Œèšç±»
    # è¿™æ ·K-Meanså¯ä»¥åŸºäºæ•°æ®çš„çœŸå®åˆ†å¸ƒï¼ˆåŒ…æ‹¬é¢‘æ¬¡ï¼‰è¿›è¡Œæ›´å¥½çš„èšç±»
    effective_size = min(len(blocks_data), codebook_size)
    
    if len(blocks_data) <= codebook_size:
        # å¦‚æœæ•°æ®é‡å°äºç æœ¬å¤§å°ï¼Œéœ€è¦å»é‡é¿å…é‡å¤
        blocks_as_tuples = [tuple(block) for block in blocks_data]
        unique_tuples = list(set(blocks_as_tuples))
        unique_blocks = np.array(unique_tuples, dtype=np.uint8)
        
        codebook = np.zeros((codebook_size, BYTES_PER_BLOCK), dtype=np.uint8)
        codebook[:len(unique_blocks)] = unique_blocks
        if len(unique_blocks) > 0:
            for i in range(len(unique_blocks), codebook_size):
                codebook[i] = unique_blocks[-1]
        return codebook, len(unique_blocks)
    
    # å¯¹äºå¤§æ•°æ®é›†ï¼Œç›´æ¥è¿›è¡ŒK-Meansèšç±»
    kmeans = MiniBatchKMeans(
        n_clusters=codebook_size,
        random_state=42,
        batch_size=min(1000, len(blocks_data)),
        max_iter=max_iter,
        n_init=3
    )
    blocks_for_clustering = convert_blocks_for_clustering(blocks_data)
    kmeans.fit(blocks_for_clustering)
    codebook = convert_codebook_from_clustering(kmeans.cluster_centers_)
    
    return codebook, codebook_size

def generate_big_block_codebook(big_blocks: list, codebook_size: int = DEFAULT_BIG_BLOCK_CODEBOOK_SIZE, 
                               max_iter: int = 100) -> np.ndarray:
    """ç”Ÿæˆ4x4å¤§å—ç è¡¨"""
    if len(big_blocks) == 0:
        return np.zeros((codebook_size, BYTES_PER_BIG_BLOCK), dtype=np.uint8)
    
    big_blocks_array = np.array(big_blocks)
    if len(big_blocks_array) <= codebook_size:
        # æ•°æ®é‡å°äºç è¡¨å¤§å°
        codebook = np.zeros((codebook_size, BYTES_PER_BIG_BLOCK), dtype=np.uint8)
        codebook[:len(big_blocks_array)] = big_blocks_array
        if len(big_blocks_array) > 0:
            for i in range(len(big_blocks_array), codebook_size):
                codebook[i] = big_blocks_array[-1]
        return codebook
    
    # ä½¿ç”¨K-Meansèšç±»
    big_blocks_for_clustering = convert_big_blocks_for_clustering(big_blocks_array)
    kmeans = MiniBatchKMeans(
        n_clusters=codebook_size,
        random_state=42,
        batch_size=min(1000, len(big_blocks_array)),
        max_iter=max_iter,
        n_init=3
    )
    kmeans.fit(big_blocks_for_clustering)
    codebook = convert_big_block_codebook_from_clustering(kmeans.cluster_centers_)
    
    return codebook

def convert_blocks_for_clustering(blocks_data: np.ndarray) -> np.ndarray:
    """å°†å—æ•°æ®è½¬æ¢ä¸ºæ­£ç¡®çš„èšç±»æ ¼å¼"""
    if len(blocks_data) == 0:
        return blocks_data.astype(np.float32)
    
    if blocks_data.ndim > 2:
        blocks_data = blocks_data.reshape(-1, BYTES_PER_BLOCK)
    
    blocks_float = blocks_data.astype(np.float32)
    
    for i in range(4, BYTES_PER_BLOCK):
        blocks_float[:, i] = blocks_data[:, i].view(np.int8).astype(np.float32)
    
    return blocks_float

def convert_codebook_from_clustering(codebook_float: np.ndarray) -> np.ndarray:
    """å°†èšç±»ç»“æœè½¬æ¢å›æ­£ç¡®çš„å—æ ¼å¼"""
    codebook = np.zeros_like(codebook_float, dtype=np.uint8)
    
    codebook[:, 0:4] = np.clip(codebook_float[:, 0:4].round(), 0, 255).astype(np.uint8)
    
    for i in range(4, BYTES_PER_BLOCK):
        clipped_values = np.clip(codebook_float[:, i].round(), -128, 127).astype(np.int8)
        codebook[:, i] = clipped_values.view(np.uint8)
    
    return codebook

def convert_big_blocks_for_clustering(big_blocks: np.ndarray) -> np.ndarray:
    """å°†4x4å¤§å—è½¬æ¢ä¸ºèšç±»æ ¼å¼"""
    if len(big_blocks) == 0:
        return big_blocks.astype(np.float32)
    
    if big_blocks.ndim > 2:
        big_blocks = big_blocks.reshape(-1, BYTES_PER_BIG_BLOCK)
    
    big_blocks_float = big_blocks.astype(np.float32)
    
    # è‰²åº¦åˆ†é‡éœ€è¦è½¬æ¢ä¸ºæœ‰ç¬¦å·æ•°
    for i in range(16, BYTES_PER_BIG_BLOCK):
        big_blocks_float[:, i] = big_blocks[:, i].view(np.int8).astype(np.float32)
    
    return big_blocks_float

def convert_big_block_codebook_from_clustering(codebook_float: np.ndarray) -> np.ndarray:
    """å°†èšç±»ç»“æœè½¬æ¢å›4x4å¤§å—æ ¼å¼"""
    codebook = np.zeros_like(codebook_float, dtype=np.uint8)
    
    # Yåˆ†é‡
    codebook[:, 0:16] = np.clip(codebook_float[:, 0:16].round(), 0, 255).astype(np.uint8)
    
    # è‰²åº¦åˆ†é‡
    for i in range(16, BYTES_PER_BIG_BLOCK):
        clipped_values = np.clip(codebook_float[:, i].round(), -128, 127).astype(np.int8)
        codebook[:, i] = clipped_values.view(np.uint8)
    
    return codebook

def quantize_big_blocks(big_blocks: list, big_block_codebook: np.ndarray) -> tuple:
    """é‡åŒ–4x4å¤§å—ï¼Œè¿”å›ç´¢å¼•å’Œé‡å»ºçš„å—"""
    if len(big_blocks) == 0:
        return np.array([], dtype=np.uint8), []
    
    big_blocks_array = np.array(big_blocks)
    big_blocks_for_clustering = convert_big_blocks_for_clustering(big_blocks_array)
    codebook_for_clustering = convert_big_blocks_for_clustering(big_block_codebook)
    
    # è®¡ç®—è·ç¦»å’Œæ‰¾åˆ°æœ€è¿‘çš„ç å­—
    distances = cdist(big_blocks_for_clustering, codebook_for_clustering, metric='euclidean')
    indices = np.argmin(distances, axis=1).astype(np.uint8)
    
    # é‡å»ºå—
    reconstructed_big_blocks = [big_block_codebook[idx] for idx in indices]
    
    return indices, reconstructed_big_blocks

def calculate_distortion_sad(original_blocks: list, reconstructed_blocks: list) -> float:
    """è®¡ç®—å¤±çœŸåº¦é‡ - SAD (Sum of Absolute Differences)"""
    if len(original_blocks) != len(reconstructed_blocks):
        return float('inf')
    
    total_sad = 0.0
    for orig, recon in zip(original_blocks, reconstructed_blocks):
        # åªè®¡ç®—Yåˆ†é‡çš„SAD
        y_orig = orig[:4].astype(np.float32)
        y_recon = recon[:4].astype(np.float32)
        total_sad += np.sum(np.abs(y_orig - y_recon))
    
    return total_sad / len(original_blocks)  # å¹³å‡SAD

def calculate_distortion_mse(original_blocks: list, reconstructed_blocks: list) -> float:
    """è®¡ç®—å¤±çœŸåº¦é‡ - MSE (Mean Squared Error)"""
    if len(original_blocks) != len(reconstructed_blocks):
        return float('inf')
    
    total_mse = 0.0
    for orig, recon in zip(original_blocks, reconstructed_blocks):
        # åªè®¡ç®—Yåˆ†é‡çš„MSE
        y_orig = orig[:4].astype(np.float32)
        y_recon = recon[:4].astype(np.float32)
        total_mse += np.sum((y_orig - y_recon) ** 2)
    
    return total_mse / (len(original_blocks) * 4)  # å¹³å‡MSE

# é»˜è®¤ä½¿ç”¨SAD
calculate_distortion = calculate_distortion_sad

def classify_4x4_blocks_with_big_codebook(blocks: np.ndarray, big_block_codebook: np.ndarray,
                                        variance_threshold: float = 5.0, 
                                        distortion_threshold: float = 10.0) -> tuple:
    """ä½¿ç”¨4x4å¤§å—ç è¡¨å¯¹4x4å—è¿›è¡Œåˆ†ç±»"""
    blocks_h, blocks_w = blocks.shape[:2]
    big_blocks_h = blocks_h // 2
    big_blocks_w = blocks_w // 2
    
    big_block_indices = {}  # ä½¿ç”¨4x4å¤§å—ç è¡¨çš„å—
    small_blocks = []       # éœ€è¦ç”¨2x2å°å—ç è¡¨çš„å—
    block_types = {}        # è®°å½•æ¯ä¸ª4x4å—çš„ç±»å‹
    
    for big_by in range(big_blocks_h):
        for big_bx in range(big_blocks_w):
            # æ”¶é›†4x4å¤§å—å†…çš„4ä¸ª2x2å°å—
            blocks_4x4 = []
            for sub_by in range(2):
                for sub_bx in range(2):
                    by = big_by * 2 + sub_by
                    bx = big_bx * 2 + sub_bx
                    if by < blocks_h and bx < blocks_w:
                        blocks_4x4.append(blocks[by, bx])
                    else:
                        blocks_4x4.append(np.zeros(BYTES_PER_BLOCK, dtype=np.uint8))
            
            # å°è¯•ç”¨4x4å¤§å—ç è¡¨
            big_block = pack_big_block_from_2x2_blocks(blocks_4x4)
            indices, reconstructed = quantize_big_blocks([big_block], big_block_codebook)
            
            if len(reconstructed) > 0:
                # è®¡ç®—å¤±çœŸ
                reconstructed_2x2_blocks = unpack_big_block_to_2x2_blocks(reconstructed[0])
                distortion = calculate_distortion(blocks_4x4, reconstructed_2x2_blocks)
                
                if distortion <= distortion_threshold:
                    # å¤±çœŸå¯æ¥å—ï¼Œä½¿ç”¨4x4å¤§å—ç è¡¨
                    big_block_indices[(big_by, big_bx)] = indices[0]
                    block_types[(big_by, big_bx)] = 'big_block'
                else:
                    # å¤±çœŸå¤ªå¤§ï¼Œä½¿ç”¨2x2å°å—ç è¡¨
                    small_blocks.extend(blocks_4x4)
                    block_types[(big_by, big_bx)] = 'small_blocks'
            else:
                # é‡åŒ–å¤±è´¥ï¼Œä½¿ç”¨2x2å°å—ç è¡¨
                small_blocks.extend(blocks_4x4)
                block_types[(big_by, big_bx)] = 'small_blocks'
    
    return big_block_indices, small_blocks, block_types

def encode_strip_i_frame_with_big_blocks(blocks: np.ndarray, big_block_codebook: np.ndarray,
                                       small_block_codebook: np.ndarray, block_types: dict,
                                       big_block_indices: dict) -> bytes:
    """ç¼–ç Iå¸§æ¡å¸¦ï¼ˆåˆ é™¤è‰²å—æ”¯æŒï¼‰"""
    data = bytearray()
    data.append(FRAME_TYPE_I)
    
    if blocks.size > 0:
        blocks_h, blocks_w = blocks.shape[:2]
        big_blocks_h = blocks_h // 2
        big_blocks_w = blocks_w // 2
        
        # å­˜å‚¨4x4å¤§å—ç è¡¨
        data.extend(big_block_codebook.flatten().tobytes())
        
        # å­˜å‚¨2x2å°å—ç è¡¨
        data.extend(small_block_codebook.flatten().tobytes())
        
        # æŒ‰4x4å¤§å—çš„é¡ºåºç¼–ç 
        for big_by in range(big_blocks_h):
            for big_bx in range(big_blocks_w):
                if (big_by, big_bx) in block_types:
                    block_type = block_types[(big_by, big_bx)]
                    
                    if block_type == 'big_block':
                        # 4x4å¤§å—ï¼š0xFE + 1ä¸ªå¤§å—ç è¡¨ç´¢å¼•
                        data.append(BIG_BLOCK_MARKER)
                        big_idx = big_block_indices[(big_by, big_bx)]
                        data.append(big_idx)
                        
                    else:  # small_blocks
                        # çº¹ç†å—ï¼š4ä¸ªå°å—ç è¡¨ç´¢å¼•
                        for sub_by in range(2):
                            for sub_bx in range(2):
                                by = big_by * 2 + sub_by
                                bx = big_bx * 2 + sub_bx
                                if by < blocks_h and bx < blocks_w:
                                    block = blocks[by, bx]
                                    small_idx = quantize_blocks_unified(block.reshape(1, -1), small_block_codebook)[0]
                                    data.append(small_idx)
                                else:
                                    data.append(0)
    
    return bytes(data)

def generate_gop_codebooks_with_big_blocks(frames: list, strip_count: int, i_frame_interval: int,
                                         variance_threshold: float, diff_threshold: float,
                                         distortion_threshold: float = 10.0,
                                         big_block_codebook_size: int = DEFAULT_BIG_BLOCK_CODEBOOK_SIZE,
                                         small_block_codebook_size: int = EFFECTIVE_UNIFIED_CODEBOOK_SIZE,
                                         kmeans_max_iter: int = 100, i_frame_weight: int = 3) -> dict:
    """ä¸ºæ¯ä¸ªGOPç”Ÿæˆ4x4å¤§å—ç è¡¨å’Œ2x2å°å—ç è¡¨ï¼ˆåˆ é™¤è‰²å—æ”¯æŒï¼‰"""
    print("æ­£åœ¨ä¸ºæ¯ä¸ªGOPç”Ÿæˆå¤§å—ç è¡¨å’Œå°å—ç è¡¨...")
    
    gop_codebooks = {}
    
    i_frame_positions = []
    for frame_idx in range(len(frames)):
        if frame_idx % i_frame_interval == 0:
            i_frame_positions.append(frame_idx)
    
    for gop_idx, gop_start in enumerate(i_frame_positions):
        if gop_idx + 1 < len(i_frame_positions):
            gop_end = i_frame_positions[gop_idx + 1]
        else:
            gop_end = len(frames)
        
        print(f"  å¤„ç†GOP {gop_idx}: å¸§ {gop_start} åˆ° {gop_end-1}")
        
        gop_codebooks[gop_start] = []
        
        for strip_idx in range(strip_count):
            all_big_blocks = []
            all_small_blocks = []
            block_types_list = []
            
            # å¤„ç†GOPä¸­çš„æ¯ä¸€å¸§
            prev_strip_blocks = None
            
            for frame_idx in range(gop_start, gop_end):
                strip_blocks = frames[frame_idx][strip_idx]
                if strip_blocks.size == 0:
                    continue
                
                # ç¡®å®šéœ€è¦å¤„ç†çš„å¤§å—
                is_i_frame = (frame_idx == gop_start)
                
                if is_i_frame:
                    blocks_h, blocks_w = strip_blocks.shape[:2]
                    big_blocks_h = blocks_h // 2
                    big_blocks_w = blocks_w // 2
                    updated_big_blocks = {(big_by, big_bx) for big_by in range(big_blocks_h) for big_bx in range(big_blocks_w)}
                else:
                    updated_big_blocks = identify_updated_big_blocks(strip_blocks, prev_strip_blocks, diff_threshold)
                
                # ä»æœ‰æ•ˆå¤§å—ä¸­æå–æ•°æ®ç”¨äºè®­ç»ƒç è¡¨
                for big_by, big_bx in updated_big_blocks:
                    blocks_4x4 = []
                    for sub_by in range(2):
                        for sub_bx in range(2):
                            by = big_by * 2 + sub_by
                            bx = big_bx * 2 + sub_bx
                            if by < strip_blocks.shape[0] and bx < strip_blocks.shape[1]:
                                blocks_4x4.append(strip_blocks[by, bx])
                            else:
                                blocks_4x4.append(np.zeros(BYTES_PER_BLOCK, dtype=np.uint8))
                    
                    # æ·»åŠ åˆ°è®­ç»ƒæ•°æ®
                    big_block = pack_big_block_from_2x2_blocks(blocks_4x4)
                    if is_i_frame:
                        all_big_blocks.extend([big_block] * i_frame_weight)
                        all_small_blocks.extend(blocks_4x4 * i_frame_weight)
                    else:
                        all_big_blocks.append(big_block)
                        all_small_blocks.extend(blocks_4x4)
                
                prev_strip_blocks = strip_blocks.copy()
            
            # ç”Ÿæˆç è¡¨
            big_block_codebook = generate_big_block_codebook(all_big_blocks, big_block_codebook_size, kmeans_max_iter)
            small_block_codebook = generate_unified_codebook_simplified(
                all_small_blocks, small_block_codebook_size, kmeans_max_iter)
            
            # ä¸ºæ¯ä¸€å¸§ç”Ÿæˆåˆ†ç±»ä¿¡æ¯
            for frame_idx in range(gop_start, gop_end):
                strip_blocks = frames[frame_idx][strip_idx]
                if strip_blocks.size == 0:
                    continue
                
                big_block_indices, _, block_types = classify_4x4_blocks_with_big_codebook(
                    strip_blocks, big_block_codebook, variance_threshold, distortion_threshold)
                block_types_list.append((frame_idx, block_types, big_block_indices))
            
            gop_codebooks[gop_start].append({
                'big_block_codebook': big_block_codebook,
                'small_block_codebook': small_block_codebook,
                'block_types_list': block_types_list,
                'distortion_threshold': distortion_threshold
            })
            
            print(f"    æ¡å¸¦{strip_idx}: å¤§å—{len(all_big_blocks)}ä¸ª, å°å—{len(all_small_blocks)}ä¸ª")
    
    return gop_codebooks

def generate_unified_codebook_simplified(small_blocks: list, 
                                       codebook_size: int = EFFECTIVE_UNIFIED_CODEBOOK_SIZE,
                                       kmeans_max_iter: int = 100) -> np.ndarray:
    """ç”Ÿæˆ2x2å°å—çš„ç»Ÿä¸€ç è¡¨ï¼ˆ254é¡¹ï¼Œé¿å…0xFEï¼‰"""
    if small_blocks:
        blocks_array = np.array(small_blocks)
        codebook, _ = generate_codebook(blocks_array, codebook_size, kmeans_max_iter)
        
        # åˆ›å»º254é¡¹ç è¡¨
        full_codebook = np.zeros((codebook_size, BYTES_PER_BLOCK), dtype=np.uint8)
        actual_size = min(len(codebook), codebook_size)
        full_codebook[:actual_size] = codebook[:actual_size]
        
        # å¡«å……å‰©ä½™é¡¹
        if actual_size > 0:
            for i in range(actual_size, codebook_size):
                full_codebook[i] = full_codebook[actual_size - 1]
    else:
        full_codebook = np.zeros((codebook_size, BYTES_PER_BLOCK), dtype=np.uint8)
    
    return full_codebook

def encode_strip_p_frame_with_big_blocks(current_blocks: np.ndarray, prev_blocks: np.ndarray,
                                       big_block_codebook: np.ndarray, small_block_codebook: np.ndarray,
                                       block_types: dict, big_block_indices: dict,
                                       diff_threshold: float, force_i_threshold: float = 0.7,
                                       variance_threshold: float = 5.0, distortion_threshold: float = 10.0) -> tuple:
    """ç¼–ç På¸§æ¡å¸¦ï¼ˆåˆ é™¤è‰²å—æ”¯æŒï¼‰"""
    if prev_blocks is None or current_blocks.shape != prev_blocks.shape:
        i_frame_data = encode_strip_i_frame_with_big_blocks(
            current_blocks, big_block_codebook, small_block_codebook, block_types, big_block_indices)
        return i_frame_data, True, 0, 0, 0
    
    blocks_h, blocks_w = current_blocks.shape[:2]
    total_blocks = blocks_h * blocks_w
    
    if total_blocks == 0:
        return b'', True, 0, 0, 0
    
    # è¯†åˆ«éœ€è¦æ›´æ–°çš„å¤§å—
    updated_big_blocks = identify_updated_big_blocks(current_blocks, prev_blocks, diff_threshold)
    
    big_blocks_h = blocks_h // 2
    big_blocks_w = blocks_w // 2
    total_big_blocks = big_blocks_h * big_blocks_w
    
    # åˆ¤æ–­æ˜¯å¦éœ€è¦Iå¸§
    update_ratio = len(updated_big_blocks) / total_big_blocks if total_big_blocks > 0 else 0
    if update_ratio > force_i_threshold:
        i_frame_data = encode_strip_i_frame_with_big_blocks(
            current_blocks, big_block_codebook, small_block_codebook, block_types, big_block_indices)
        return i_frame_data, True, 0, 0, 0
    
    # è®¡ç®—åŒºåŸŸæ•°é‡
    zones_count = (big_blocks_h + ZONE_HEIGHT_BIG_BLOCKS - 1) // ZONE_HEIGHT_BIG_BLOCKS
    if zones_count > 8:
        zones_count = 8
    
    # æŒ‰åŒºåŸŸç»„ç»‡æ›´æ–°
    zone_detail_updates = [[] for _ in range(zones_count)]
    zone_big_block_updates = [[] for _ in range(zones_count)]
    
    for big_by, big_bx in updated_big_blocks:
        # è®¡ç®—å±äºå“ªä¸ªåŒºåŸŸ
        zone_idx = min(big_by // ZONE_HEIGHT_BIG_BLOCKS, zones_count - 1)
        zone_relative_by = big_by % ZONE_HEIGHT_BIG_BLOCKS
        zone_relative_idx = zone_relative_by * big_blocks_w + big_bx
        
        if (big_by, big_bx) in block_types:
            block_type = block_types[(big_by, big_bx)]
            
            if block_type == 'big_block':
                # 4x4å¤§å—æ›´æ–°
                big_idx = big_block_indices[(big_by, big_bx)]
                zone_big_block_updates[zone_idx].append((zone_relative_idx, big_idx))
                
            else:  # small_blocks
                # çº¹ç†å—æ›´æ–°
                indices = []
                for sub_by in range(2):
                    for sub_bx in range(2):
                        by = big_by * 2 + sub_by
                        bx = big_bx * 2 + sub_bx
                        if by < blocks_h and bx < blocks_w:
                            block = current_blocks[by, bx]
                            small_idx = quantize_blocks_unified(block.reshape(1, -1), small_block_codebook)[0]
                            indices.append(small_idx)
                        else:
                            indices.append(0)
                zone_detail_updates[zone_idx].append((zone_relative_idx, indices))
    
    # ç¼–ç På¸§
    data = bytearray()
    data.append(FRAME_TYPE_P)
    
    # ç»Ÿè®¡ä½¿ç”¨çš„åŒºåŸŸæ•°é‡
    used_zones = 0
    total_detail_updates = 0
    total_big_block_updates = 0
    
    # ç”ŸæˆåŒºåŸŸbitmap
    zone_bitmap = 0
    for zone_idx in range(zones_count):
        if zone_detail_updates[zone_idx] or zone_big_block_updates[zone_idx]:
            zone_bitmap |= (1 << zone_idx)
            used_zones += 1
            total_detail_updates += len(zone_detail_updates[zone_idx])
            total_big_block_updates += len(zone_big_block_updates[zone_idx])
    
    data.append(zone_bitmap)
    
    # æŒ‰åŒºåŸŸç¼–ç æ›´æ–°ï¼ˆç°åœ¨åªæœ‰2ç§ç±»å‹ï¼‰
    for zone_idx in range(zones_count):
        if zone_bitmap & (1 << zone_idx):
            detail_updates = zone_detail_updates[zone_idx]
            big_block_updates = zone_big_block_updates[zone_idx]
            
            data.append(len(detail_updates))
            data.append(len(big_block_updates))
            
            # å­˜å‚¨çº¹ç†å—æ›´æ–°
            for relative_idx, indices in detail_updates:
                data.append(relative_idx)
                for idx in indices:
                    data.append(idx)
            
            # å­˜å‚¨4x4å¤§å—æ›´æ–°
            for relative_idx, big_idx in big_block_updates:
                data.append(relative_idx)
                data.append(big_idx)
    
    total_updates = total_detail_updates + total_big_block_updates
    return bytes(data), False, used_zones, 0, total_detail_updates

def pack_big_block_from_2x2_blocks(blocks_2x2: list) -> np.ndarray:
    """å°†4ä¸ª2x2å—ç»„åˆæˆä¸€ä¸ª4x4å¤§å—"""
    big_block = np.zeros(BYTES_PER_BIG_BLOCK, dtype=np.uint8)
    
    # å­˜å‚¨16ä¸ªYå€¼å’Œ4ç»„è‰²åº¦ä¿¡æ¯
    y_offset = 0
    chroma_offset = 16
    
    for i, block in enumerate(blocks_2x2):
        # å¤åˆ¶4ä¸ªYå€¼
        big_block[y_offset:y_offset+4] = block[:4]
        y_offset += 4
        
        # å¤åˆ¶è‰²åº¦ä¿¡æ¯
        big_block[chroma_offset:chroma_offset+3] = block[4:7]
        chroma_offset += 3
    
    return big_block

def unpack_big_block_to_2x2_blocks(big_block: np.ndarray) -> list:
    """å°†4x4å¤§å—æ‹†åˆ†æˆ4ä¸ª2x2å—"""
    blocks_2x2 = []
    
    for i in range(4):
        block = np.zeros(BYTES_PER_BLOCK, dtype=np.uint8)
        # å¤åˆ¶Yå€¼
        y_start = i * 4
        block[:4] = big_block[y_start:y_start+4]
        # å¤åˆ¶è‰²åº¦ä¿¡æ¯
        chroma_start = 16 + i * 3
        block[4:7] = big_block[chroma_start:chroma_start+3]
        blocks_2x2.append(block)
    
    return blocks_2x2

def quantize_blocks_unified(blocks_data: np.ndarray, codebook: np.ndarray) -> np.ndarray:
    """ä½¿ç”¨ç»Ÿä¸€ç è¡¨å¯¹å—è¿›è¡Œé‡åŒ–ï¼ˆé¿å…äº§ç”Ÿ0xFEï¼‰"""
    if len(blocks_data) == 0:
        return np.array([], dtype=np.uint8)
    
    # åªä½¿ç”¨å‰254é¡¹è¿›è¡Œé‡åŒ–
    effective_codebook = codebook[:EFFECTIVE_UNIFIED_CODEBOOK_SIZE]
    
    blocks_for_clustering = convert_blocks_for_clustering(blocks_data)
    codebook_for_clustering = convert_blocks_for_clustering(effective_codebook)
    
    # ä½¿ç”¨NumbaåŠ é€Ÿçš„è·ç¦»è®¡ç®—
    indices = quantize_blocks_distance_numba(blocks_for_clustering, codebook_for_clustering)
    
    return indices

@njit
def quantize_blocks_distance_numba(blocks_for_clustering, codebook_for_clustering):
    """NumbaåŠ é€Ÿçš„å—é‡åŒ–è·ç¦»è®¡ç®—"""
    n_blocks = blocks_for_clustering.shape[0]
    n_codebook = codebook_for_clustering.shape[0]
    indices = np.zeros(n_blocks, dtype=np.uint8)
    
    for i in range(n_blocks):
        min_dist = np.inf
        best_idx = 0
        
        for j in range(n_codebook):
            dist = 0.0
            for k in range(BYTES_PER_BLOCK):
                diff = blocks_for_clustering[i, k] - codebook_for_clustering[j, k]
                dist += diff * diff
            
            if dist < min_dist:
                min_dist = dist
                best_idx = j
        
        indices[i] = best_idx
    
    return indices

def identify_updated_big_blocks(current_blocks: np.ndarray, prev_blocks: np.ndarray,
                               diff_threshold: float) -> set:
    """è¯†åˆ«éœ€è¦æ›´æ–°çš„4x4å¤§å—ä½ç½® - NumbaåŠ é€Ÿç‰ˆæœ¬"""
    if prev_blocks is None or current_blocks.shape != prev_blocks.shape:
        # å¦‚æœæ²¡æœ‰å‰ä¸€å¸§ï¼Œæ‰€æœ‰å¤§å—éƒ½éœ€è¦æ›´æ–°
        blocks_h, blocks_w = current_blocks.shape[:2]
        big_blocks_h = blocks_h // 2
        big_blocks_w = blocks_w // 2
        return {(big_by, big_bx) for big_by in range(big_blocks_h) for big_bx in range(big_blocks_w)}
    
    blocks_h, blocks_w = current_blocks.shape[:2]
    
    # ä½¿ç”¨NumbaåŠ é€Ÿçš„å—å·®å¼‚è®¡ç®—
    current_flat = current_blocks.reshape(-1, BYTES_PER_BLOCK)
    prev_flat = prev_blocks.reshape(-1, BYTES_PER_BLOCK)
    block_diffs = compute_block_differences_numba(current_flat, prev_flat, blocks_h, blocks_w)
    
    # ä½¿ç”¨NumbaåŠ é€Ÿçš„æ›´æ–°å—è¯†åˆ«
    updated_list = identify_updated_blocks_numba(block_diffs, diff_threshold, blocks_h, blocks_w)
    
    return set(updated_list)

@njit
def compute_block_differences_numba(current_flat, prev_flat, blocks_h, blocks_w):
    """NumbaåŠ é€Ÿçš„å—å·®å¼‚è®¡ç®—"""
    block_diffs = np.zeros((blocks_h, blocks_w), dtype=np.float64)
    
    for i in range(blocks_h * blocks_w):
        y_diff_sum = 0.0
        for j in range(4):  # åªè®¡ç®—Yåˆ†é‡å·®å¼‚
            current_val = int(current_flat[i, j])
            prev_val = int(prev_flat[i, j])
            if current_val >= prev_val:
                diff = current_val - prev_val
            else:
                diff = prev_val - current_val
            y_diff_sum += diff
        block_diffs[i // blocks_w, i % blocks_w] = y_diff_sum / 4.0
    
    return block_diffs

@njit
def identify_updated_blocks_numba(block_diffs, diff_threshold, blocks_h, blocks_w):
    """NumbaåŠ é€Ÿçš„æ›´æ–°å—è¯†åˆ«"""
    big_blocks_h = blocks_h // 2
    big_blocks_w = blocks_w // 2
    updated_positions = []
    
    for big_by in range(big_blocks_h):
        for big_bx in range(big_blocks_w):
            needs_update = False
            
            # æ£€æŸ¥4ä¸ª2x2å­å—çš„ä½ç½®
            for sub_by in range(2):
                for sub_bx in range(2):
                    by = big_by * 2 + sub_by
                    bx = big_bx * 2 + sub_bx
                    
                    if by < blocks_h and bx < blocks_w:
                        if block_diffs[by, bx] > diff_threshold:
                            needs_update = True
                            break
                if needs_update:
                    break
            
            if needs_update:
                updated_positions.append((big_by, big_bx))
    
    return updated_positions

class EncodingStats:
    """ç¼–ç ç»Ÿè®¡ç±» - ä¿®å¤ç»Ÿè®¡é—®é¢˜"""
    def __init__(self):
        # å¸§ç»Ÿè®¡
        self.total_frames_processed = 0
        self.total_i_frames = 0
        self.forced_i_frames = 0
        self.threshold_i_frames = 0
        self.total_p_frames = 0
        
        # å¤§å°ç»Ÿè®¡
        self.total_i_frame_bytes = 0
        self.total_p_frame_bytes = 0
        self.total_big_block_codebook_bytes = 0
        self.total_small_block_codebook_bytes = 0
        self.total_index_bytes = 0
        self.total_p_overhead_bytes = 0
        
        # å—ç±»å‹ç»Ÿè®¡ - ä¿®å¤
        self.big_block_count = 0
        self.small_block_count = 0
        
        # På¸§å—æ›´æ–°ç»Ÿè®¡ - æ–°å¢è¯¦ç»†ç»Ÿè®¡
        self.p_frame_updates = []
        self.zone_usage = defaultdict(int)
        self.detail_update_count = 0
        self.big_block_update_count = 0
        self.detail_update_bytes = 0  # çº¹ç†å—æ›´æ–°å­—èŠ‚æ•°
        self.big_block_update_bytes = 0  # å¤§å—æ›´æ–°å­—èŠ‚æ•°
        
        # æ¡å¸¦ç»Ÿè®¡
        self.strip_stats = defaultdict(lambda: {
            'i_frames': 0, 'p_frames': 0, 
            'i_bytes': 0, 'p_bytes': 0
        })
    
    def add_i_frame(self, strip_idx, size_bytes, is_forced=True, codebook_size=0, index_size=0):
        self.total_frames_processed += 1
        self.total_i_frames += 1
        if is_forced:
            self.forced_i_frames += 1
        else:
            self.threshold_i_frames += 1
        
        self.total_i_frame_bytes += size_bytes
        
        # ä¿®å¤ç æœ¬ç»Ÿè®¡ - åˆ†åˆ«è®¡ç®—å¤§å—å’Œå°å—ç æœ¬
        big_codebook_bytes = DEFAULT_BIG_BLOCK_CODEBOOK_SIZE * BYTES_PER_BIG_BLOCK
        small_codebook_bytes = EFFECTIVE_UNIFIED_CODEBOOK_SIZE * BYTES_PER_BLOCK
        self.total_big_block_codebook_bytes += big_codebook_bytes
        self.total_small_block_codebook_bytes += small_codebook_bytes
        
        # ç´¢å¼•å¤§å° = æ€»å¤§å° - å¸§ç±»å‹æ ‡è®° - ä¸¤ä¸ªç æœ¬å¤§å°
        actual_index_size = size_bytes - 1 - big_codebook_bytes - small_codebook_bytes
        self.total_index_bytes += max(0, actual_index_size)
        
        self.strip_stats[strip_idx]['i_frames'] += 1
        self.strip_stats[strip_idx]['i_bytes'] += size_bytes
    
    def add_p_frame(self, strip_idx, size_bytes, updates_count, zone_count, 
                   color_updates=0, detail_updates=0):
        self.total_frames_processed += 1
        self.total_p_frames += 1
        self.total_p_frame_bytes += size_bytes
        self.p_frame_updates.append(updates_count)
        self.zone_usage[zone_count] += 1
        
        # På¸§å¼€é”€ï¼šå¸§ç±»å‹(1) + bitmap(1) + æ¯ä¸ªåŒºåŸŸçš„è®¡æ•°(2*zones)
        overhead = 2 + zone_count * 2  # ç°åœ¨åªæœ‰2ç§å—ç±»å‹
        self.total_p_overhead_bytes += overhead
        
        # è¯¦ç»†æ›´æ–°ç»Ÿè®¡
        self.detail_update_count += detail_updates
        big_block_updates = updates_count - detail_updates
        self.big_block_update_count += big_block_updates
        
        # è®¡ç®—æ›´æ–°æ•°æ®å­—èŠ‚æ•°
        detail_bytes = detail_updates * 5  # 1å­—èŠ‚ä½ç½® + 4å­—èŠ‚ç´¢å¼•
        big_block_bytes = big_block_updates * 2  # 1å­—èŠ‚ä½ç½® + 1å­—èŠ‚ç´¢å¼•
        self.detail_update_bytes += detail_bytes
        self.big_block_update_bytes += big_block_bytes
        
        self.strip_stats[strip_idx]['p_frames'] += 1
        self.strip_stats[strip_idx]['p_bytes'] += size_bytes
    
    def add_block_type_stats(self, big_blocks, small_blocks):
        self.big_block_count += big_blocks
        self.small_block_count += small_blocks
    
    def print_summary(self, total_frames, total_bytes):
        print(f"\nğŸ“Š ç¼–ç ç»Ÿè®¡æŠ¥å‘Š")
        print(f"=" * 60)
        
        # è®¡ç®—æ¡å¸¦çº§åˆ«çš„ç»Ÿè®¡
        strip_count = len(self.strip_stats) if self.strip_stats else 1
        
        # åŸºæœ¬ç»Ÿè®¡
        print(f"ğŸ¬ å¸§ç»Ÿè®¡:")
        print(f"   è§†é¢‘å¸§æ•°: {total_frames}")
        print(f"   æ¡å¸¦æ€»æ•°: {strip_count}")
        print(f"   å¤„ç†çš„æ¡å¸¦å¸§: {self.total_frames_processed}")
        print(f"   Iå¸§æ¡å¸¦: {self.total_i_frames} ({self.total_i_frames/self.total_frames_processed*100:.1f}%)")
        print(f"     - å¼ºåˆ¶Iå¸§: {self.forced_i_frames}")
        print(f"     - è¶…é˜ˆå€¼Iå¸§: {self.threshold_i_frames}")
        print(f"   På¸§æ¡å¸¦: {self.total_p_frames} ({self.total_p_frames/self.total_frames_processed*100:.1f}%)")
        
        # å¤§å°ç»Ÿè®¡
        print(f"\nğŸ’¾ ç©ºé—´å ç”¨:")
        print(f"   æ€»å¤§å°: {total_bytes:,} bytes ({total_bytes/1024:.1f} KB)")
        print(f"   Iå¸§æ•°æ®: {self.total_i_frame_bytes:,} bytes ({self.total_i_frame_bytes/total_bytes*100:.1f}%)")
        print(f"   På¸§æ•°æ®: {self.total_p_frame_bytes:,} bytes ({self.total_p_frame_bytes/total_bytes*100:.1f}%)")
        
        if self.total_i_frames > 0:
            print(f"   å¹³å‡Iå¸§å¤§å°: {self.total_i_frame_bytes/self.total_i_frames:.1f} bytes")
        if self.total_p_frames > 0:
            print(f"   å¹³å‡På¸§å¤§å°: {self.total_p_frame_bytes/self.total_p_frames:.1f} bytes")
        
        # æ•°æ®æ„æˆç»Ÿè®¡
        print(f"\nğŸ¨ æ•°æ®æ„æˆ:")
        print(f"   å¤§å—ç æœ¬æ•°æ®: {self.total_big_block_codebook_bytes:,} bytes ({self.total_big_block_codebook_bytes/total_bytes*100:.1f}%)")
        print(f"   å°å—ç æœ¬æ•°æ®: {self.total_small_block_codebook_bytes:,} bytes ({self.total_small_block_codebook_bytes/total_bytes*100:.1f}%)")
        print(f"   Iå¸§ç´¢å¼•: {self.total_index_bytes:,} bytes ({self.total_index_bytes/total_bytes*100:.1f}%)")
        
        # På¸§æ•°æ®æ„æˆ
        p_frame_data_bytes = self.total_p_frame_bytes - self.total_p_overhead_bytes
        print(f"   På¸§æ›´æ–°æ•°æ®: {p_frame_data_bytes:,} bytes ({p_frame_data_bytes/total_bytes*100:.1f}%)")
        print(f"     - çº¹ç†å—æ›´æ–°: {self.detail_update_bytes:,} bytes ({self.detail_update_bytes/total_bytes*100:.1f}%)")
        print(f"     - å¤§å—æ›´æ–°: {self.big_block_update_bytes:,} bytes ({self.big_block_update_bytes/total_bytes*100:.1f}%)")
        print(f"   På¸§å¼€é”€: {self.total_p_overhead_bytes:,} bytes ({self.total_p_overhead_bytes/total_bytes*100:.1f}%)")
        
        # å—ç±»å‹ç»Ÿè®¡
        print(f"\nğŸ§© å—ç±»å‹åˆ†å¸ƒ:")
        total_block_types = self.big_block_count + self.small_block_count
        if total_block_types > 0:
            print(f"   4x4å¤§å—: {self.big_block_count} ä¸ª ({self.big_block_count/total_block_types*100:.1f}%)")
            print(f"   2x2çº¹ç†å—: {self.small_block_count} ä¸ª ({self.small_block_count/total_block_types*100:.1f}%)")
        
        # På¸§æ›´æ–°ç»Ÿè®¡
        if self.p_frame_updates:
            avg_updates = statistics.mean(self.p_frame_updates)
            median_updates = statistics.median(self.p_frame_updates)
            max_updates = max(self.p_frame_updates)
            min_updates = min(self.p_frame_updates)
            
            print(f"\nâš¡ På¸§æ›´æ–°åˆ†æ:")
            print(f"   å¹³å‡æ›´æ–°å—æ•°: {avg_updates:.1f}")
            print(f"   ä¸­ä½æ•°æ›´æ–°å—æ•°: {median_updates}")
            print(f"   æœ€å¤§æ›´æ–°å—æ•°: {max_updates}")
            print(f"   æœ€å°æ›´æ–°å—æ•°: {min_updates}")
            print(f"   çº¹ç†å—æ›´æ–°æ€»æ•°: {self.detail_update_count:,}")
            print(f"   å¤§å—æ›´æ–°æ€»æ•°: {self.big_block_update_count:,}")
        
        # åŒºåŸŸä½¿ç”¨ç»Ÿè®¡
        if self.zone_usage:
            print(f"\nğŸ—ºï¸  åŒºåŸŸä½¿ç”¨åˆ†å¸ƒ:")
            for zone_count in sorted(self.zone_usage.keys()):
                frames_count = self.zone_usage[zone_count]
                if self.total_p_frames > 0:
                    print(f"   {zone_count}ä¸ªåŒºåŸŸ: {frames_count}æ¬¡ ({frames_count/self.total_p_frames*100:.1f}%)")
        
        # æ¡å¸¦ç»Ÿè®¡
        print(f"\nğŸ“ æ¡å¸¦ç»Ÿè®¡:")
        for strip_idx in sorted(self.strip_stats.keys()):
            stats = self.strip_stats[strip_idx]
            total_strip_frames = stats['i_frames'] + stats['p_frames']
            total_strip_bytes = stats['i_bytes'] + stats['p_bytes']
            if total_strip_frames > 0:
                print(f"   æ¡å¸¦{strip_idx}: {total_strip_frames}å¸§, {total_strip_bytes:,}bytes, "
                      f"å¹³å‡{total_strip_bytes/total_strip_frames:.1f}bytes/å¸§")
        
        # å‹ç¼©æ•ˆç‡
        raw_size = total_frames * WIDTH * HEIGHT * 2
        compression_ratio = raw_size / total_bytes if total_bytes > 0 else 0
        print(f"\nğŸ“ˆ å‹ç¼©æ•ˆç‡:")
        print(f"   åŸå§‹å¤§å°ä¼°ç®—: {raw_size:,} bytes ({raw_size/1024/1024:.1f} MB)")
        print(f"   å‹ç¼©æ¯”: {compression_ratio:.1f}:1")
        print(f"   å‹ç¼©ç‡: {(1-total_bytes/raw_size)*100:.1f}%")

# å…¨å±€ç»Ÿè®¡å¯¹è±¡
encoding_stats = EncodingStats()

def main():
    pa = argparse.ArgumentParser(description="Encode to GBA YUV9 with big block codebook")
    pa.add_argument("input")
    pa.add_argument("--duration", type=float, default=5.0)
    pa.add_argument("--full-duration", action="store_true")
    pa.add_argument("--fps", type=int, default=30)
    pa.add_argument("--out", default="video_data")
    pa.add_argument("--strip-count", type=int, default=DEFAULT_STRIP_COUNT)
    pa.add_argument("--i-frame-interval", type=int, default=60)
    pa.add_argument("--diff-threshold", type=float, default=2.0)
    pa.add_argument("--force-i-threshold", type=float, default=0.7)
    pa.add_argument("--variance-threshold", type=float, default=5.0)
    pa.add_argument("--distortion-threshold", type=float, default=10.0,
                   help="å¤±çœŸé˜ˆå€¼ï¼Œç”¨äºå†³å®šæ˜¯å¦ä½¿ç”¨4x4å¤§å—ç è¡¨ï¼ˆé»˜è®¤10.0ï¼‰")
    pa.add_argument("--big-block-codebook-size", type=int, default=DEFAULT_BIG_BLOCK_CODEBOOK_SIZE)
    pa.add_argument("--small-block-codebook-size", type=int, default=EFFECTIVE_UNIFIED_CODEBOOK_SIZE)
    pa.add_argument("--kmeans-max-iter", type=int, default=200)
    pa.add_argument("--threads", type=int, default=None)
    pa.add_argument("--i-frame-weight", type=int, default=3)
    pa.add_argument("--dither", action="store_true")

    args = pa.parse_args()

    cap = cv2.VideoCapture(args.input)
    if not cap.isOpened():
        raise SystemExit("âŒ æ‰“ä¸å¼€è¾“å…¥æ–‡ä»¶")

    src_fps = cap.get(cv2.CAP_PROP_FPS) or 30
    every = int(round(src_fps / args.fps))
    
    if args.full_duration:
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        grab_max = total_frames
        actual_duration = total_frames / src_fps
        print(f"ç¼–ç æ•´ä¸ªè§†é¢‘: {total_frames} å¸§ï¼Œæ—¶é•¿ {actual_duration:.2f} ç§’")
    else:
        grab_max = int(args.duration * src_fps)
        print(f"ç¼–ç æ—¶é•¿: {args.duration} ç§’ ({grab_max} å¸§)")

    strip_heights = calculate_strip_heights(HEIGHT, args.strip_count)
    print(f"æ¡å¸¦é…ç½®: {args.strip_count} ä¸ªæ¡å¸¦ï¼Œé«˜åº¦åˆ†åˆ«ä¸º: {strip_heights}")
    print(f"ç æœ¬é…ç½®: å¤§å—ç è¡¨{args.big_block_codebook_size}é¡¹, å°å—ç è¡¨{args.small_block_codebook_size}é¡¹")
    if args.dither:
        print(f"ğŸ¨ å·²å¯ç”¨æŠ–åŠ¨ç®—æ³•ï¼ˆè›‡å½¢æ‰«æï¼‰")
    
    frames = []
    idx = 0
    print("æ­£åœ¨æå–å¸§...")
    with concurrent.futures.ThreadPoolExecutor(max_workers=args.threads) as executor:
        while idx < grab_max:
            ret, frm = cap.read()
            if not ret:
                break
            if idx % every == 0:
                frm = cv2.resize(frm, (WIDTH, HEIGHT), cv2.INTER_AREA)
                # if args.dither:
                #     frm = apply_dither_optimized(frm)
                strip_y_list = []
                y = 0
                for strip_height in strip_heights:
                    strip_y_list.append((frm, y, strip_height))
                    y += strip_height
                future_to_idx = {
                    executor.submit(pack_yuv420_strip, *args): i
                    for i, args in enumerate(strip_y_list)
                }
                frame_strips = [None] * len(strip_y_list)
                for future in concurrent.futures.as_completed(future_to_idx):
                    i = future_to_idx[future]
                    frame_strips[i] = future.result()
                frames.append(frame_strips)
                
                if len(frames) % 30 == 0:
                    print(f"  å·²æå– {len(frames)} å¸§")
            idx += 1
    cap.release()

    if not frames:
        raise SystemExit("âŒ æ²¡æœ‰ä»»ä½•å¸§è¢«é‡‡æ ·")

    print(f"æ€»å…±æå–äº† {len(frames)} å¸§")

    # ç”Ÿæˆç è¡¨
    gop_codebooks = generate_gop_codebooks_with_big_blocks(
        frames, args.strip_count, args.i_frame_interval, 
        args.variance_threshold, args.diff_threshold, args.distortion_threshold,
        args.big_block_codebook_size, args.small_block_codebook_size,
        args.kmeans_max_iter, args.i_frame_weight
    )

    # ç¼–ç æ‰€æœ‰å¸§
    print("æ­£åœ¨ç¼–ç å¸§...")
    encoded_frames = []
    frame_offsets = []
    current_offset = 0
    prev_strips = [None] * args.strip_count
    
    for frame_idx, current_strips in enumerate(frames):
        frame_offsets.append(current_offset)
        
        # æ‰¾åˆ°å½“å‰GOP
        gop_start = (frame_idx // args.i_frame_interval) * args.i_frame_interval
        gop_data = gop_codebooks[gop_start]
        
        frame_data = bytearray()
        
        for strip_idx, current_strip in enumerate(current_strips):
            strip_gop_data = gop_data[strip_idx]
            big_block_codebook = strip_gop_data['big_block_codebook']
            small_block_codebook = strip_gop_data['small_block_codebook']
            
            # æ‰¾åˆ°å½“å‰å¸§çš„åˆ†ç±»ä¿¡æ¯
            block_types = None
            big_block_indices = None
            for fid, bt, bbi in strip_gop_data['block_types_list']:
                if fid == frame_idx:
                    block_types = bt
                    big_block_indices = bbi
                    break
            
            force_i_frame = (frame_idx % args.i_frame_interval == 0) or frame_idx == 0
            
            if force_i_frame or prev_strips[strip_idx] is None:
                strip_data = encode_strip_i_frame_with_big_blocks(
                    current_strip, big_block_codebook, small_block_codebook, 
                    block_types, big_block_indices
                )
                is_i_frame = True
                
                # è®¡ç®—ç æœ¬å’Œç´¢å¼•å¤§å°
                big_codebook_size = args.big_block_codebook_size * BYTES_PER_BIG_BLOCK
                small_codebook_size = args.small_block_codebook_size * BYTES_PER_BLOCK
                index_size = len(strip_data) - 1 - big_codebook_size - small_codebook_size
                
                encoding_stats.add_i_frame(
                    strip_idx, len(strip_data), 
                    is_forced=force_i_frame,
                    codebook_size=big_codebook_size + small_codebook_size,
                    index_size=max(0, index_size)
                )
            else:
                strip_data, is_i_frame, used_zones, color_updates, detail_updates = encode_strip_p_frame_with_big_blocks(
                    current_strip, prev_strips[strip_idx],
                    big_block_codebook, small_block_codebook, block_types, big_block_indices,
                    args.diff_threshold, args.force_i_threshold, args.variance_threshold, args.distortion_threshold
                )
                
                if is_i_frame:
                    big_codebook_size = args.big_block_codebook_size * BYTES_PER_BIG_BLOCK
                    small_codebook_size = args.small_block_codebook_size * BYTES_PER_BLOCK
                    index_size = len(strip_data) - 1 - big_codebook_size - small_codebook_size
                    
                    encoding_stats.add_i_frame(
                        strip_idx, len(strip_data), 
                        is_forced=False,
                        codebook_size=big_codebook_size + small_codebook_size,
                        index_size=max(0, index_size)
                    )
                else:
                    total_updates = color_updates + detail_updates
                    
                    encoding_stats.add_p_frame(
                        strip_idx, len(strip_data), total_updates, used_zones,
                        color_updates, detail_updates
                    )
            
            frame_data.extend(struct.pack('<H', len(strip_data)))
            frame_data.extend(strip_data)
            
            prev_strips[strip_idx] = current_strip.copy() if current_strip.size > 0 else None
        
        encoded_frames.append(bytes(frame_data))
        current_offset += len(frame_data)
        
        if frame_idx % 30 == 0 or frame_idx == len(frames) - 1:
            print(f"  å·²ç¼–ç  {frame_idx + 1}/{len(frames)} å¸§")
    
    all_data = b''.join(encoded_frames)
    
    write_header(pathlib.Path(args.out).with_suffix(".h"), len(frames), len(all_data), 
                args.strip_count, strip_heights, args.big_block_codebook_size, args.small_block_codebook_size)
    write_source(pathlib.Path(args.out).with_suffix(".c"), all_data, frame_offsets, strip_heights)
    
    # æ‰“å°è¯¦ç»†ç»Ÿè®¡
    encoding_stats.print_summary(len(frames), len(all_data))

def write_header(path_h: pathlib.Path, frame_cnt: int, total_bytes: int, strip_count: int, 
                strip_heights: list, big_block_codebook_size: int, small_block_codebook_size: int):
    guard = "VIDEO_DATA_H"
    
    with path_h.open("w", encoding="utf-8") as f:
        f.write(textwrap.dedent(f"""\
            #ifndef {guard}
            #define {guard}

            #define VIDEO_FRAME_COUNT   {frame_cnt}
            #define VIDEO_WIDTH         {WIDTH}
            #define VIDEO_HEIGHT        {HEIGHT}
            #define VIDEO_TOTAL_BYTES   {total_bytes}
            #define VIDEO_STRIP_COUNT   {strip_count}
            #define BIG_BLOCK_CODEBOOK_SIZE {big_block_codebook_size}
            #define SMALL_BLOCK_CODEBOOK_SIZE {small_block_codebook_size}
            #define EFFECTIVE_UNIFIED_CODEBOOK_SIZE {EFFECTIVE_UNIFIED_CODEBOOK_SIZE}
            
            // å¸§ç±»å‹å®šä¹‰
            #define FRAME_TYPE_I        0x00
            #define FRAME_TYPE_P        0x01
            
            // ç‰¹æ®Šæ ‡è®°ï¼ˆåˆ é™¤è‰²å—æ ‡è®°ï¼‰
            #define BIG_BLOCK_MARKER    0xFE
            
            // å—å‚æ•°
            #define BLOCK_WIDTH         2
            #define BLOCK_HEIGHT        2
            #define BYTES_PER_BLOCK     7
            #define BYTES_PER_BIG_BLOCK 28

            // æ¡å¸¦é«˜åº¦æ•°ç»„
            extern const unsigned char strip_heights[VIDEO_STRIP_COUNT];
            
            extern const unsigned char video_data[VIDEO_TOTAL_BYTES];
            extern const unsigned int frame_offsets[VIDEO_FRAME_COUNT];

            #endif // {guard}
            """))

def write_source(path_c: pathlib.Path, data: bytes, frame_offsets: list, strip_heights: list):
    with path_c.open("w", encoding="utf-8") as f:
        f.write('#include "video_data.h"\n\n')
        

        
        f.write("const unsigned char strip_heights[] = {\n")
        f.write("    " + ', '.join(map(str, strip_heights)) + "\n")
        f.write("};\n\n")
        
        f.write("const unsigned int frame_offsets[] = {\n")
        for i in range(0, len(frame_offsets), 8):
            chunk = ', '.join(f"{offset}" for offset in frame_offsets[i:i+8])
            f.write("    " + chunk + ",\n")
        f.write("};\n\n")
        
        f.write("const unsigned char video_data[] = {\n")
        per_line = 16
        for i in range(0, len(data), per_line):
            chunk = ', '.join(f"0x{v:02X}" for v in data[i:i+per_line])
            f.write("    " + chunk + ",\n")
        f.write("};\n")

if __name__ == "__main__":
    main()