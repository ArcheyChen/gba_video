#!/usr/bin/env python3

import argparse, cv2, numpy as np, pathlib, textwrap
import struct
import concurrent.futures
import multiprocessing as mp
from sklearn.cluster import MiniBatchKMeans
from scipy.spatial.distance import cdist
from collections import defaultdict
import statistics
from numba import jit, njit, types
from numba.typed import List

from dither_opt import apply_dither_optimized

WIDTH, HEIGHT = 240, 160
DEFAULT_UNIFIED_CODEBOOK_SIZE = 256   # ç»Ÿä¸€ç æœ¬å¤§å°
EFFECTIVE_UNIFIED_CODEBOOK_SIZE = 254  # æœ‰æ•ˆç æœ¬å¤§å°ï¼ˆ0xFFä¿ç•™ï¼‰

# æ ‡è®°å¸¸é‡
COLOR_BLOCK_MARKER = 0xFF

Y_COEFF  = np.array([0.28571429,  0.57142857,  0.14285714])
CB_COEFF = np.array([-0.14285714, -0.28571429,  0.42857143])
CR_COEFF = np.array([ 0.35714286, -0.28571429, -0.07142857])
BLOCK_W, BLOCK_H = 2, 2
BYTES_PER_BLOCK  = 7  # 4Y + d_r + d_g + d_b

# æ–°å¢å¸¸é‡
ZONE_HEIGHT_PIXELS = 16  # æ¯ä¸ªåŒºåŸŸçš„åƒç´ é«˜åº¦
ZONE_HEIGHT_BIG_BLOCKS = ZONE_HEIGHT_PIXELS // (BLOCK_H * 2)  # æ¯ä¸ªåŒºåŸŸçš„4x4å¤§å—è¡Œæ•° (16åƒç´  = 4è¡Œ4x4å¤§å—)

# å¸§ç±»å‹æ ‡è¯†
FRAME_TYPE_I = 0x00  # Iå¸§ï¼ˆå…³é”®å¸§ï¼‰
FRAME_TYPE_P = 0x01  # På¸§ï¼ˆå·®åˆ†å¸§ï¼‰

@njit
def clip_value(value, min_val, max_val):
    """Numbaå…¼å®¹çš„clipå‡½æ•°"""
    if value < min_val:
        return min_val
    elif value > max_val:
        return max_val
    else:
        return value

@njit
def pack_yuv420_frame_numba(bgr_frame):
    """NumbaåŠ é€Ÿçš„æ•´å¸§YUV420è½¬æ¢"""
    blocks_h = HEIGHT // BLOCK_H
    blocks_w = WIDTH // BLOCK_W
    
    block_array = np.zeros((blocks_h, blocks_w, BYTES_PER_BLOCK), dtype=np.uint8)
    
    for by in range(blocks_h):
        for bx in range(blocks_w):
            # æå–2x2å—
            y_start = by * BLOCK_H
            x_start = bx * BLOCK_W
            
            # BGR to YUV conversion for 2x2 block
            cb_sum = 0.0
            cr_sum = 0.0
            y_values = np.zeros(4, dtype=np.uint8)
            
            idx = 0
            for dy in range(BLOCK_H):
                for dx in range(BLOCK_W):
                    if y_start + dy < HEIGHT and x_start + dx < WIDTH:
                        b = float(bgr_frame[y_start + dy, x_start + dx, 0])
                        g = float(bgr_frame[y_start + dy, x_start + dx, 1])  
                        r = float(bgr_frame[y_start + dy, x_start + dx, 2])
                        
                        y = r * 0.28571429 + g * 0.57142857 + b * 0.14285714
                        cb = r * (-0.14285714) + g * (-0.28571429) + b * 0.42857143
                        cr = r * 0.35714286 + g * (-0.28571429) + b * (-0.07142857)
                        
                        y_values[idx] = np.uint8(clip_value(y / 2.0, 0.0, 255.0))
                        cb_sum += cb
                        cr_sum += cr
                        idx += 1
            
            # Store Y values
            block_array[by, bx, 0:4] = y_values
            
            # Compute and store chroma
            cb_mean = cb_sum / 4.0
            cr_mean = cr_sum / 4.0
            
            d_r = clip_value(cr_mean, -128.0, 127.0)
            d_g = clip_value((-(cb_mean/2.0) - cr_mean) / 2.0, -128.0, 127.0)  
            d_b = clip_value(cb_mean, -128.0, 127.0)
            
            # å°†æœ‰ç¬¦å·å€¼è½¬æ¢ä¸ºæ— ç¬¦å·å­—èŠ‚å­˜å‚¨
            block_array[by, bx, 4] = np.uint8(np.int8(d_r).view(np.uint8))
            block_array[by, bx, 5] = np.uint8(np.int8(d_g).view(np.uint8))
            block_array[by, bx, 6] = np.uint8(np.int8(d_b).view(np.uint8))
    
    return block_array

def pack_yuv420_frame(frame_bgr: np.ndarray) -> np.ndarray:
    """ä½¿ç”¨NumbaåŠ é€Ÿçš„æ•´å¸§YUVè½¬æ¢åŒ…è£…å‡½æ•°"""
    return pack_yuv420_frame_numba(frame_bgr)

def calculate_block_variance(blocks_4x4: list) -> float:
    """è®¡ç®—4x4å—çš„æ–¹å·®ï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦ä¸ºçº¯è‰²å—"""
    # å°†4ä¸ª2x2å—åˆå¹¶ä¸ºä¸€ä¸ª4x4çš„Yå€¼æ•°ç»„
    y_values = []
    for block in blocks_4x4:
        y_values.extend(block[:4])  # åªå–Yå€¼
    
    y_array = np.array(y_values)
    return np.var(y_array)

@njit
def calculate_block_variance_numba(y_values):
    """NumbaåŠ é€Ÿçš„æ–¹å·®è®¡ç®—"""
    mean_val = np.mean(y_values)
    variance = 0.0
    for val in y_values:
        diff = val - mean_val
        variance += diff * diff
    return variance / len(y_values)

def calculate_2x2_block_variance(block: np.ndarray) -> float:
    """è®¡ç®—å•ä¸ª2x2å—çš„æ–¹å·®ï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦ä¸ºçº¯è‰²"""
    y_values = block[:4].astype(np.float64)
    return calculate_block_variance_numba(y_values)

def classify_4x4_blocks(blocks: np.ndarray, variance_threshold: float = 5.0) -> tuple:
    """å°†4x4å—åˆ†ç±»ä¸ºå¤§è‰²å—å’Œçº¹ç†å—"""
    blocks_h, blocks_w = blocks.shape[:2]
    big_blocks_h = blocks_h // 2
    big_blocks_w = blocks_w // 2
    
    color_blocks = []  # å¤§è‰²å—ï¼ˆç”¨ä¸‹é‡‡æ ·çš„2x2å—è¡¨ç¤ºï¼‰
    detail_blocks = []  # çº¹ç†å—
    block_types = {}   # è®°å½•æ¯ä¸ª4x4å—çš„ç±»å‹ {(big_by, big_bx): 'color' or 'detail'}
    
    for big_by in range(big_blocks_h):
        for big_bx in range(big_blocks_w):
            # æ”¶é›†4x4å¤§å—å†…çš„4ä¸ª2x2å°å—
            blocks_4x4 = []
            for sub_by in range(2):
                for sub_bx in range(2):
                    by = big_by * 2 + sub_by
                    bx = big_bx * 2 + sub_bx
                    if by < blocks_h and bx < blocks_w:
                        blocks_4x4.append(blocks[by, bx])
                    else:
                        blocks_4x4.append(np.zeros(BYTES_PER_BLOCK, dtype=np.uint8))
            
            # æ£€æŸ¥æ¯ä¸ª2x2å­å—æ˜¯å¦å†…éƒ¨ä¸€è‡´ï¼ˆè€Œéæ•´ä¸ª4x4å—ï¼‰
            all_2x2_blocks_are_uniform = True
            for block in blocks_4x4:
                if calculate_2x2_block_variance(block) > variance_threshold:
                    all_2x2_blocks_are_uniform = False
                    break
            
            if all_2x2_blocks_are_uniform:
                # å¤§è‰²å—ï¼šæ¯ä¸ª2x2å—å†…éƒ¨éƒ½ä¸€è‡´ï¼Œå¯ä»¥ä¸‹é‡‡æ ·ä¸ºä¸€ä¸ª2x2å—
                # æ¯ä¸ª2x2å—å–å…¶å†…éƒ¨çš„å¹³å‡å€¼ï¼Œç„¶åç”¨è¿™4ä¸ªå¹³å‡å€¼ç»„æˆä¸€ä¸ªä¸‹é‡‡æ ·çš„2x2å—
                downsampled_block = np.zeros(BYTES_PER_BLOCK, dtype=np.uint8)
                
                # æå–æ¯ä¸ª2x2å—çš„å†…éƒ¨å¹³å‡å€¼
                y_values = []
                d_r_values = []
                d_g_values = []
                d_b_values = []
                
                for block in blocks_4x4:
                    # æ¯ä¸ª2x2å—å†…éƒ¨ä¸€è‡´ï¼Œå–å…¶4ä¸ªYå€¼çš„å¹³å‡
                    avg_y = np.mean(block[:4])
                    y_values.append(int(avg_y))
                    
                    # è‰²åº¦åˆ†é‡ç›´æ¥ä½¿ç”¨ï¼ˆå·²ç»æ˜¯è¯¥2x2å—çš„å¹³å‡è‰²åº¦ï¼‰
                    d_r_values.append(block[4].view(np.int8))
                    d_g_values.append(block[5].view(np.int8))
                    d_b_values.append(block[6].view(np.int8))
                
                # æ„å»ºä¸‹é‡‡æ ·çš„2x2å—ï¼šç”¨4ä¸ª2x2å­å—çš„å¹³å‡å€¼æ„æˆæ–°çš„2x2å—
                downsampled_block[:4] = np.array(y_values, dtype=np.uint8)
                downsampled_block[4] = np.clip(np.mean(d_r_values), -128, 127).astype(np.int8).view(np.uint8)
                downsampled_block[5] = np.clip(np.mean(d_g_values), -128, 127).astype(np.int8).view(np.uint8)
                downsampled_block[6] = np.clip(np.mean(d_b_values), -128, 127).astype(np.int8).view(np.uint8)
                
                color_blocks.append(downsampled_block)
                block_types[(big_by, big_bx)] = 'color'
            else:
                # çº¹ç†å—ï¼šè‡³å°‘æœ‰ä¸€ä¸ª2x2å­å—å†…éƒ¨ä¸ä¸€è‡´ï¼Œä¿ç•™æ‰€æœ‰4ä¸ª2x2å—
                detail_blocks.extend(blocks_4x4)
                block_types[(big_by, big_bx)] = 'detail'
    
    return color_blocks, detail_blocks, block_types

def classify_4x4_blocks_unified(blocks: np.ndarray, variance_threshold: float = 5.0) -> tuple:
    """å°†4x4å—åˆ†ç±»ä¸ºå¤§è‰²å—å’Œçº¹ç†å—ï¼Œç”¨äºç»Ÿä¸€ç æœ¬"""
    blocks_h, blocks_w = blocks.shape[:2]
    big_blocks_h = blocks_h // 2
    big_blocks_w = blocks_w // 2
    
    all_blocks = []  # æ‰€æœ‰2x2å—
    block_types = {}   # è®°å½•æ¯ä¸ª4x4å—çš„ç±»å‹å’Œå¯¹åº”çš„2x2å—ç´¢å¼•
    
    for big_by in range(big_blocks_h):
        for big_bx in range(big_blocks_w):
            # æ”¶é›†4x4å¤§å—å†…çš„4ä¸ª2x2å°å—
            blocks_4x4 = []
            for sub_by in range(2):
                for sub_bx in range(2):
                    by = big_by * 2 + sub_by
                    bx = big_bx * 2 + sub_bx
                    if by < blocks_h and bx < blocks_w:
                        blocks_4x4.append(blocks[by, bx])
                    else:
                        blocks_4x4.append(np.zeros(BYTES_PER_BLOCK, dtype=np.uint8))
            
            # æ£€æŸ¥æ¯ä¸ª2x2å­å—æ˜¯å¦å†…éƒ¨ä¸€è‡´ï¼ˆè€Œéæ•´ä¸ª4x4å—ï¼‰
            all_2x2_blocks_are_uniform = True
            for block in blocks_4x4:
                if calculate_2x2_block_variance(block) > variance_threshold:
                    all_2x2_blocks_are_uniform = False
                    break
            
            if all_2x2_blocks_are_uniform:
                # å¤§è‰²å—ï¼šæ¯ä¸ª2x2å­å—å†…éƒ¨éƒ½ä¸€è‡´ï¼Œç”¨ä¸‹é‡‡æ ·çš„2x2å—è¡¨ç¤º
                downsampled_block = np.zeros(BYTES_PER_BLOCK, dtype=np.uint8)
                
                y_values = []
                d_r_values = []
                d_g_values = []
                d_b_values = []
                
                for block in blocks_4x4:
                    # æ¯ä¸ª2x2å—å†…éƒ¨ä¸€è‡´ï¼Œå–å…¶å†…éƒ¨å¹³å‡å€¼
                    avg_y = np.mean(block[:4])
                    y_values.append(int(avg_y))
                    d_r_values.append(block[4].view(np.int8))
                    d_g_values.append(block[5].view(np.int8))
                    d_b_values.append(block[6].view(np.int8))
                
                # ç”¨4ä¸ª2x2å­å—çš„å¹³å‡å€¼æ„æˆä¸‹é‡‡æ ·å—
                downsampled_block[:4] = np.array(y_values, dtype=np.uint8)
                downsampled_block[4] = np.clip(np.mean(d_r_values), -128, 127).astype(np.int8).view(np.uint8)
                downsampled_block[5] = np.clip(np.mean(d_g_values), -128, 127).astype(np.int8).view(np.uint8)
                downsampled_block[6] = np.clip(np.mean(d_b_values), -128, 127).astype(np.int8).view(np.uint8)
                
                block_idx = len(all_blocks)
                all_blocks.append(downsampled_block)
                block_types[(big_by, big_bx)] = ('color', [block_idx])
            else:
                # çº¹ç†å—ï¼šè‡³å°‘æœ‰ä¸€ä¸ª2x2å­å—å†…éƒ¨ä¸ä¸€è‡´ï¼Œä¿ç•™æ‰€æœ‰4ä¸ª2x2å—
                block_indices = []
                for block in blocks_4x4:
                    block_idx = len(all_blocks)
                    all_blocks.append(block)
                    block_indices.append(block_idx)
                block_types[(big_by, big_bx)] = ('detail', block_indices)
    
    return all_blocks, block_types

def generate_codebook(blocks_data: np.ndarray, codebook_size: int, max_iter: int = 100) -> tuple:
    """ä½¿ç”¨K-Meansèšç±»ç”Ÿæˆç è¡¨"""
    if len(blocks_data) == 0:
        return np.zeros((codebook_size, BYTES_PER_BLOCK), dtype=np.uint8), 0
    
    if blocks_data.ndim > 2:
        blocks_data = blocks_data.reshape(-1, BYTES_PER_BLOCK)
    
    # ç§»é™¤å»é‡æ“ä½œï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®è¿›è¡Œèšç±»
    # è¿™æ ·K-Meanså¯ä»¥åŸºäºæ•°æ®çš„çœŸå®åˆ†å¸ƒï¼ˆåŒ…æ‹¬é¢‘æ¬¡ï¼‰è¿›è¡Œæ›´å¥½çš„èšç±»
    effective_size = min(len(blocks_data), codebook_size)
    
    if len(blocks_data) <= codebook_size:
        # å¦‚æœæ•°æ®é‡å°äºç æœ¬å¤§å°ï¼Œéœ€è¦å»é‡é¿å…é‡å¤
        blocks_as_tuples = [tuple(block) for block in blocks_data]
        unique_tuples = list(set(blocks_as_tuples))
        unique_blocks = np.array(unique_tuples, dtype=np.uint8)
        
        codebook = np.zeros((codebook_size, BYTES_PER_BLOCK), dtype=np.uint8)
        codebook[:len(unique_blocks)] = unique_blocks
        if len(unique_blocks) > 0:
            for i in range(len(unique_blocks), codebook_size):
                codebook[i] = unique_blocks[-1]
        return codebook, len(unique_blocks)
    
    # å¯¹äºå¤§æ•°æ®é›†ï¼Œç›´æ¥è¿›è¡ŒK-Meansèšç±»
    kmeans = MiniBatchKMeans(
        n_clusters=codebook_size,
        random_state=42,
        batch_size=min(1000, len(blocks_data)),
        max_iter=max_iter,
        n_init=3
    )
    blocks_for_clustering = convert_blocks_for_clustering(blocks_data)
    kmeans.fit(blocks_for_clustering)
    codebook = convert_codebook_from_clustering(kmeans.cluster_centers_)
    
    return codebook, codebook_size

def generate_unified_codebook(all_blocks: list, codebook_size: int = DEFAULT_UNIFIED_CODEBOOK_SIZE,
                             kmeans_max_iter: int = 100) -> np.ndarray:
    """ç”Ÿæˆç»Ÿä¸€ç æœ¬ï¼ˆä¿ç•™0xFFä½œä¸ºç‰¹æ®Šæ ‡è®°ï¼‰"""
    if all_blocks:
        blocks_array = np.array(all_blocks)
        # åªä½¿ç”¨255é¡¹æœ‰æ•ˆç æœ¬ï¼Œä¿ç•™0xFF
        effective_size = min(codebook_size - 1, EFFECTIVE_UNIFIED_CODEBOOK_SIZE)
        codebook, _ = generate_codebook(blocks_array, effective_size, kmeans_max_iter)
        
        # åˆ›å»ºå®Œæ•´çš„256é¡¹ç æœ¬
        full_codebook = np.zeros((codebook_size, BYTES_PER_BLOCK), dtype=np.uint8)
        full_codebook[:effective_size] = codebook[:effective_size]
        # ç¬¬255é¡¹ï¼ˆç´¢å¼•255/0xFFï¼‰å¤åˆ¶æœ€åä¸€ä¸ªæœ‰æ•ˆé¡¹ä½œä¸ºå ä½
        if effective_size > 0:
            full_codebook[255] = full_codebook[effective_size - 1]
    else:
        full_codebook = np.zeros((codebook_size, BYTES_PER_BLOCK), dtype=np.uint8)
    
    return full_codebook

@njit
def quantize_blocks_distance_numba(blocks_for_clustering, codebook_for_clustering):
    """NumbaåŠ é€Ÿçš„å—é‡åŒ–è·ç¦»è®¡ç®—"""
    n_blocks = blocks_for_clustering.shape[0]
    n_codebook = codebook_for_clustering.shape[0]
    indices = np.zeros(n_blocks, dtype=np.uint8)
    
    for i in range(n_blocks):
        min_dist = np.inf
        best_idx = 0
        
        for j in range(n_codebook):
            dist = 0.0
            for k in range(BYTES_PER_BLOCK):
                diff = blocks_for_clustering[i, k] - codebook_for_clustering[j, k]
                dist += diff * diff
            
            if dist < min_dist:
                min_dist = dist
                best_idx = j
        
        indices[i] = best_idx
    
    return indices

def quantize_blocks_unified(blocks_data: np.ndarray, codebook: np.ndarray) -> np.ndarray:
    """ä½¿ç”¨ç»Ÿä¸€ç è¡¨å¯¹å—è¿›è¡Œé‡åŒ–ï¼ˆé¿å…äº§ç”Ÿ0xFFï¼‰"""
    if len(blocks_data) == 0:
        return np.array([], dtype=np.uint8)
    
    # åªä½¿ç”¨å‰255é¡¹è¿›è¡Œé‡åŒ–
    effective_codebook = codebook[:EFFECTIVE_UNIFIED_CODEBOOK_SIZE]
    
    blocks_for_clustering = convert_blocks_for_clustering(blocks_data)
    codebook_for_clustering = convert_blocks_for_clustering(effective_codebook)
    
    # ä½¿ç”¨NumbaåŠ é€Ÿçš„è·ç¦»è®¡ç®—
    indices = quantize_blocks_distance_numba(blocks_for_clustering, codebook_for_clustering)
    
    return indices

@njit
def compute_2x2_block_differences_numba(current_flat, prev_flat, blocks_h, blocks_w):
    """NumbaåŠ é€Ÿçš„2x2å—å·®å¼‚è®¡ç®—"""
    block_diffs = np.zeros((blocks_h, blocks_w), dtype=np.float64)
    
    for i in range(blocks_h * blocks_w):
        y_diff_sum = 0.0
        for j in range(4):  # åªè®¡ç®—Yåˆ†é‡å·®å¼‚
            current_val = int(current_flat[i, j])
            prev_val = int(prev_flat[i, j])
            if current_val >= prev_val:
                diff = current_val - prev_val
            else:
                diff = prev_val - current_val
            y_diff_sum += diff
        block_diffs[i // blocks_w, i % blocks_w] = y_diff_sum / 4.0
    
    return block_diffs

def encode_i_frame_unified(blocks: np.ndarray, unified_codebook: np.ndarray, 
                          block_types: dict) -> bytes:
    """ç¼–ç Iå¸§ï¼ˆç»Ÿä¸€ç æœ¬ï¼‰"""
    data = bytearray()
    data.append(FRAME_TYPE_I)
    
    if blocks.size > 0:
        blocks_h, blocks_w = blocks.shape[:2]
        big_blocks_h = blocks_h // 2
        big_blocks_w = blocks_w // 2
        
        # å­˜å‚¨ç»Ÿä¸€ç æœ¬
        data.extend(unified_codebook.flatten().tobytes())
        
        # æŒ‰4x4å¤§å—çš„é¡ºåºç¼–ç 
        for big_by in range(big_blocks_h):
            for big_bx in range(big_blocks_w):
                # å¤„ç†block_typesä¸ºNoneçš„æƒ…å†µ
                if block_types is None or (big_by, big_bx) not in block_types:
                    # é»˜è®¤ä¸ºçº¹ç†å—å¤„ç†
                    for sub_by in range(2):
                        for sub_bx in range(2):
                            by = big_by * 2 + sub_by
                            bx = big_bx * 2 + sub_bx
                            if by < blocks_h and bx < blocks_w:
                                block = blocks[by, bx]
                                unified_idx = quantize_blocks_unified(block.reshape(1, -1), unified_codebook)[0]
                                data.append(unified_idx)
                            else:
                                data.append(0)
                else:
                    block_type, block_indices = block_types[(big_by, big_bx)]
                    
                    if block_type == 'color':
                        # è‰²å—ï¼šæ ‡è®°0xFF + 1ä¸ªç æœ¬ç´¢å¼•
                        data.append(COLOR_BLOCK_MARKER)
                        
                        # ä»åŸå§‹blocksé‡å»ºå¹³å‡å—
                        blocks_4x4 = []
                        for sub_by in range(2):
                            for sub_bx in range(2):
                                by = big_by * 2 + sub_by
                                bx = big_bx * 2 + sub_bx
                                if by < blocks_h and bx < blocks_w:
                                    blocks_4x4.append(blocks[by, bx])
                        
                        avg_block = np.mean(blocks_4x4, axis=0).round().astype(np.uint8)
                        for i in range(4, 7):
                            avg_val = np.mean([b[i].view(np.int8) for b in blocks_4x4])
                            avg_block[i] = np.clip(avg_val, -128, 127).astype(np.int8).view(np.uint8)
                        
                        unified_idx = quantize_blocks_unified(avg_block.reshape(1, -1), unified_codebook)[0]
                        data.append(unified_idx)
                    else:
                        # çº¹ç†å—ï¼š4ä¸ªç æœ¬ç´¢å¼•
                        for sub_by in range(2):
                            for sub_bx in range(2):
                                by = big_by * 2 + sub_by
                                bx = big_bx * 2 + sub_bx
                                if by < blocks_h and bx < blocks_w:
                                    block = blocks[by, bx]
                                    unified_idx = quantize_blocks_unified(block.reshape(1, -1), unified_codebook)[0]
                                    data.append(unified_idx)
                                else:
                                    data.append(0)
    
    return bytes(data)

def encode_p_frame_unified(current_blocks: np.ndarray, prev_blocks: np.ndarray,
                          unified_codebook: np.ndarray, block_types: dict,
                          diff_threshold: float, force_i_threshold: float = 0.7) -> tuple:
    """å·®åˆ†ç¼–ç På¸§ï¼ˆç»Ÿä¸€ç æœ¬ï¼‰"""
    if prev_blocks is None or current_blocks.shape != prev_blocks.shape:
        i_frame_data = encode_i_frame_unified(current_blocks, unified_codebook, block_types)
        return i_frame_data, True, 0, 0, 0
    
    blocks_h, blocks_w = current_blocks.shape[:2]
    total_blocks = blocks_h * blocks_w
    
    if total_blocks == 0:
        return b'', True, 0, 0, 0
    
    # ä½¿ç”¨NumbaåŠ é€Ÿçš„2x2å—å·®å¼‚è®¡ç®—
    current_flat = current_blocks.reshape(-1, BYTES_PER_BLOCK)
    prev_flat = prev_blocks.reshape(-1, BYTES_PER_BLOCK)
    block_diffs = compute_2x2_block_differences_numba(current_flat, prev_flat, blocks_h, blocks_w)
    
    big_blocks_h = blocks_h // 2
    big_blocks_w = blocks_w // 2
    
    # è®¡ç®—åŒºåŸŸæ•°é‡
    zones_count = (big_blocks_h + ZONE_HEIGHT_BIG_BLOCKS - 1) // ZONE_HEIGHT_BIG_BLOCKS
    
    # æŒ‰åŒºåŸŸç»„ç»‡æ›´æ–°
    zone_detail_updates = [[] for _ in range(zones_count)]
    zone_color_updates = [[] for _ in range(zones_count)]
    total_updated_blocks = 0
    
    for big_by in range(big_blocks_h):
        for big_bx in range(big_blocks_w):
            # æ£€æŸ¥4x4å¤§å—å†…æ¯ä¸ª2x2å­å—æ˜¯å¦éœ€è¦æ›´æ–°
            positions = [
                (big_by * 2, big_bx * 2),
                (big_by * 2, big_bx * 2 + 1),
                (big_by * 2 + 1, big_bx * 2),
                (big_by * 2 + 1, big_bx * 2 + 1)
            ]
            
            # æ£€æŸ¥æ¯ä¸ª2x2å­å—æ˜¯å¦éœ€è¦æ›´æ–°
            subblock_needs_update = []
            any_subblock_needs_update = False
            
            for by, bx in positions:
                if by < blocks_h and bx < blocks_w:
                    needs_update = block_diffs[by, bx] > diff_threshold
                    subblock_needs_update.append(needs_update)
                    if needs_update:
                        any_subblock_needs_update = True
                else:
                    subblock_needs_update.append(False)
            
            if any_subblock_needs_update:
                # è®¡ç®—å±äºå“ªä¸ªåŒºåŸŸ
                zone_idx = min(big_by // ZONE_HEIGHT_BIG_BLOCKS, zones_count - 1)
                # è®¡ç®—åœ¨åŒºåŸŸå†…çš„ç›¸å¯¹åæ ‡
                zone_relative_by = big_by % ZONE_HEIGHT_BIG_BLOCKS
                zone_relative_idx = zone_relative_by * big_blocks_w + big_bx
                
                # ç»Ÿè®¡å®é™…æ›´æ–°çš„å­å—æ•°
                actual_updated_subblocks = sum(subblock_needs_update)
                total_updated_blocks += actual_updated_subblocks
                
                # æ£€æŸ¥block_typesæ˜¯å¦ä¸ºNoneæˆ–ä¸åŒ…å«å½“å‰å—
                is_color_block = (block_types is not None and 
                                (big_by, big_bx) in block_types and 
                                block_types[(big_by, big_bx)][0] == 'color')
                
                if is_color_block:
                    # è‰²å—æ›´æ–°ï¼šå¦‚æœä»»ä½•å­å—éœ€è¦æ›´æ–°ï¼Œå°±æ›´æ–°æ•´ä¸ªè‰²å—
                    blocks_4x4 = []
                    for by, bx in positions:
                        if by < blocks_h and bx < blocks_w:
                            blocks_4x4.append(current_blocks[by, bx])
                    
                    avg_block = np.mean(blocks_4x4, axis=0).round().astype(np.uint8)
                    for i in range(4, 7):
                        avg_val = np.mean([b[i].view(np.int8) for b in blocks_4x4])
                        avg_block[i] = np.clip(avg_val, -128, 127).astype(np.int8).view(np.uint8)
                    
                    color_idx = quantize_blocks_unified(avg_block.reshape(1, -1), unified_codebook)[0]
                    zone_color_updates[zone_idx].append((zone_relative_idx, color_idx))
                else:
                    # çº¹ç†å—æ›´æ–°ï¼šä¸ºæ¯ä¸ª2x2å­å—ç”Ÿæˆç´¢å¼•ï¼Œä¸éœ€è¦æ›´æ–°çš„ç”¨0xFFæ ‡è®°
                    indices = []
                    for i, (by, bx) in enumerate(positions):
                        if by < blocks_h and bx < blocks_w and subblock_needs_update[i]:
                            # éœ€è¦æ›´æ–°çš„å­å—ï¼šé‡åŒ–å¹¶è·å–ç´¢å¼•
                            block = current_blocks[by, bx]
                            unified_idx = quantize_blocks_unified(block.reshape(1, -1), unified_codebook)[0]
                            indices.append(unified_idx)
                        else:
                            # ä¸éœ€è¦æ›´æ–°çš„å­å—ï¼šä½¿ç”¨è·³è¿‡æ ‡è®°
                            indices.append(COLOR_BLOCK_MARKER)  # 0xFF
                    
                    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å­å—éƒ½ä¸éœ€è¦æ›´æ–°ï¼ˆé˜²æ­¢å‘é€å…¨FFçš„å—ï¼‰
                    if any(idx != COLOR_BLOCK_MARKER for idx in indices):
                        zone_detail_updates[zone_idx].append((zone_relative_idx, indices))
    
    # åˆ¤æ–­æ˜¯å¦éœ€è¦Iå¸§
    update_ratio = total_updated_blocks / total_blocks
    if update_ratio > force_i_threshold:
        i_frame_data = encode_i_frame_unified(current_blocks, unified_codebook, block_types)
        return i_frame_data, True, 0, 0, 0
    
    # ç¼–ç På¸§
    data = bytearray()
    data.append(FRAME_TYPE_P)
    
    # ç»Ÿè®¡ä½¿ç”¨çš„åŒºåŸŸæ•°é‡
    used_zones = 0
    total_color_updates = 0
    total_detail_updates = 0
    
    # ç”Ÿæˆä¸¤ä¸ªåŒºåŸŸbitmap
    detail_zone_bitmap = 0
    color_zone_bitmap = 0
    
    for zone_idx in range(zones_count):
        if zone_detail_updates[zone_idx]:
            detail_zone_bitmap |= (1 << zone_idx)
            total_detail_updates += len(zone_detail_updates[zone_idx])
        if zone_color_updates[zone_idx]:
            color_zone_bitmap |= (1 << zone_idx)
            total_color_updates += len(zone_color_updates[zone_idx])
    
    # è®¡ç®—å®é™…ä½¿ç”¨çš„åŒºåŸŸæ•°ï¼ˆä¸¤ä¸ªbitmapçš„å¹¶é›†ï¼‰
    combined_bitmap = detail_zone_bitmap | color_zone_bitmap
    used_zones = bin(combined_bitmap).count('1')
    
    # å†™å…¥ä¸¤ä¸ªu16 bitmap
    data.extend(struct.pack('<H', detail_zone_bitmap))
    data.extend(struct.pack('<H', color_zone_bitmap))
    
    # æŒ‰åŒºåŸŸç¼–ç çº¹ç†å—æ›´æ–°
    for zone_idx in range(zones_count):
        if detail_zone_bitmap & (1 << zone_idx):
            detail_updates = zone_detail_updates[zone_idx]
            data.append(len(detail_updates))
            
            # å­˜å‚¨çº¹ç†å—æ›´æ–°
            for relative_idx, indices in detail_updates:
                data.append(relative_idx)
                for idx in indices:
                    data.append(idx)
    
    # æŒ‰åŒºåŸŸç¼–ç è‰²å—æ›´æ–°
    for zone_idx in range(zones_count):
        if color_zone_bitmap & (1 << zone_idx):
            color_updates = zone_color_updates[zone_idx]
            data.append(len(color_updates))
            
            # å­˜å‚¨è‰²å—æ›´æ–°
            for relative_idx, unified_idx in color_updates:
                data.append(relative_idx)
                data.append(unified_idx)
    
    return bytes(data), False, used_zones, total_color_updates, total_detail_updates

@njit
def identify_updated_blocks_numba(block_diffs, diff_threshold, blocks_h, blocks_w):
    """NumbaåŠ é€Ÿçš„æ›´æ–°å—è¯†åˆ«"""
    big_blocks_h = blocks_h // 2
    big_blocks_w = blocks_w // 2
    updated_positions = []
    
    for big_by in range(big_blocks_h):
        for big_bx in range(big_blocks_w):
            needs_update = False
            
            # æ£€æŸ¥4ä¸ª2x2å­å—çš„ä½ç½®
            for sub_by in range(2):
                for sub_bx in range(2):
                    by = big_by * 2 + sub_by
                    bx = big_bx * 2 + sub_bx
                    
                    if by < blocks_h and bx < blocks_w:
                        if block_diffs[by, bx] > diff_threshold:
                            needs_update = True
                            break
                if needs_update:
                    break
            
            if needs_update:
                updated_positions.append((big_by, big_bx))
    
    return updated_positions

def identify_updated_big_blocks(current_blocks: np.ndarray, prev_blocks: np.ndarray,
                               diff_threshold: float) -> set:
    """è¯†åˆ«éœ€è¦æ›´æ–°çš„4x4å¤§å—ä½ç½® - NumbaåŠ é€Ÿç‰ˆæœ¬"""
    if prev_blocks is None or current_blocks.shape != prev_blocks.shape:
        # å¦‚æœæ²¡æœ‰å‰ä¸€å¸§ï¼Œæ‰€æœ‰å¤§å—éƒ½éœ€è¦æ›´æ–°
        blocks_h, blocks_w = current_blocks.shape[:2]
        big_blocks_h = blocks_h // 2
        big_blocks_w = blocks_w // 2
        return {(big_by, big_bx) for big_by in range(big_blocks_h) for big_bx in range(big_blocks_w)}
    
    blocks_h, blocks_w = current_blocks.shape[:2]
    
    # ä½¿ç”¨NumbaåŠ é€Ÿçš„å—å·®å¼‚è®¡ç®—
    current_flat = current_blocks.reshape(-1, BYTES_PER_BLOCK)
    prev_flat = prev_blocks.reshape(-1, BYTES_PER_BLOCK)
    block_diffs = compute_2x2_block_differences_numba(current_flat, prev_flat, blocks_h, blocks_w)
    
    # ä½¿ç”¨NumbaåŠ é€Ÿçš„æ›´æ–°å—è¯†åˆ«
    updated_list = identify_updated_blocks_numba(block_diffs, diff_threshold, blocks_h, blocks_w)
    
    return set(updated_list)

def convert_blocks_for_clustering(blocks_data: np.ndarray) -> np.ndarray:
    """å°†å—æ•°æ®è½¬æ¢ä¸ºæ­£ç¡®çš„èšç±»æ ¼å¼"""
    if len(blocks_data) == 0:
        return blocks_data.astype(np.float32)
    
    if blocks_data.ndim > 2:
        blocks_data = blocks_data.reshape(-1, BYTES_PER_BLOCK)
    
    blocks_float = blocks_data.astype(np.float32)
    
    for i in range(4, BYTES_PER_BLOCK):
        blocks_float[:, i] = blocks_data[:, i].view(np.int8).astype(np.float32)
    
    return blocks_float

def convert_codebook_from_clustering(codebook_float: np.ndarray) -> np.ndarray:
    """å°†èšç±»ç»“æœè½¬æ¢å›æ­£ç¡®çš„å—æ ¼å¼"""
    codebook = np.zeros_like(codebook_float, dtype=np.uint8)
    
    codebook[:, 0:4] = np.clip(codebook_float[:, 0:4].round(), 0, 255).astype(np.uint8)
    
    for i in range(4, BYTES_PER_BLOCK):
        clipped_values = np.clip(codebook_float[:, i].round(), -128, 127).astype(np.int8)
        codebook[:, i] = clipped_values.view(np.uint8)
    
    return codebook

def extract_effective_blocks_from_big_blocks(blocks: np.ndarray, big_block_positions: set,
                                           variance_threshold: float = 5.0) -> list:
    """ä»æŒ‡å®šçš„4x4å¤§å—ä½ç½®æå–æœ‰æ•ˆçš„2x2å—"""
    blocks_h, blocks_w = blocks.shape[:2]
    effective_blocks = []
    
    for big_by, big_bx in big_block_positions:
        # æ”¶é›†4x4å¤§å—å†…çš„4ä¸ª2x2å°å—
        blocks_4x4 = []
        for sub_by in range(2):
            for sub_bx in range(2):
                by = big_by * 2 + sub_by
                bx = big_bx * 2 + sub_bx
                if by < blocks_h and bx < blocks_w:
                    blocks_4x4.append(blocks[by, bx])
                else:
                    blocks_4x4.append(np.zeros(BYTES_PER_BLOCK, dtype=np.uint8))
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºè‰²å—ï¼ˆæ¯ä¸ª2x2å­å—å†…éƒ¨ä¸€è‡´ï¼‰
        all_2x2_blocks_are_uniform = True
        for block in blocks_4x4:
            if calculate_2x2_block_variance(block) > variance_threshold:
                all_2x2_blocks_are_uniform = False
                break
        
        if all_2x2_blocks_are_uniform:
            # è‰²å—ï¼šç”Ÿæˆä¸‹é‡‡æ ·çš„2x2å—
            downsampled_block = np.zeros(BYTES_PER_BLOCK, dtype=np.uint8)
            
            y_values = []
            d_r_values = []
            d_g_values = []
            d_b_values = []
            
            for block in blocks_4x4:
                # æ¯ä¸ª2x2å—å†…éƒ¨ä¸€è‡´ï¼Œå–å…¶å†…éƒ¨å¹³å‡å€¼
                avg_y = np.mean(block[:4])
                y_values.append(int(avg_y))
                d_r_values.append(block[4].view(np.int8))
                d_g_values.append(block[5].view(np.int8))
                d_b_values.append(block[6].view(np.int8))
            
            # ç”¨4ä¸ª2x2å­å—çš„å¹³å‡å€¼æ„æˆä¸‹é‡‡æ ·å—
            downsampled_block[:4] = np.array(y_values, dtype=np.uint8)
            downsampled_block[4] = np.clip(np.mean(d_r_values), -128, 127).astype(np.int8).view(np.uint8)
            downsampled_block[5] = np.clip(np.mean(d_g_values), -128, 127).astype(np.int8).view(np.uint8)
            downsampled_block[6] = np.clip(np.mean(d_b_values), -128, 127).astype(np.int8).view(np.uint8)
            
            effective_blocks.append(downsampled_block)
        else:
            # çº¹ç†å—ï¼šæ·»åŠ æ‰€æœ‰4ä¸ª2x2å—
            effective_blocks.extend(blocks_4x4)
    
    return effective_blocks

def process_single_gop_frame(args_tuple):
    """å¤„ç†å•ä¸ªGOPçš„å•å¸§ - ç”¨äºå¤šè¿›ç¨‹"""
    (gop_start, gop_end, frame_data_list, variance_threshold, 
     diff_threshold, codebook_size, kmeans_max_iter, i_frame_weight) = args_tuple
    
    try:
        effective_blocks = []
        block_types_list = []
        
        # å¤„ç†GOPä¸­çš„æ¯ä¸€å¸§
        prev_blocks = None
        
        for frame_idx in range(gop_start, gop_end):
            relative_frame_idx = frame_idx - gop_start
            if relative_frame_idx >= len(frame_data_list):
                break
                
            frame_blocks = frame_data_list[relative_frame_idx]
            if frame_blocks.size == 0:
                continue
            
            # ç¡®å®šå¸§ç±»å‹å’Œéœ€è¦æ›´æ–°çš„å¤§å—
            is_i_frame = (frame_idx == gop_start)  # GOPç¬¬ä¸€å¸§æ˜¯Iå¸§
            
            if is_i_frame:
                # Iå¸§ï¼šæ‰€æœ‰å¤§å—éƒ½æœ‰æ•ˆ
                blocks_h, blocks_w = frame_blocks.shape[:2]
                big_blocks_h = blocks_h // 2
                big_blocks_w = blocks_w // 2
                updated_big_blocks = {(big_by, big_bx) for big_by in range(big_blocks_h) for big_bx in range(big_blocks_w)}
            else:
                # På¸§ï¼šåªæœ‰æ›´æ–°çš„å¤§å—æœ‰æ•ˆ
                updated_big_blocks = identify_updated_big_blocks(frame_blocks, prev_blocks, diff_threshold)
            
            # ä»æœ‰æ•ˆå¤§å—ä¸­æå–2x2å—
            frame_effective_blocks = extract_effective_blocks_from_big_blocks(
                frame_blocks, updated_big_blocks, variance_threshold)
            
            # Iå¸§å—åŠ æƒï¼šå¤åˆ¶å¤šæ¬¡ä»¥å¢åŠ åœ¨èšç±»ä¸­çš„å½±å“åŠ›
            if is_i_frame:
                weighted_blocks = frame_effective_blocks * i_frame_weight  # å¤åˆ¶i_frame_weightæ¬¡
                effective_blocks.extend(weighted_blocks)
            else:
                effective_blocks.extend(frame_effective_blocks)
            
            # ç”Ÿæˆå®Œæ•´çš„block_typesç”¨äºç¼–ç ï¼ˆæ‰€æœ‰å¤§å—ï¼Œä¸åªæ˜¯æœ‰æ•ˆçš„ï¼‰
            frame_blocks_list, block_types = classify_4x4_blocks_unified(frame_blocks, variance_threshold)
            block_types_list.append((frame_idx, block_types))
            
            prev_blocks = frame_blocks.copy()
        
        # ä½¿ç”¨æœ‰æ•ˆå—ç”Ÿæˆç»Ÿä¸€ç æœ¬
        unified_codebook = generate_unified_codebook(effective_blocks, codebook_size, kmeans_max_iter)
        
        return {
            'gop_start': gop_start,
            'unified_codebook': unified_codebook,
            'block_types_list': block_types_list,
            'total_blocks_count': len(effective_blocks),
            'success': True
        }
        
    except Exception as e:
        return {
            'gop_start': gop_start,
            'error': str(e),
            'success': False
        }

def generate_gop_unified_codebooks(frames: list, i_frame_interval: int,
                                  variance_threshold: float, diff_threshold: float,
                                  codebook_size: int = DEFAULT_UNIFIED_CODEBOOK_SIZE,
                                  kmeans_max_iter: int = 100, i_frame_weight: int = 3,
                                  max_workers: int = None) -> dict:
    """ä¸ºæ¯ä¸ªGOPç”Ÿæˆç»Ÿä¸€ç æœ¬ï¼ˆå¤šè¿›ç¨‹ç‰ˆæœ¬ï¼‰"""
    print("æ­£åœ¨ä¸ºæ¯ä¸ªGOPç”Ÿæˆç»Ÿä¸€ç æœ¬ï¼ˆå¤šè¿›ç¨‹ï¼ŒåŸºäºæœ‰æ•ˆå—ï¼ŒIå¸§åŠ æƒï¼‰...")
    
    if max_workers is None:
        max_workers = max(1, mp.cpu_count() - 1)  # ç•™ä¸€ä¸ªæ ¸å¿ƒç»™ç³»ç»Ÿ
    
    print(f"ä½¿ç”¨ {max_workers} ä¸ªè¿›ç¨‹å¹¶è¡Œå¤„ç†")
    
    gop_codebooks = {}
    
    # ç¡®å®šIå¸§ä½ç½®
    i_frame_positions = []
    for frame_idx in range(len(frames)):
        if frame_idx % i_frame_interval == 0:
            i_frame_positions.append(frame_idx)
    
    # å‡†å¤‡ä»»åŠ¡å‚æ•°
    tasks = []
    for gop_idx, gop_start in enumerate(i_frame_positions):
        if gop_idx + 1 < len(i_frame_positions):
            gop_end = i_frame_positions[gop_idx + 1]
        else:
            gop_end = len(frames)
        
        # æå–è¯¥GOPçš„æ‰€æœ‰å¸§æ•°æ®
        frame_data_list = []
        for frame_idx in range(gop_start, gop_end):
            if frame_idx < len(frames):
                frame_data_list.append(frames[frame_idx])
            else:
                break
        
        if frame_data_list:  # åªæœ‰å½“æœ‰æ•°æ®æ—¶æ‰æ·»åŠ ä»»åŠ¡
            task_args = (
                gop_start, gop_end, frame_data_list,
                variance_threshold, diff_threshold, codebook_size,
                kmeans_max_iter, i_frame_weight
            )
            tasks.append(task_args)
    
    total_tasks = len(tasks)
    print(f"æ€»å…± {len(i_frame_positions)} ä¸ªGOPï¼Œ{total_tasks} ä¸ªå¤„ç†ä»»åŠ¡")
    
    # ä½¿ç”¨å¤šè¿›ç¨‹å¤„ç†
    completed_tasks = 0
    
    # è®¾ç½®è¿›ç¨‹å¯åŠ¨æ–¹æ³•ï¼ˆé¿å…æŸäº›å¹³å°çš„é—®é¢˜ï¼‰
    try:
        mp.set_start_method('spawn', force=True)
    except RuntimeError:
        pass  # å¦‚æœå·²ç»è®¾ç½®è¿‡å°±è·³è¿‡
    
    with mp.Pool(processes=max_workers) as pool:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        results = []
        for task_args in tasks:
            result = pool.apply_async(process_single_gop_frame, (task_args,))
            results.append(result)
        
        # æ”¶é›†ç»“æœå¹¶æ˜¾ç¤ºè¿›åº¦
        processed_results = []
        for i, result in enumerate(results):
            try:
                task_result = result.get(timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
                processed_results.append(task_result)
                completed_tasks += 1
                
                if completed_tasks % max(1, total_tasks // 20) == 0 or completed_tasks == total_tasks:
                    progress = completed_tasks / total_tasks * 100
                    print(f"  è¿›åº¦: {completed_tasks}/{total_tasks} ({progress:.1f}%)")
                    
            except Exception as e:
                print(f"  âš ï¸ ä»»åŠ¡ {i} å¤„ç†å¤±è´¥: {e}")
                # åˆ›å»ºä¸€ä¸ªå¤±è´¥çš„ç»“æœ
                task_args = tasks[i]
                processed_results.append({
                    'gop_start': task_args[0],
                    'success': False,
                    'error': str(e)
                })
    
    # ç»„ç»‡ç»“æœ
    failed_count = 0
    for result in processed_results:
        if not result['success']:
            print(f"  âŒ GOP {result['gop_start']} å¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            failed_count += 1
            continue
        
        gop_start = result['gop_start']
        gop_codebooks[gop_start] = {
            'unified_codebook': result['unified_codebook'],
            'block_types_list': result['block_types_list'],
            'total_blocks_count': result['total_blocks_count']
        }
    
    if failed_count > 0:
        print(f"  âš ï¸ å…±æœ‰ {failed_count} ä¸ªä»»åŠ¡å¤„ç†å¤±è´¥")
    else:
        print(f"  âœ… æ‰€æœ‰ {total_tasks} ä¸ªä»»åŠ¡å¤„ç†å®Œæˆ")
    
    # éªŒè¯ç»“æœå®Œæ•´æ€§
    for gop_start in i_frame_positions:
        if gop_start not in gop_codebooks:
            print(f"  âš ï¸ GOP {gop_start} ç¼ºå°‘æ•°æ®ï¼Œä½¿ç”¨é»˜è®¤ç æœ¬")
            # åˆ›å»ºé»˜è®¤ç æœ¬
            default_codebook = np.zeros((codebook_size, BYTES_PER_BLOCK), dtype=np.uint8)
            gop_codebooks[gop_start] = {
                'unified_codebook': default_codebook,
                'block_types_list': [],
                'total_blocks_count': 0
            }
    
    return gop_codebooks

class EncodingStats:
    """ç¼–ç ç»Ÿè®¡ç±»"""
    def __init__(self):
        # å¸§ç»Ÿè®¡
        self.total_frames_processed = 0
        self.total_i_frames = 0
        self.forced_i_frames = 0  # å¼ºåˆ¶Iå¸§ï¼ˆGOPå¼€å§‹ï¼‰
        self.threshold_i_frames = 0  # è¶…é˜ˆå€¼Iå¸§
        self.total_p_frames = 0
        
        # å¤§å°ç»Ÿè®¡
        self.total_i_frame_bytes = 0
        self.total_p_frame_bytes = 0
        self.total_codebook_bytes = 0  # åªè®¡ç®—Iå¸§ä¸­çš„ç æœ¬æ•°æ®
        self.total_index_bytes = 0     # åªè®¡ç®—Iå¸§ä¸­çš„ç´¢å¼•æ•°æ®
        self.total_p_overhead_bytes = 0  # På¸§çš„å¼€é”€æ•°æ®ï¼ˆbitmapç­‰ï¼‰
        
        # På¸§å—æ›´æ–°ç»Ÿè®¡
        self.p_frame_updates = []  # æ¯ä¸ªPå¸§çš„æ›´æ–°å—æ•°
        self.zone_usage = defaultdict(int)  # åŒºåŸŸä½¿ç”¨æ¬¡æ•°
        
        # ç»†èŠ‚ç»Ÿè®¡
        self.color_block_bytes = 0
        self.detail_block_bytes = 0
        self.color_update_count = 0
        self.detail_update_count = 0
    
    def add_i_frame(self, size_bytes, is_forced=True, codebook_size=0, index_size=0):
        self.total_frames_processed += 1
        self.total_i_frames += 1
        if is_forced:
            self.forced_i_frames += 1
        else:
            self.threshold_i_frames += 1
        
        self.total_i_frame_bytes += size_bytes
        self.total_codebook_bytes += codebook_size
        self.total_index_bytes += index_size
    
    def add_p_frame(self, size_bytes, updates_count, zone_count, 
                   color_updates=0, detail_updates=0):
        self.total_frames_processed += 1
        self.total_p_frames += 1
        self.total_p_frame_bytes += size_bytes
        self.p_frame_updates.append(updates_count)
        self.zone_usage[zone_count] += 1
        
        # På¸§å¼€é”€ï¼šå¸§ç±»å‹(1) + bitmap(2) + æ¯ä¸ªåŒºåŸŸçš„è®¡æ•°(2*zones)
        overhead = 3 + zone_count * 2
        self.total_p_overhead_bytes += overhead
        
        self.color_update_count += color_updates
        self.detail_update_count += detail_updates
    
    def print_summary(self, total_frames, total_bytes):
        print(f"\nğŸ“Š ç¼–ç ç»Ÿè®¡æŠ¥å‘Š")
        print(f"=" * 60)
        
        # åŸºæœ¬ç»Ÿè®¡
        print(f"ğŸ¬ å¸§ç»Ÿè®¡:")
        print(f"   è§†é¢‘å¸§æ•°: {total_frames}")
        print(f"   Iå¸§: {self.total_i_frames} ({self.total_i_frames/total_frames*100:.1f}%)")
        print(f"     - å¼ºåˆ¶Iå¸§: {self.forced_i_frames}")
        print(f"     - è¶…é˜ˆå€¼Iå¸§: {self.threshold_i_frames}")
        print(f"   På¸§: {self.total_p_frames} ({self.total_p_frames/total_frames*100:.1f}%)")
        
        # å¤§å°ç»Ÿè®¡
        print(f"\nğŸ’¾ ç©ºé—´å ç”¨:")
        print(f"   æ€»å¤§å°: {total_bytes:,} bytes ({total_bytes/1024:.1f} KB)")
        print(f"   Iå¸§æ•°æ®: {self.total_i_frame_bytes:,} bytes ({self.total_i_frame_bytes/total_bytes*100:.1f}%)")
        print(f"   På¸§æ•°æ®: {self.total_p_frame_bytes:,} bytes ({self.total_p_frame_bytes/total_bytes*100:.1f}%)")
        
        if self.total_i_frames > 0:
            print(f"   å¹³å‡Iå¸§å¤§å°: {self.total_i_frame_bytes/self.total_i_frames:.1f} bytes")
        if self.total_p_frames > 0:
            print(f"   å¹³å‡På¸§å¤§å°: {self.total_p_frame_bytes/self.total_p_frames:.1f} bytes")
        
        # æ•°æ®æ„æˆç»Ÿè®¡
        print(f"\nğŸ¨ æ•°æ®æ„æˆ:")
        print(f"   ç æœ¬æ•°æ®: {self.total_codebook_bytes:,} bytes ({self.total_codebook_bytes/total_bytes*100:.1f}%)")
        print(f"   Iå¸§ç´¢å¼•: {self.total_index_bytes:,} bytes ({self.total_index_bytes/total_bytes*100:.1f}%)")
        
        # På¸§æ•°æ®æ„æˆ
        p_frame_data_bytes = self.total_p_frame_bytes - self.total_p_overhead_bytes
        print(f"   På¸§æ›´æ–°æ•°æ®: {p_frame_data_bytes:,} bytes ({p_frame_data_bytes/total_bytes*100:.1f}%)")
        print(f"   På¸§å¼€é”€: {self.total_p_overhead_bytes:,} bytes ({self.total_p_overhead_bytes/total_bytes*100:.1f}%)")
        
        # På¸§æ›´æ–°ç»Ÿè®¡
        if self.p_frame_updates:
            avg_updates = statistics.mean(self.p_frame_updates)
            median_updates = statistics.median(self.p_frame_updates)
            max_updates = max(self.p_frame_updates)
            min_updates = min(self.p_frame_updates)
            
            print(f"\nâš¡ På¸§æ›´æ–°åˆ†æ:")
            print(f"   å¹³å‡æ›´æ–°å—æ•°: {avg_updates:.1f}")
            print(f"   ä¸­ä½æ•°æ›´æ–°å—æ•°: {median_updates}")
            print(f"   æœ€å¤§æ›´æ–°å—æ•°: {max_updates}")
            print(f"   æœ€å°æ›´æ–°å—æ•°: {min_updates}")
            print(f"   è‰²å—æ›´æ–°æ€»æ•°: {self.color_update_count:,}")
            print(f"   çº¹ç†å—æ›´æ–°æ€»æ•°: {self.detail_update_count:,}")
        
        # åŒºåŸŸä½¿ç”¨ç»Ÿè®¡
        if self.zone_usage:
            print(f"\nğŸ—ºï¸  åŒºåŸŸä½¿ç”¨åˆ†å¸ƒ:")
            for zone_count in sorted(self.zone_usage.keys()):
                frames_count = self.zone_usage[zone_count]
                if self.total_p_frames > 0:
                    print(f"   {zone_count}ä¸ªåŒºåŸŸ: {frames_count}æ¬¡ ({frames_count/self.total_p_frames*100:.1f}%)")
        
        # å‹ç¼©æ•ˆç‡
        raw_size = total_frames * WIDTH * HEIGHT * 2  # å‡è®¾16ä½åƒç´ 
        compression_ratio = raw_size / total_bytes if total_bytes > 0 else 0
        print(f"\nğŸ“ˆ å‹ç¼©æ•ˆç‡:")
        print(f"   åŸå§‹å¤§å°ä¼°ç®—: {raw_size:,} bytes ({raw_size/1024/1024:.1f} MB)")
        print(f"   å‹ç¼©æ¯”: {compression_ratio:.1f}:1")
        print(f"   å‹ç¼©ç‡: {(1-total_bytes/raw_size)*100:.1f}%")

# å…¨å±€ç»Ÿè®¡å¯¹è±¡
encoding_stats = EncodingStats()

def main():
    pa = argparse.ArgumentParser(description="Encode to GBA YUV9 with unified codebook")
    pa.add_argument("input")
    pa.add_argument("--duration", type=float, default=5.0)
    pa.add_argument("--full-duration", action="store_true")
    pa.add_argument("--fps", type=int, default=30)
    pa.add_argument("--out", default="video_data")
    pa.add_argument("--i-frame-interval", type=int, default=60)
    pa.add_argument("--diff-threshold", type=float, default=2.0)
    pa.add_argument("--force-i-threshold", type=float, default=0.7)
    pa.add_argument("--variance-threshold", type=float, default=5.0,
                   help="æ–¹å·®é˜ˆå€¼ï¼Œç”¨äºåŒºåˆ†çº¯è‰²å—å’Œçº¹ç†å—ï¼ˆé»˜è®¤5.0ï¼‰")
    pa.add_argument("--codebook-size", type=int, default=DEFAULT_UNIFIED_CODEBOOK_SIZE,
                   help=f"ç»Ÿä¸€ç æœ¬å¤§å°ï¼ˆé»˜è®¤{DEFAULT_UNIFIED_CODEBOOK_SIZE}ï¼‰")
    pa.add_argument("--kmeans-max-iter", type=int, default=200)
    pa.add_argument("--threads", type=int, default=None)
    pa.add_argument("--i-frame-weight", type=int, default=3,
                   help="Iå¸§å—åœ¨èšç±»ä¸­çš„æƒé‡å€æ•°ï¼ˆé»˜è®¤3ï¼‰")
    pa.add_argument("--max-workers", type=int, default=None,
                   help="GOPå¤„ç†çš„æœ€å¤§è¿›ç¨‹æ•°ï¼ˆé»˜è®¤ä¸ºCPUæ ¸å¿ƒæ•°-1ï¼‰")
    pa.add_argument("--dither", action="store_true",
                   help="å¯ç”¨Floyd-SteinbergæŠ–åŠ¨ç®—æ³•æå‡ç”»è´¨")
    args = pa.parse_args()

    cap = cv2.VideoCapture(args.input)
    if not cap.isOpened():
        raise SystemExit("âŒ æ‰“ä¸å¼€è¾“å…¥æ–‡ä»¶")

    src_fps = cap.get(cv2.CAP_PROP_FPS) or 30
    # è®¡ç®—å®é™…è¾“å‡ºFPSï¼šå¦‚æœç›®æ ‡FPSé«˜äºæºFPSï¼Œä½¿ç”¨æºFPS
    actual_output_fps = min(args.fps, src_fps)
    every = int(round(src_fps / actual_output_fps))
    
    if args.full_duration:
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        grab_max = total_frames
        actual_duration = total_frames / src_fps
        print(f"ç¼–ç æ•´ä¸ªè§†é¢‘: {total_frames} å¸§ï¼Œæ—¶é•¿ {actual_duration:.2f} ç§’")
    else:
        grab_max = int(args.duration * src_fps)
        print(f"ç¼–ç æ—¶é•¿: {args.duration} ç§’ ({grab_max} å¸§)")

    print(f"æºè§†é¢‘FPS: {src_fps:.2f}, ç›®æ ‡FPS: {args.fps}, å®é™…è¾“å‡ºFPS: {actual_output_fps:.2f}")
    print(f"ç æœ¬é…ç½®: ç»Ÿä¸€ç æœ¬{args.codebook_size}é¡¹")
    if args.dither:
        print(f"ğŸ¨ å·²å¯ç”¨æŠ–åŠ¨ç®—æ³•ï¼ˆè›‡å½¢æ‰«æï¼‰")
    
    frames = []
    idx = 0
    print("æ­£åœ¨æå–å¸§...")
    
    while idx < grab_max:
        ret, frm = cap.read()
        if not ret:
            break
        if idx % every == 0:
            frm = cv2.resize(frm, (WIDTH, HEIGHT), cv2.INTER_AREA)
            frm = cv2.GaussianBlur(frm, (3, 3), 0.41)
            if args.dither:
                frm = apply_dither_optimized(frm)
            
            frame_blocks = pack_yuv420_frame(frm)
            frames.append(frame_blocks)
            
            if len(frames) % 30 == 0:
                print(f"  å·²æå– {len(frames)} å¸§")
        idx += 1
    cap.release()

    if not frames:
        raise SystemExit("âŒ æ²¡æœ‰ä»»ä½•å¸§è¢«é‡‡æ ·")

    print(f"æ€»å…±æå–äº† {len(frames)} å¸§")

    # ç”Ÿæˆç»Ÿä¸€ç æœ¬ï¼ˆä¼ å…¥max_workerså‚æ•°ï¼‰
    gop_codebooks = generate_gop_unified_codebooks(
        frames, args.i_frame_interval, 
        args.variance_threshold, args.diff_threshold, args.codebook_size, 
        args.kmeans_max_iter, args.i_frame_weight, args.max_workers
    )

    # åŸºäº GOP å†… P å¸§çº¹ç†å—ä½¿ç”¨é¢‘æ¬¡ï¼Œå¯¹æ¯ä¸ªç æœ¬é¡¹é™åºé‡æ’
    import numpy as _np
    for gop_start, gop_data in gop_codebooks.items():
        codebook = gop_data['unified_codebook']
        counts = _np.zeros(len(codebook), dtype=int)
        # GOP èŒƒå›´ï¼šèµ·å§‹å¸§ä¸‹ä¸€ä¸ªåˆ°ä¸‹ä¸€ä¸ª I å¸§
        gop_end = min(gop_start + args.i_frame_interval, len(frames))
        for fid in range(gop_start + 1, gop_end):
            cur = frames[fid]
            prev = frames[fid - 1]
            # è¯†åˆ«æ›´æ–°çš„å¤§å—
            updated = identify_updated_big_blocks(cur, prev, args.diff_threshold)
            # å–å‡ºè¯¥å¸§çš„ block_types
            bt_map = None
            for fno, bt in gop_data['block_types_list']:
                if fno == fid:
                    bt_map = bt; break
            # ç´¯åŠ æ¯ä¸ªçº¹ç†å­å—çš„ç´¢å¼•ä½¿ç”¨æ¬¡æ•°
            for by, bx in updated:
                is_color = bt_map and bt_map.get((by, bx), ('detail',))[0] == 'color'
                if not is_color:
                    for sy in (0,1):
                        for sx in (0,1):
                            y, x = by*2+sy, bx*2+sx
                            if y < cur.shape[0] and x < cur.shape[1]:
                                b = cur[y, x]
                                idx = quantize_blocks_unified(b.reshape(1, -1), codebook)[0]
                                counts[idx] += 1
        # æ ¹æ® counts é™åºæ’åºï¼Œstable ä¿æŒç›¸åŒé¢‘æ¬¡é¡¹åŸåº
        order = _np.argsort(-counts, kind='stable')
        gop_data['unified_codebook'] = codebook[order]
    
    # ç¼–ç æ‰€æœ‰å¸§
    print("æ­£åœ¨ç¼–ç å¸§...")
    encoded_frames = []
    frame_offsets = []
    current_offset = 0
    prev_frame = None
    
    for frame_idx, current_frame in enumerate(frames):
        frame_offsets.append(current_offset)
        
        # æ‰¾åˆ°å½“å‰GOP
        gop_start = (frame_idx // args.i_frame_interval) * args.i_frame_interval
        gop_data = gop_codebooks[gop_start]
        
        unified_codebook = gop_data['unified_codebook']
        
        # æ‰¾åˆ°å½“å‰å¸§çš„block_typesï¼Œå¤„ç†ç¼ºå¤±çš„æƒ…å†µ
        block_types = None
        for fid, bt in gop_data['block_types_list']:
            if fid == frame_idx:
                block_types = bt
                break
        
        # å¦‚æœblock_typesä»ç„¶ä¸ºNoneï¼Œç”Ÿæˆé»˜è®¤çš„block_types
        if block_types is None:
            print(f"  âš ï¸ å¸§ {frame_idx} ç¼ºå°‘block_typesï¼Œä½¿ç”¨é»˜è®¤åˆ†ç±»")
            # ä¸´æ—¶ç”Ÿæˆblock_types
            _, block_types = classify_4x4_blocks_unified(current_frame, args.variance_threshold)
        
        force_i_frame = (frame_idx % args.i_frame_interval == 0) or frame_idx == 0
        
        if force_i_frame or prev_frame is None:
            frame_data = encode_i_frame_unified(
                current_frame, unified_codebook, block_types
            )
            is_i_frame = True
            
            # è®¡ç®—ç æœ¬å’Œç´¢å¼•å¤§å°
            codebook_size = args.codebook_size * BYTES_PER_BLOCK
            index_size = len(frame_data) - 1 - codebook_size
            
            encoding_stats.add_i_frame(
                len(frame_data), 
                is_forced=force_i_frame,
                codebook_size=codebook_size,
                index_size=max(0, index_size)
            )
        else:
            frame_data, is_i_frame, used_zones, color_updates, detail_updates = encode_p_frame_unified(
                current_frame, prev_frame,
                unified_codebook, block_types,
                args.diff_threshold, args.force_i_threshold
            )
            
            if is_i_frame:
                codebook_size = args.codebook_size * BYTES_PER_BLOCK
                index_size = len(frame_data) - 1 - codebook_size
                
                encoding_stats.add_i_frame(
                    len(frame_data), 
                    is_forced=False,
                    codebook_size=codebook_size,
                    index_size=max(0, index_size)
                )
            else:
                total_updates = color_updates + detail_updates
                
                encoding_stats.add_p_frame(
                    len(frame_data), total_updates, used_zones,
                    color_updates, detail_updates
                )
        
        encoded_frames.append(frame_data)
        current_offset += len(frame_data)
        
        prev_frame = current_frame.copy() if current_frame.size > 0 else None
        
        if frame_idx % 30 == 0 or frame_idx == len(frames) - 1:
            print(f"  å·²ç¼–ç  {frame_idx + 1}/{len(frames)} å¸§")
    
    all_data = b''.join(encoded_frames)
    
    write_header(pathlib.Path(args.out).with_suffix(".h"), len(frames), len(all_data), 
                args.codebook_size, actual_output_fps)
    write_source(pathlib.Path(args.out).with_suffix(".c"), all_data, frame_offsets)
    
    # æ‰“å°è¯¦ç»†ç»Ÿè®¡
    encoding_stats.print_summary(len(frames), len(all_data))

def write_header(path_h: pathlib.Path, frame_cnt: int, total_bytes: int, codebook_size: int, output_fps: float):
    guard = "VIDEO_DATA_H"
    
    with path_h.open("w", encoding="utf-8") as f:
        f.write(textwrap.dedent(f"""\
            #ifndef {guard}
            #define {guard}

            #define VIDEO_FRAME_COUNT   {frame_cnt}
            #define VIDEO_WIDTH         {WIDTH}
            #define VIDEO_HEIGHT        {HEIGHT}
            #define VIDEO_TOTAL_BYTES   {total_bytes}
            #define VIDEO_FPS           {int(round(output_fps*10000))}
            #define UNIFIED_CODEBOOK_SIZE {codebook_size}
            #define EFFECTIVE_UNIFIED_CODEBOOK_SIZE {EFFECTIVE_UNIFIED_CODEBOOK_SIZE}
            
            // å¸§ç±»å‹å®šä¹‰
            #define FRAME_TYPE_I        0x00
            #define FRAME_TYPE_P        0x01
            
            // ç‰¹æ®Šæ ‡è®°
            #define COLOR_BLOCK_MARKER  0xFF
            
            // å—å‚æ•°
            #define BLOCK_WIDTH         2
            #define BLOCK_HEIGHT        2
            #define BYTES_PER_BLOCK     7
            
            extern const unsigned char video_data[VIDEO_TOTAL_BYTES];
            extern const unsigned int frame_offsets[VIDEO_FRAME_COUNT];

            #endif // {guard}
            """))

def write_source(path_c: pathlib.Path, data: bytes, frame_offsets: list):
    with path_c.open("w", encoding="utf-8") as f:
        f.write('#include "video_data.h"\n\n')
        
        f.write("const unsigned int frame_offsets[] = {\n")
        for i in range(0, len(frame_offsets), 8):
            chunk = ', '.join(f"{offset}" for offset in frame_offsets[i:i+8])
            f.write("    " + chunk + ",\n")
        f.write("};\n\n")
        
        f.write("const unsigned char video_data[] = {\n")
        per_line = 16
        for i in range(0, len(data), per_line):
            chunk = ', '.join(f"0x{v:02X}" for v in data[i:i+per_line])
            f.write("    " + chunk + ",\n")
        f.write("};\n")
if __name__ == "__main__":
    main()