#!/usr/bin/env python3
"""
gba_encode.py  v7  â€”â€”  æŠŠè§†é¢‘/å›¾ç‰‡åºåˆ—è½¬æˆ GBA Mode3 YUV9 æ•°æ®ï¼ˆæ”¯æŒæ¡å¸¦å¸§é—´å·®åˆ† + ç»Ÿä¸€ç æœ¬å‘é‡é‡åŒ–ï¼‰
è¾“å‡º video_data.c / video_data.h
é»˜è®¤ 5 s @ 30 fpsï¼Œå¯ç”¨ --duration / --fps ä¿®æ”¹ï¼Œæˆ–ä½¿ç”¨ --full-duration ç¼–ç æ•´ä¸ªè§†é¢‘
æ”¯æŒæ¡å¸¦å¤„ç†ï¼Œæ¯ä¸ªæ¡å¸¦ç‹¬ç«‹è¿›è¡ŒI/På¸§ç¼–ç  + ç»Ÿä¸€ç æœ¬å‹ç¼©ï¼ˆæœ‰æ•ˆ255é¡¹ï¼Œ0xFFä¿ç•™ä½œä¸ºè‰²å—æ ‡è®°ï¼‰
"""

import argparse, cv2, numpy as np, pathlib, textwrap
import struct
import concurrent.futures
from sklearn.cluster import MiniBatchKMeans
from scipy.spatial.distance import cdist
from collections import defaultdict
import statistics

WIDTH, HEIGHT = 240, 160
DEFAULT_STRIP_COUNT = 4
DEFAULT_UNIFIED_CODEBOOK_SIZE = 256   # ç»Ÿä¸€ç æœ¬å¤§å°
EFFECTIVE_UNIFIED_CODEBOOK_SIZE = 254  # æœ‰æ•ˆç æœ¬å¤§å°ï¼ˆ0xFFä¿ç•™ï¼‰
BIG_BLOCK_CODEBOOK_SIZE = 256  # å¤§å—ç´¢å¼•ç è¡¨å¤§å°
EFFECTIVE_BIG_BLOCK_CODEBOOK_SIZE = 254  # æœ‰æ•ˆå¤§å—ç´¢å¼•ç è¡¨å¤§å°ï¼ˆ0xFF, 0xFEä¿ç•™ï¼‰

# æ ‡è®°å¸¸é‡
COLOR_BLOCK_MARKER = 0xFF
COMPLEX_TEXTURE_MARKER = 0xFE

Y_COEFF  = np.array([0.28571429,  0.57142857,  0.14285714])
CB_COEFF = np.array([-0.14285714, -0.28571429,  0.42857143])
CR_COEFF = np.array([ 0.35714286, -0.28571429, -0.07142857])
BLOCK_W, BLOCK_H = 2, 2
BYTES_PER_BLOCK  = 7  # 4Y + d_r + d_g + d_b

# æ–°å¢å¸¸é‡
ZONE_HEIGHT_PIXELS = 16  # æ¯ä¸ªåŒºåŸŸçš„åƒç´ é«˜åº¦
ZONE_HEIGHT_BIG_BLOCKS = ZONE_HEIGHT_PIXELS // (BLOCK_H * 2)  # æ¯ä¸ªåŒºåŸŸçš„4x4å¤§å—è¡Œæ•° (16åƒç´  = 4è¡Œ4x4å¤§å—)

# å¸§ç±»å‹æ ‡è¯†
FRAME_TYPE_I = 0x00  # Iå¸§ï¼ˆå…³é”®å¸§ï¼‰
FRAME_TYPE_P = 0x01  # På¸§ï¼ˆå·®åˆ†å¸§ï¼‰

def calculate_strip_heights(height: int, strip_count: int) -> list:
    """è®¡ç®—æ¯ä¸ªæ¡å¸¦çš„é«˜åº¦ï¼Œç¡®ä¿æ¯ä¸ªæ¡å¸¦é«˜åº¦éƒ½æ˜¯4çš„å€æ•°"""
    if height % 4 != 0:
        raise ValueError(f"è§†é¢‘é«˜åº¦ {height} å¿…é¡»æ˜¯4çš„å€æ•°")
    
    base_height = (height // strip_count // 4) * 4
    remaining_height = height - (base_height * strip_count)
    
    strip_heights = []
    for i in range(strip_count):
        current_height = base_height
        if remaining_height >= 4:
            current_height += 4
            remaining_height -= 4
        strip_heights.append(current_height)
    
    if sum(strip_heights) != height:
        raise ValueError(f"æ¡å¸¦é«˜åº¦åˆ†é…é”™è¯¯: {strip_heights} æ€»å’Œ {sum(strip_heights)} != {height}")
    
    for i, h in enumerate(strip_heights):
        if h % 4 != 0:
            raise ValueError(f"æ¡å¸¦ {i} é«˜åº¦ {h} ä¸æ˜¯4çš„å€æ•°")
    
    return strip_heights

def pack_yuv420_strip(frame_bgr: np.ndarray, strip_y: int, strip_height: int) -> np.ndarray:
    """å‘é‡åŒ–å®ç°ï¼ŒæŠŠæŒ‡å®šæ¡å¸¦çš„ 240Ã—strip_heightÃ—3 BGR â†’ YUV420"""
    strip_bgr = frame_bgr[strip_y:strip_y + strip_height, :, :]
    B = strip_bgr[:, :, 0].astype(np.float32)
    G = strip_bgr[:, :, 1].astype(np.float32)
    R = strip_bgr[:, :, 2].astype(np.float32)

    Y  = (R*Y_COEFF[0]  + G*Y_COEFF[1]  + B*Y_COEFF[2]).round()
    Cb = (R*CB_COEFF[0] + G*CB_COEFF[1] + B*CB_COEFF[2]).round()
    Cr = (R*CR_COEFF[0] + G*CR_COEFF[1] + B*CR_COEFF[2]).round()

    Y  = np.clip(Y,  0, 255).astype(np.uint8)
    Cb = np.clip(Cb, -128, 127).astype(np.int16)
    Cr = np.clip(Cr, -128, 127).astype(np.int16)

    h, w = strip_bgr.shape[:2]
    blocks_h = h // BLOCK_H
    blocks_w = w // BLOCK_W

    Y_blocks  = Y.reshape(blocks_h, BLOCK_H, blocks_w, BLOCK_W)
    Cb_blocks = Cb.reshape(blocks_h, BLOCK_H, blocks_w, BLOCK_W)
    Cr_blocks = Cr.reshape(blocks_h, BLOCK_H, blocks_w, BLOCK_W)

    y_flat = (Y_blocks.transpose(0,2,1,3).reshape(blocks_h, blocks_w, 4) >> 1).astype(np.uint8)
    cb_mean = np.clip(Cb_blocks.mean(axis=(1,3)).round(), -128, 127).astype(np.int16)
    cr_mean = np.clip(Cr_blocks.mean(axis=(1,3)).round(), -128, 127).astype(np.int16)
    
    d_r = np.clip(cr_mean, -128, 127).astype(np.int8)
    d_g = np.clip((-(cb_mean >> 1) - cr_mean) >> 1, -128, 127).astype(np.int8)
    d_b = np.clip(cb_mean, -128, 127).astype(np.int8)

    block_array = np.zeros((blocks_h, blocks_w, BYTES_PER_BLOCK), dtype=np.uint8)
    block_array[..., 0:4] = y_flat
    block_array[..., 4] = d_r.view(np.uint8)
    block_array[..., 5] = d_g.view(np.uint8)
    block_array[..., 6] = d_b.view(np.uint8)
    
    return block_array

def calculate_block_variance(blocks_4x4: list) -> float:
    """è®¡ç®—4x4å—çš„æ–¹å·®ï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦ä¸ºçº¯è‰²å—"""
    # å°†4ä¸ª2x2å—åˆå¹¶ä¸ºä¸€ä¸ª4x4çš„Yå€¼æ•°ç»„
    y_values = []
    for block in blocks_4x4:
        y_values.extend(block[:4])  # åªå–Yå€¼
    
    y_array = np.array(y_values)
    return np.var(y_array)

def classify_4x4_blocks(blocks: np.ndarray, variance_threshold: float = 5.0) -> tuple:
    """å°†4x4å—åˆ†ç±»ä¸ºçº¯è‰²å—å’Œçº¹ç†å—"""
    blocks_h, blocks_w = blocks.shape[:2]
    big_blocks_h = blocks_h // 2
    big_blocks_w = blocks_w // 2
    
    color_blocks = []  # çº¯è‰²å—
    detail_blocks = []  # çº¹ç†å—
    block_types = {}   # è®°å½•æ¯ä¸ª4x4å—çš„ç±»å‹ {(big_by, big_bx): 'color' or 'detail'}
    
    for big_by in range(big_blocks_h):
        for big_bx in range(big_blocks_w):
            # æ”¶é›†4x4å¤§å—å†…çš„4ä¸ª2x2å°å—
            blocks_4x4 = []
            for sub_by in range(2):
                for sub_bx in range(2):
                    by = big_by * 2 + sub_by
                    bx = big_bx * 2 + sub_bx
                    if by < blocks_h and bx < blocks_w:
                        blocks_4x4.append(blocks[by, bx])
                    else:
                        blocks_4x4.append(np.zeros(BYTES_PER_BLOCK, dtype=np.uint8))
            
            # è®¡ç®—æ–¹å·®åˆ¤æ–­æ˜¯å¦ä¸ºçº¯è‰²å—
            variance = calculate_block_variance(blocks_4x4)
            
            if variance < variance_threshold:
                # çº¯è‰²å—ï¼šè®¡ç®—å¹³å‡å€¼ä½œä¸ºä»£è¡¨
                avg_block = np.mean(blocks_4x4, axis=0).round().astype(np.uint8)
                # å¯¹äºd_r, d_g, d_béœ€è¦ç‰¹æ®Šå¤„ç†
                for i in range(4, 7):
                    avg_val = np.mean([b[i].view(np.int8) for b in blocks_4x4])
                    avg_block[i] = np.clip(avg_val, -128, 127).astype(np.int8).view(np.uint8)
                
                color_blocks.append(avg_block)
                block_types[(big_by, big_bx)] = 'color'
            else:
                # çº¹ç†å—ï¼šä¿ç•™æ‰€æœ‰4ä¸ª2x2å—
                detail_blocks.extend(blocks_4x4)
                block_types[(big_by, big_bx)] = 'detail'
    
    return color_blocks, detail_blocks, block_types

def classify_4x4_blocks_unified(blocks: np.ndarray, variance_threshold: float = 5.0) -> tuple:
    """å°†4x4å—åˆ†ç±»ä¸ºçº¯è‰²å—å’Œçº¹ç†å—ï¼Œç”¨äºç»Ÿä¸€ç æœ¬"""
    blocks_h, blocks_w = blocks.shape[:2]
    big_blocks_h = blocks_h // 2
    big_blocks_w = blocks_w // 2
    
    all_blocks = []  # æ‰€æœ‰2x2å—
    block_types = {}   # è®°å½•æ¯ä¸ª4x4å—çš„ç±»å‹å’Œå¯¹åº”çš„2x2å—ç´¢å¼•
    
    for big_by in range(big_blocks_h):
        for big_bx in range(big_blocks_w):
            # æ”¶é›†4x4å¤§å—å†…çš„4ä¸ª2x2å°å—
            blocks_4x4 = []
            for sub_by in range(2):
                for sub_bx in range(2):
                    by = big_by * 2 + sub_by
                    bx = big_bx * 2 + sub_bx
                    if by < blocks_h and bx < blocks_w:
                        blocks_4x4.append(blocks[by, bx])
                    else:
                        blocks_4x4.append(np.zeros(BYTES_PER_BLOCK, dtype=np.uint8))
            
            # è®¡ç®—æ–¹å·®åˆ¤æ–­æ˜¯å¦ä¸ºçº¯è‰²å—
            variance = calculate_block_variance(blocks_4x4)
            
            if variance < variance_threshold:
                # çº¯è‰²å—ï¼šè®¡ç®—å¹³å‡å€¼ä½œä¸ºä¸€ä¸ª2x2å—
                avg_block = np.mean(blocks_4x4, axis=0).round().astype(np.uint8)
                for i in range(4, 7):
                    avg_val = np.mean([b[i].view(np.int8) for b in blocks_4x4])
                    avg_block[i] = np.clip(avg_val, -128, 127).astype(np.int8).view(np.uint8)
                
                block_idx = len(all_blocks)
                all_blocks.append(avg_block)
                block_types[(big_by, big_bx)] = ('color', [block_idx])
            else:
                # çº¹ç†å—ï¼šä¿ç•™æ‰€æœ‰4ä¸ª2x2å—
                block_indices = []
                for block in blocks_4x4:
                    block_idx = len(all_blocks)
                    all_blocks.append(block)
                    block_indices.append(block_idx)
                block_types[(big_by, big_bx)] = ('detail', block_indices)
    
    return all_blocks, block_types

def generate_codebook(blocks_data: np.ndarray, codebook_size: int, max_iter: int = 100) -> tuple:
    """ä½¿ç”¨K-Meansèšç±»ç”Ÿæˆç è¡¨"""
    if len(blocks_data) == 0:
        return np.zeros((codebook_size, BYTES_PER_BLOCK), dtype=np.uint8), 0
    
    if blocks_data.ndim > 2:
        blocks_data = blocks_data.reshape(-1, BYTES_PER_BLOCK)
    
    blocks_as_tuples = [tuple(block) for block in blocks_data]
    unique_tuples = list(set(blocks_as_tuples))
    unique_blocks = np.array(unique_tuples, dtype=np.uint8)
    
    effective_size = min(len(unique_blocks), codebook_size)
    
    if len(unique_blocks) <= codebook_size:
        codebook = np.zeros((codebook_size, BYTES_PER_BLOCK), dtype=np.uint8)
        codebook[:len(unique_blocks)] = unique_blocks
        if len(unique_blocks) > 0:
            for i in range(len(unique_blocks), codebook_size):
                codebook[i] = unique_blocks[-1]
        return codebook, effective_size
    
    kmeans = MiniBatchKMeans(
        n_clusters=codebook_size,
        random_state=42,
        batch_size=min(1000, len(blocks_data)),
        max_iter=max_iter,
        n_init=3
    )
    blocks_for_clustering = convert_blocks_for_clustering(blocks_data)
    kmeans.fit(blocks_for_clustering)
    codebook = convert_codebook_from_clustering(kmeans.cluster_centers_)
    
    return codebook, codebook_size

def generate_unified_codebook(all_blocks: list, codebook_size: int = DEFAULT_UNIFIED_CODEBOOK_SIZE,
                             kmeans_max_iter: int = 100) -> np.ndarray:
    """ç”Ÿæˆç»Ÿä¸€ç æœ¬ï¼ˆä¿ç•™0xFFä½œä¸ºç‰¹æ®Šæ ‡è®°ï¼‰"""
    if all_blocks:
        blocks_array = np.array(all_blocks)
        # åªä½¿ç”¨255é¡¹æœ‰æ•ˆç æœ¬ï¼Œä¿ç•™0xFF
        effective_size = min(codebook_size - 1, EFFECTIVE_UNIFIED_CODEBOOK_SIZE)
        codebook, _ = generate_codebook(blocks_array, effective_size, kmeans_max_iter)
        
        # åˆ›å»ºå®Œæ•´çš„256é¡¹ç æœ¬
        full_codebook = np.zeros((codebook_size, BYTES_PER_BLOCK), dtype=np.uint8)
        full_codebook[:effective_size] = codebook[:effective_size]
        # ç¬¬255é¡¹ï¼ˆç´¢å¼•255/0xFFï¼‰å¤åˆ¶æœ€åä¸€ä¸ªæœ‰æ•ˆé¡¹ä½œä¸ºå ä½
        if effective_size > 0:
            full_codebook[255] = full_codebook[effective_size - 1]
    else:
        full_codebook = np.zeros((codebook_size, BYTES_PER_BLOCK), dtype=np.uint8)
    
    return full_codebook

def quantize_blocks_unified(blocks_data: np.ndarray, codebook: np.ndarray) -> np.ndarray:
    """ä½¿ç”¨ç»Ÿä¸€ç è¡¨å¯¹å—è¿›è¡Œé‡åŒ–ï¼ˆé¿å…äº§ç”Ÿ0xFFï¼‰"""
    if len(blocks_data) == 0:
        return np.array([], dtype=np.uint8)
    
    # åªä½¿ç”¨å‰255é¡¹è¿›è¡Œé‡åŒ–
    effective_codebook = codebook[:EFFECTIVE_UNIFIED_CODEBOOK_SIZE]
    
    blocks_for_clustering = convert_blocks_for_clustering(blocks_data)
    codebook_for_clustering = convert_blocks_for_clustering(effective_codebook)
    
    blocks_expanded = blocks_for_clustering[:, np.newaxis, :]
    codebook_expanded = codebook_for_clustering[np.newaxis, :, :]
    
    diff = blocks_expanded - codebook_expanded
    squared_distances = np.sum(diff * diff, axis=2)
    indices = np.argmin(squared_distances, axis=1).astype(np.uint8)
    
    return indices

def convert_blocks_for_clustering(blocks_data: np.ndarray) -> np.ndarray:
    """å°†å—æ•°æ®è½¬æ¢ä¸ºæ­£ç¡®çš„èšç±»æ ¼å¼"""
    if len(blocks_data) == 0:
        return blocks_data.astype(np.float32)
    
    if blocks_data.ndim > 2:
        blocks_data = blocks_data.reshape(-1, BYTES_PER_BLOCK)
    
    blocks_float = blocks_data.astype(np.float32)
    
    for i in range(4, BYTES_PER_BLOCK):
        blocks_float[:, i] = blocks_data[:, i].view(np.int8).astype(np.float32)
    
    return blocks_float

def convert_codebook_from_clustering(codebook_float: np.ndarray) -> np.ndarray:
    """å°†èšç±»ç»“æœè½¬æ¢å›æ­£ç¡®çš„å—æ ¼å¼"""
    codebook = np.zeros_like(codebook_float, dtype=np.uint8)
    
    codebook[:, 0:4] = np.clip(codebook_float[:, 0:4].round(), 0, 255).astype(np.uint8)
    
    for i in range(4, BYTES_PER_BLOCK):
        clipped_values = np.clip(codebook_float[:, i].round(), -128, 127).astype(np.int8)
        codebook[:, i] = clipped_values.view(np.uint8)
    
    return codebook

def encode_strip_i_frame_unified(blocks: np.ndarray, unified_codebook: np.ndarray, 
                                block_types: dict) -> bytes:
    """ç¼–ç æ¡å¸¦Iå¸§ï¼ˆç»Ÿä¸€ç æœ¬ï¼‰"""
    data = bytearray()
    data.append(FRAME_TYPE_I)
    
    if blocks.size > 0:
        blocks_h, blocks_w = blocks.shape[:2]
        big_blocks_h = blocks_h // 2
        big_blocks_w = blocks_w // 2
        
        # å­˜å‚¨ç»Ÿä¸€ç æœ¬
        data.extend(unified_codebook.flatten().tobytes())
        
        # æŒ‰4x4å¤§å—çš„é¡ºåºç¼–ç 
        for big_by in range(big_blocks_h):
            for big_bx in range(big_blocks_w):
                if (big_by, big_bx) in block_types:
                    block_type, block_indices = block_types[(big_by, big_bx)]
                    
                    if block_type == 'color':
                        # è‰²å—ï¼šæ ‡è®°0xFF + 1ä¸ªç æœ¬ç´¢å¼•
                        data.append(0xFF)
                        
                        # ä»åŸå§‹blocksé‡å»ºå¹³å‡å—
                        blocks_4x4 = []
                        for sub_by in range(2):
                            for sub_bx in range(2):
                                by = big_by * 2 + sub_by
                                bx = big_bx * 2 + sub_bx
                                if by < blocks_h and bx < blocks_w:
                                    blocks_4x4.append(blocks[by, bx])
                        
                        avg_block = np.mean(blocks_4x4, axis=0).round().astype(np.uint8)
                        for i in range(4, 7):
                            avg_val = np.mean([b[i].view(np.int8) for b in blocks_4x4])
                            avg_block[i] = np.clip(avg_val, -128, 127).astype(np.int8).view(np.uint8)
                        
                        unified_idx = quantize_blocks_unified(avg_block.reshape(1, -1), unified_codebook)[0]
                        data.append(unified_idx)
                    else:
                        # çº¹ç†å—ï¼š4ä¸ªç æœ¬ç´¢å¼•
                        for sub_by in range(2):
                            for sub_bx in range(2):
                                by = big_by * 2 + sub_by
                                bx = big_bx * 2 + sub_bx
                                if by < blocks_h and bx < blocks_w:
                                    block = blocks[by, bx]
                                    unified_idx = quantize_blocks_unified(block.reshape(1, -1), unified_codebook)[0]
                                    data.append(unified_idx)
                                else:
                                    data.append(0)
    
    return bytes(data)

def encode_strip_differential_unified(current_blocks: np.ndarray, prev_blocks: np.ndarray,
                                     unified_codebook: np.ndarray, block_types: dict, 
                                     diff_threshold: float, force_i_threshold: float = 0.7) -> tuple:
    """å·®åˆ†ç¼–ç å½“å‰æ¡å¸¦ï¼ˆç»Ÿä¸€ç æœ¬ï¼‰"""
    if prev_blocks is None or current_blocks.shape != prev_blocks.shape:
        i_frame_data = encode_strip_i_frame_unified(current_blocks, unified_codebook, block_types)
        return i_frame_data, True, 0, 0, 0
    
    blocks_h, blocks_w = current_blocks.shape[:2]
    total_blocks = blocks_h * blocks_w
    
    if total_blocks == 0:
        return b'', True, 0, 0, 0
    
    # è®¡ç®—å—å·®å¼‚
    current_flat = current_blocks.reshape(-1, BYTES_PER_BLOCK)
    prev_flat = prev_blocks.reshape(-1, BYTES_PER_BLOCK)
    
    y_current = current_flat[:, :4].astype(np.int16)
    y_prev = prev_flat[:, :4].astype(np.int16)
    y_diff = np.abs(y_current - y_prev)
    block_diffs_flat = y_diff.mean(axis=1)
    block_diffs = block_diffs_flat.reshape(blocks_h, blocks_w)
    
    big_blocks_h = blocks_h // 2
    big_blocks_w = blocks_w // 2
    
    # è®¡ç®—åŒºåŸŸæ•°é‡
    zones_count = (big_blocks_h + ZONE_HEIGHT_BIG_BLOCKS - 1) // ZONE_HEIGHT_BIG_BLOCKS
    if zones_count > 8:
        zones_count = 8
    
    # æŒ‰åŒºåŸŸç»„ç»‡æ›´æ–°
    zone_detail_updates = [[] for _ in range(zones_count)]
    zone_color_updates = [[] for _ in range(zones_count)]
    total_updated_blocks = 0
    
    for big_by in range(big_blocks_h):
        for big_bx in range(big_blocks_w):
            needs_update = False
            positions = [
                (big_by * 2, big_bx * 2),
                (big_by * 2, big_bx * 2 + 1),
                (big_by * 2 + 1, big_bx * 2),
                (big_by * 2 + 1, big_bx * 2 + 1)
            ]
            
            for by, bx in positions:
                if by < blocks_h and bx < blocks_w:
                    if block_diffs[by, bx] > diff_threshold:
                        needs_update = True
                        break
            
            if needs_update:
                # è®¡ç®—å±äºå“ªä¸ªåŒºåŸŸ
                zone_idx = min(big_by // ZONE_HEIGHT_BIG_BLOCKS, zones_count - 1)
                # è®¡ç®—åœ¨åŒºåŸŸå†…çš„ç›¸å¯¹åæ ‡
                zone_relative_by = big_by % ZONE_HEIGHT_BIG_BLOCKS
                zone_relative_idx = zone_relative_by * big_blocks_w + big_bx
                
                total_updated_blocks += 4
                
                if (big_by, big_bx) in block_types and block_types[(big_by, big_bx)] == 'color':
                    # è‰²å—æ›´æ–°
                    blocks_4x4 = []
                    for by, bx in positions:
                        if by < blocks_h and bx < blocks_w:
                            blocks_4x4.append(current_blocks[by, bx])
                    
                    avg_block = np.mean(blocks_4x4, axis=0).round().astype(np.uint8)
                    for i in range(4, 7):
                        avg_val = np.mean([b[i].view(np.int8) for b in blocks_4x4])
                        avg_block[i] = np.clip(avg_val, -128, 127).astype(np.int8).view(np.uint8)
                    
                    color_idx = quantize_blocks_unified(avg_block.reshape(1, -1), unified_codebook)[0]
                    zone_color_updates[zone_idx].append((zone_relative_idx, color_idx))
                else:
                    # çº¹ç†å—æ›´æ–°
                    indices = []
                    for by, bx in positions:
                        if by < blocks_h and bx < blocks_w:
                            block = current_blocks[by, bx]
                            unified_idx = quantize_blocks_unified(block.reshape(1, -1), unified_codebook)[0]
                            indices.append(unified_idx)
                        else:
                            indices.append(0)
                    zone_detail_updates[zone_idx].append((zone_relative_idx, indices))
    
    # åˆ¤æ–­æ˜¯å¦éœ€è¦Iå¸§
    update_ratio = total_updated_blocks / total_blocks
    if update_ratio > force_i_threshold:
        i_frame_data = encode_strip_i_frame_unified(current_blocks, unified_codebook, block_types)
        return i_frame_data, True, 0, 0, 0
    
    # ç¼–ç På¸§
    data = bytearray()
    data.append(FRAME_TYPE_P)
    
    # ç»Ÿè®¡ä½¿ç”¨çš„åŒºåŸŸæ•°é‡
    used_zones = 0
    total_color_updates = 0
    total_detail_updates = 0
    
    # ç”ŸæˆåŒºåŸŸbitmap
    zone_bitmap = 0
    for zone_idx in range(zones_count):
        if zone_detail_updates[zone_idx] or zone_color_updates[zone_idx]:
            zone_bitmap |= (1 << zone_idx)
            used_zones += 1
            total_color_updates += len(zone_color_updates[zone_idx])
            total_detail_updates += len(zone_detail_updates[zone_idx])
    
    data.append(zone_bitmap)
    
    # æŒ‰åŒºåŸŸç¼–ç æ›´æ–°
    for zone_idx in range(zones_count):
        if zone_bitmap & (1 << zone_idx):
            detail_updates = zone_detail_updates[zone_idx]
            color_updates = zone_color_updates[zone_idx]
            
            data.append(len(detail_updates))
            data.append(len(color_updates))
            
            # å­˜å‚¨çº¹ç†å—æ›´æ–°
            for relative_idx, indices in detail_updates:
                data.append(relative_idx)
                for idx in indices:
                    data.append(idx)
            
            # å­˜å‚¨è‰²å—æ›´æ–°
            for relative_idx, unified_idx in color_updates:
                data.append(relative_idx)
                data.append(unified_idx)
    
    return bytes(data), False, used_zones, total_color_updates, total_detail_updates

def generate_gop_unified_codebooks(frames: list, strip_count: int, i_frame_interval: int,
                                  variance_threshold: float, codebook_size: int = DEFAULT_UNIFIED_CODEBOOK_SIZE,
                                  kmeans_max_iter: int = 100) -> dict:
    """ä¸ºæ¯ä¸ªGOPç”Ÿæˆç»Ÿä¸€ç æœ¬"""
    print("æ­£åœ¨ä¸ºæ¯ä¸ªGOPç”Ÿæˆç»Ÿä¸€ç æœ¬...")
    
    gop_codebooks = {}
    
    i_frame_positions = []
    for frame_idx in range(len(frames)):
        if frame_idx % i_frame_interval == 0:
            i_frame_positions.append(frame_idx)
    
    for gop_idx, gop_start in enumerate(i_frame_positions):
        if gop_idx + 1 < len(i_frame_positions):
            gop_end = i_frame_positions[gop_idx + 1]
        else:
            gop_end = len(frames)
        
        print(f"  å¤„ç†GOP {gop_idx}: å¸§ {gop_start} åˆ° {gop_end-1}")
        
        gop_codebooks[gop_start] = []
        
        for strip_idx in range(strip_count):
            all_blocks = []
            block_types_list = []
            
            for frame_idx in range(gop_start, gop_end):
                strip_blocks = frames[frame_idx][strip_idx]
                if strip_blocks.size > 0:
                    frame_blocks, block_types = classify_4x4_blocks_unified(strip_blocks, variance_threshold)
                    all_blocks.extend(frame_blocks)
                    block_types_list.append((frame_idx, block_types))
            
            # ç”Ÿæˆç»Ÿä¸€ç æœ¬
            unified_codebook = generate_unified_codebook(all_blocks, codebook_size, kmeans_max_iter)
            
            gop_codebooks[gop_start].append({
                'unified_codebook': unified_codebook,
                'block_types_list': block_types_list,
                'total_blocks_count': len(all_blocks)
            })
            
            # ç»Ÿè®¡è‰²å—å’Œçº¹ç†å—æ•°é‡
            color_count = 0
            detail_count = 0
            for _, block_types in block_types_list:
                for (big_by, big_bx), (block_type, _) in block_types.items():
                    if block_type == 'color':
                        color_count += 1
                    else:
                        detail_count += 1
            
            print(f"    æ¡å¸¦{strip_idx}: æ€»å—æ•°{len(all_blocks)}, è‰²å—{color_count}, çº¹ç†å—{detail_count}")
    
    return gop_codebooks

def generate_big_block_codebook(detail_big_blocks: list, unified_codebook: np.ndarray, 
                               error_threshold: float = 10.0, 
                               kmeans_max_iter: int = 100) -> tuple:
    """ç”Ÿæˆçº¹ç†å¤§å—ç´¢å¼•ç è¡¨ï¼ˆä½¿ç”¨ç›´æ–¹å›¾Top-Kæ–¹æ³•ï¼‰"""
    if not detail_big_blocks:
        # ç©ºçš„å¤§å—ç è¡¨
        big_block_codebook = np.zeros((BIG_BLOCK_CODEBOOK_SIZE, 4), dtype=np.uint8)
        return big_block_codebook, []
    
    # Step 1: æ”¶é›†æ‰€æœ‰4-tuple (a,b,c,d) å¹¶ç»Ÿè®¡é¢‘æ¬¡
    from collections import Counter
    tuple_counter = Counter()
    big_block_tuples = []
    big_block_vectors = []  # åœ¨YUVç©ºé—´çš„å‘é‡è¡¨ç¤º
    
    for big_block_data in detail_big_blocks:
        # é‡åŒ–ä¸ºç»Ÿä¸€ç æœ¬ç´¢å¼•
        indices = quantize_blocks_unified(big_block_data, unified_codebook)
        tuple_key = tuple(indices)
        tuple_counter[tuple_key] += 1
        big_block_tuples.append(indices)
        
        # Step 2: æŠ•å½±åˆ°YUVè¿ç»­ç©ºé—´ï¼ˆç”¨äºè¯¯å·®è®¡ç®—ï¼‰
        yuv_vector = []
        for idx in indices:
            block = unified_codebook[idx]
            # å°†7å­—èŠ‚å—è½¬æ¢ä¸ºYUVè¿ç»­å€¼
            y_values = block[:4].astype(np.float32) * 2  # æ¢å¤Yå€¼ï¼ˆä¹‹å‰>>1äº†ï¼‰
            d_r = block[4].view(np.int8).astype(np.float32)
            d_g = block[5].view(np.int8).astype(np.float32)  
            d_b = block[6].view(np.int8).astype(np.float32)
            
            # é‡å»ºCb, Cr
            cb = d_b
            cr = d_r
            # d_g = (-(cb>>1) - cr) >> 1ï¼Œåæ¨ï¼š
            # cb_half = cb >> 1 = cb // 2
            # cr_plus_cb_half = cr + cb_half
            # d_g = -cr_plus_cb_half >> 1
            # æ‰€ä»¥ï¼šcr_plus_cb_half = -d_g << 1 = -d_g * 2
            # ä½†è¿™æ ·åæ¨å¯èƒ½æœ‰ç²¾åº¦æŸå¤±ï¼Œæˆ‘ä»¬ç›´æ¥ç”¨d_r, d_g, d_bä½œä¸ºè‰²åº¦åˆ†é‡
            
            # æ„å»º12ç»´å‘é‡ï¼š4ä¸ªYå€¼ + 4ä¸ªCbå€¼ + 4ä¸ªCrå€¼
            # ä¸ºç®€åŒ–ï¼Œæˆ‘ä»¬å¯¹æ¯ä¸ª2x2å—ç”¨ç›¸åŒçš„Cb,Cr
            block_vector = np.concatenate([
                y_values,  # 4ä¸ªYå€¼
                np.full(4, cb, dtype=np.float32),  # 4ä¸ªCbå€¼
                np.full(4, cr, dtype=np.float32)   # 4ä¸ªCrå€¼
            ])
            yuv_vector.extend(block_vector)
        
        big_block_vectors.append(np.array(yuv_vector))
    
    # Step 3: é€‰æ‹©Top-Kä¸ªæœ€é¢‘ç¹çš„4-tupleä½œä¸ºç å­—
    most_common = tuple_counter.most_common(EFFECTIVE_BIG_BLOCK_CODEBOOK_SIZE)
    
    # æ„å»ºå¤§å—ç è¡¨
    big_block_codebook = np.zeros((BIG_BLOCK_CODEBOOK_SIZE, 4), dtype=np.uint8)
    tuple_to_index = {}
    
    for i, (tuple_key, count) in enumerate(most_common):
        big_block_codebook[i] = np.array(tuple_key)
        tuple_to_index[tuple_key] = i
    
    # å¡«å……å‰©ä½™ä½ç½®ï¼ˆè™½ç„¶ä¸ä¼šç”¨åˆ°ï¼‰
    if len(most_common) > 0:
        last_tuple = most_common[-1][0]
        for i in range(len(most_common), EFFECTIVE_BIG_BLOCK_CODEBOOK_SIZE):
            big_block_codebook[i] = np.array(last_tuple)
    
    # Step 4: ä¸ºæ¯ä¸ªå¤§å—è®¡ç®—æ˜¯å¦ä½¿ç”¨å¤§å—ç´¢å¼•
    big_block_usage = []
    
    for i, (indices, big_vector) in enumerate(zip(big_block_tuples, big_block_vectors)):
        tuple_key = tuple(indices)
        
        if tuple_key in tuple_to_index:
            # å‘½ä¸­ç è¡¨ï¼Œè®¡ç®—YUVç©ºé—´è¯¯å·®
            big_idx = tuple_to_index[tuple_key]
            
            # é‡å»ºç è¡¨å¯¹åº”çš„YUVå‘é‡
            reconstructed_vector = []
            for idx in big_block_codebook[big_idx]:
                block = unified_codebook[idx]
                y_values = block[:4].astype(np.float32) * 2
                d_r = block[4].view(np.int8).astype(np.float32)
                d_g = block[5].view(np.int8).astype(np.float32)
                d_b = block[6].view(np.int8).astype(np.float32)
                
                cb = d_b
                cr = d_r
                
                block_vector = np.concatenate([
                    y_values,
                    np.full(4, cb, dtype=np.float32),
                    np.full(4, cr, dtype=np.float32)
                ])
                reconstructed_vector.extend(block_vector)
            
            reconstructed_vector = np.array(reconstructed_vector)
            
            # è®¡ç®—L2è¯¯å·®
            error = np.sum((big_vector - reconstructed_vector)**2)
            use_big_block = error <= error_threshold
            big_block_usage.append((use_big_block, big_idx, indices))
        else:
            # æœªå‘½ä¸­ç è¡¨ï¼Œä½¿ç”¨åŸå§‹ç´¢å¼•
            big_block_usage.append((False, 0, indices))
    
    print(f"    å¤§å—ç è¡¨ç»Ÿè®¡: æ”¶é›†{len(big_block_tuples)}ä¸ªå¤§å—, å”¯ä¸€{len(tuple_counter)}ä¸ª, Top-{len(most_common)}ä¸ª")
    
    return big_block_codebook, big_block_usage

def classify_4x4_blocks_with_big_block(blocks: np.ndarray, variance_threshold: float = 5.0) -> tuple:
    """å°†4x4å—åˆ†ç±»ï¼Œå¹¶æ”¶é›†çº¹ç†å¤§å—ç”¨äºç”Ÿæˆå¤§å—ç´¢å¼•è¡¨"""
    blocks_h, blocks_w = blocks.shape[:2]
    big_blocks_h = blocks_h // 2
    big_blocks_w = blocks_w // 2
    
    all_blocks = []  # æ‰€æœ‰2x2å—
    block_types = {}   # è®°å½•æ¯ä¸ª4x4å—çš„ç±»å‹
    detail_big_blocks = []  # çº¹ç†å¤§å—æ•°æ®
    
    for big_by in range(big_blocks_h):
        for big_bx in range(big_blocks_w):
            # æ”¶é›†4x4å¤§å—å†…çš„4ä¸ª2x2å°å—
            blocks_4x4 = []
            for sub_by in range(2):
                for sub_bx in range(2):
                    by = big_by * 2 + sub_by
                    bx = big_bx * 2 + sub_bx
                    if by < blocks_h and bx < blocks_w:
                        blocks_4x4.append(blocks[by, bx])
                    else:
                        blocks_4x4.append(np.zeros(BYTES_PER_BLOCK, dtype=np.uint8))
            
            # è®¡ç®—æ–¹å·®åˆ¤æ–­æ˜¯å¦ä¸ºçº¯è‰²å—
            variance = calculate_block_variance(blocks_4x4)
            
            if variance < variance_threshold:
                # çº¯è‰²å—
                avg_block = np.mean(blocks_4x4, axis=0).round().astype(np.uint8)
                for i in range(4, 7):
                    avg_val = np.mean([b[i].view(np.int8) for b in blocks_4x4])
                    avg_block[i] = np.clip(avg_val, -128, 127).astype(np.int8).view(np.uint8)
                
                block_idx = len(all_blocks)
                all_blocks.append(avg_block)
                block_types[(big_by, big_bx)] = ('color', [block_idx])
            else:
                # çº¹ç†å—ï¼šæ”¶é›†å¤§å—æ•°æ®
                block_indices = []
                for block in blocks_4x4:
                    block_idx = len(all_blocks)
                    all_blocks.append(block)
                    block_indices.append(block_idx)
                block_types[(big_by, big_bx)] = ('detail', block_indices)
                detail_big_blocks.append(np.array(blocks_4x4))
    
    return all_blocks, block_types, detail_big_blocks

def encode_strip_i_frame_unified_with_big_block(blocks: np.ndarray, unified_codebook: np.ndarray, 
                                               big_block_codebook: np.ndarray, block_types: dict,
                                               big_block_usage: list) -> bytes:
    """ç¼–ç æ¡å¸¦Iå¸§ï¼ˆç»Ÿä¸€ç æœ¬ + å¤§å—ç´¢å¼•ï¼‰"""
    data = bytearray()
    data.append(FRAME_TYPE_I)
    
    if blocks.size > 0:
        blocks_h, blocks_w = blocks.shape[:2]
        big_blocks_h = blocks_h // 2
        big_blocks_w = blocks_w // 2
        
        # å­˜å‚¨ç»Ÿä¸€ç æœ¬
        data.extend(unified_codebook.flatten().tobytes())
        
        # å­˜å‚¨å¤§å—ç´¢å¼•ç è¡¨
        data.extend(big_block_codebook.flatten().tobytes())
        
        # æŒ‰4x4å¤§å—çš„é¡ºåºç¼–ç 
        big_block_usage_idx = 0
        for big_by in range(big_blocks_h):
            for big_bx in range(big_blocks_w):
                if (big_by, big_bx) in block_types:
                    block_type, block_indices = block_types[(big_by, big_bx)]
                    
                    if block_type == 'color':
                        # è‰²å—ï¼šæ ‡è®°0xFF + 1ä¸ªç æœ¬ç´¢å¼•
                        data.append(COLOR_BLOCK_MARKER)
                        
                        # ä»åŸå§‹blocksé‡å»ºå¹³å‡å—
                        blocks_4x4 = []
                        for sub_by in range(2):
                            for sub_bx in range(2):
                                by = big_by * 2 + sub_by
                                bx = big_bx * 2 + sub_bx
                                if by < blocks_h and bx < blocks_w:
                                    blocks_4x4.append(blocks[by, bx])
                        
                        avg_block = np.mean(blocks_4x4, axis=0).round().astype(np.uint8)
                        for i in range(4, 7):
                            avg_val = np.mean([b[i].view(np.int8) for b in blocks_4x4])
                            avg_block[i] = np.clip(avg_val, -128, 127).astype(np.int8).view(np.uint8)
                        
                        unified_idx = quantize_blocks_unified(avg_block.reshape(1, -1), unified_codebook)[0]
                        data.append(unified_idx)
                    else:
                        # çº¹ç†å—ï¼šæ£€æŸ¥æ˜¯å¦ä½¿ç”¨å¤§å—ç´¢å¼•ä¼˜åŒ–
                        if big_block_usage_idx < len(big_block_usage):
                            use_big_block, best_big_idx, original_indices = big_block_usage[big_block_usage_idx]
                            big_block_usage_idx += 1
                            
                            if use_big_block:
                                # ä½¿ç”¨å¤§å—ç´¢å¼•ï¼šFEæ ‡è®° + 1å­—èŠ‚å¤§å—ç´¢å¼•
                                data.append(COMPLEX_TEXTURE_MARKER)
                                data.append(best_big_idx)
                            else:
                                # é»˜è®¤å°å—æ¨¡å¼ï¼šç›´æ¥4ä¸ªç»Ÿä¸€ç æœ¬ç´¢å¼•
                                for idx in original_indices:
                                    data.append(idx)
                        else:
                            # å…œåº•ï¼šç›´æ¥é‡åŒ–ä¸º4ä¸ªç´¢å¼•
                            for sub_by in range(2):
                                for sub_bx in range(2):
                                    by = big_by * 2 + sub_by
                                    bx = big_bx * 2 + sub_bx
                                    if by < blocks_h and bx < blocks_w:
                                        block = blocks[by, bx]
                                        unified_idx = quantize_blocks_unified(block.reshape(1, -1), unified_codebook)[0]
                                        data.append(unified_idx)
                                    else:
                                        data.append(0)
    
    return bytes(data)

def encode_strip_differential_unified_with_big_block(current_blocks: np.ndarray, prev_blocks: np.ndarray,
                                                   unified_codebook: np.ndarray, big_block_codebook: np.ndarray,
                                                   block_types: dict, big_block_usage: list,
                                                   diff_threshold: float, force_i_threshold: float = 0.7) -> tuple:
    """å·®åˆ†ç¼–ç å½“å‰æ¡å¸¦ï¼ˆç»Ÿä¸€ç æœ¬ + å¤§å—ç´¢å¼•ï¼‰"""
    if prev_blocks is None or current_blocks.shape != prev_blocks.shape:
        i_frame_data = encode_strip_i_frame_unified_with_big_block(
            current_blocks, unified_codebook, big_block_codebook, block_types, big_block_usage)
        return i_frame_data, True, 0, 0, 0
    
    blocks_h, blocks_w = current_blocks.shape[:2]
    total_blocks = blocks_h * blocks_w
    
    if total_blocks == 0:
        return b'', True, 0, 0, 0
    
    # è®¡ç®—å—å·®å¼‚
    current_flat = current_blocks.reshape(-1, BYTES_PER_BLOCK)
    prev_flat = prev_blocks.reshape(-1, BYTES_PER_BLOCK)
    
    y_current = current_flat[:, :4].astype(np.int16)
    y_prev = prev_flat[:, :4].astype(np.int16)
    y_diff = np.abs(y_current - y_prev)
    block_diffs_flat = y_diff.mean(axis=1)
    block_diffs = block_diffs_flat.reshape(blocks_h, blocks_w)
    
    big_blocks_h = blocks_h // 2
    big_blocks_w = blocks_w // 2
    
    # è®¡ç®—åŒºåŸŸæ•°é‡
    zones_count = (big_blocks_h + ZONE_HEIGHT_BIG_BLOCKS - 1) // ZONE_HEIGHT_BIG_BLOCKS
    if zones_count > 8:
        zones_count = 8
    
    # æŒ‰åŒºåŸŸç»„ç»‡æ›´æ–°
    zone_detail_updates = [[] for _ in range(zones_count)]
    zone_color_updates = [[] for _ in range(zones_count)]
    total_updated_blocks = 0
    
    # é‡å»ºå¤§å—ä½¿ç”¨ä¿¡æ¯çš„æ˜ å°„
    big_block_usage_map = {}
    big_block_usage_idx = 0
    for big_by in range(big_blocks_h):
        for big_bx in range(big_blocks_w):
            if (big_by, big_bx) in block_types:
                block_type, _ = block_types[(big_by, big_bx)]
                if block_type == 'detail':
                    if big_block_usage_idx < len(big_block_usage):
                        big_block_usage_map[(big_by, big_bx)] = big_block_usage[big_block_usage_idx]
                        big_block_usage_idx += 1
    
    for big_by in range(big_blocks_h):
        for big_bx in range(big_blocks_w):
            needs_update = False
            positions = [
                (big_by * 2, big_bx * 2),
                (big_by * 2, big_bx * 2 + 1),
                (big_by * 2 + 1, big_bx * 2),
                (big_by * 2 + 1, big_bx * 2 + 1)
            ]
            
            for by, bx in positions:
                if by < blocks_h and bx < blocks_w:
                    if block_diffs[by, bx] > diff_threshold:
                        needs_update = True
                        break
            
            if needs_update:
                zone_idx = min(big_by // ZONE_HEIGHT_BIG_BLOCKS, zones_count - 1)
                zone_relative_by = big_by % ZONE_HEIGHT_BIG_BLOCKS
                zone_relative_idx = zone_relative_by * big_blocks_w + big_bx
                
                total_updated_blocks += 4
                
                if (big_by, big_bx) in block_types:
                    block_type, _ = block_types[(big_by, big_bx)]
                    
                    if block_type == 'color':
                        # è‰²å—æ›´æ–°
                        blocks_4x4 = []
                        for by, bx in positions:
                            if by < blocks_h and bx < blocks_w:
                                blocks_4x4.append(current_blocks[by, bx])
                        
                        avg_block = np.mean(blocks_4x4, axis=0).round().astype(np.uint8)
                        for i in range(4, 7):
                            avg_val = np.mean([b[i].view(np.int8) for b in blocks_4x4])
                            avg_block[i] = np.clip(avg_val, -128, 127).astype(np.int8).view(np.uint8)
                        
                        unified_idx = quantize_blocks_unified(avg_block.reshape(1, -1), unified_codebook)[0]
                        zone_color_updates[zone_idx].append((zone_relative_idx, unified_idx))
                    else:
                        # çº¹ç†å—æ›´æ–°ï¼šæ£€æŸ¥å¤§å—ç´¢å¼•ä¼˜åŒ–
                        if (big_by, big_bx) in big_block_usage_map:
                            use_big_block, best_big_idx, original_indices = big_block_usage_map[(big_by, big_bx)]
                            
                            if use_big_block:
                                # ä½¿ç”¨å¤§å—ç´¢å¼•
                                zone_detail_updates[zone_idx].append((zone_relative_idx, 'big_block', best_big_idx))
                            else:
                                # ä½¿ç”¨å°å—æ¨¡å¼
                                zone_detail_updates[zone_idx].append((zone_relative_idx, 'small_blocks', original_indices))
                        else:
                            # å…œåº•ï¼šç›´æ¥é‡åŒ–
                            indices = []
                            for by, bx in positions:
                                if by < blocks_h and bx < blocks_w:
                                    block = current_blocks[by, bx]
                                    unified_idx = quantize_blocks_unified(block.reshape(1, -1), unified_codebook)[0]
                                    indices.append(unified_idx)
                                else:
                                    indices.append(0)
                            zone_detail_updates[zone_idx].append((zone_relative_idx, 'small_blocks', indices))
    
    # åˆ¤æ–­æ˜¯å¦éœ€è¦Iå¸§
    update_ratio = total_updated_blocks / total_blocks
    if update_ratio > force_i_threshold:
        i_frame_data = encode_strip_i_frame_unified_with_big_block(
            current_blocks, unified_codebook, big_block_codebook, block_types, big_block_usage)
        return i_frame_data, True, 0, 0, 0
    
    # ç¼–ç På¸§
    data = bytearray()
    data.append(FRAME_TYPE_P)
    
    # ç»Ÿè®¡ä½¿ç”¨çš„åŒºåŸŸæ•°é‡
    used_zones = 0
    total_color_updates = 0
    total_detail_updates = 0
    
    # ç”ŸæˆåŒºåŸŸbitmap
    zone_bitmap = 0
    for zone_idx in range(zones_count):
        if zone_detail_updates[zone_idx] or zone_color_updates[zone_idx]:
            zone_bitmap |= (1 << zone_idx)
            used_zones += 1
            total_color_updates += len(zone_color_updates[zone_idx])
            total_detail_updates += len(zone_detail_updates[zone_idx])
    
    data.append(zone_bitmap)
    
    # æŒ‰åŒºåŸŸç¼–ç æ›´æ–°
    for zone_idx in range(zones_count):
        if zone_bitmap & (1 << zone_idx):
            detail_updates = zone_detail_updates[zone_idx]
            color_updates = zone_color_updates[zone_idx]
            
            data.append(len(detail_updates))
            data.append(len(color_updates))
            
            # å­˜å‚¨çº¹ç†å—æ›´æ–°
            for relative_idx, update_type, *indices in detail_updates:
                data.append(relative_idx)
                if update_type == 'big_block':
                    # å¤§å—ç´¢å¼•æ¨¡å¼ï¼šFEæ ‡è®° + 1å­—èŠ‚å¤§å—ç´¢å¼•
                    data.append(COMPLEX_TEXTURE_MARKER)
                    data.append(indices[0])
                else:  # update_type == 'small_blocks'
                    # å°å—æ¨¡å¼ï¼šç›´æ¥4ä¸ªç»Ÿä¸€ç æœ¬ç´¢å¼•
                    for idx in indices[0]:
                        data.append(idx)
            
            # å­˜å‚¨è‰²å—æ›´æ–°
            for relative_idx, unified_idx in color_updates:
                data.append(relative_idx)
                data.append(COLOR_BLOCK_MARKER)
                data.append(unified_idx)
    
    return bytes(data), False, used_zones, total_color_updates, total_detail_updates

def generate_gop_unified_codebooks_with_big_block(frames: list, strip_count: int, i_frame_interval: int,
                                                 variance_threshold: float, 
                                                 codebook_size: int = DEFAULT_UNIFIED_CODEBOOK_SIZE,
                                                 error_threshold: float = 10.0,
                                                 kmeans_max_iter: int = 100) -> dict:
    """ä¸ºæ¯ä¸ªGOPç”Ÿæˆç»Ÿä¸€ç æœ¬å’Œå¤§å—ç´¢å¼•ç è¡¨"""
    print("æ­£åœ¨ä¸ºæ¯ä¸ªGOPç”Ÿæˆç»Ÿä¸€ç æœ¬å’Œå¤§å—ç´¢å¼•ç è¡¨...")
    
    gop_codebooks = {}
    
    i_frame_positions = []
    for frame_idx in range(len(frames)):
        if frame_idx % i_frame_interval == 0:
            i_frame_positions.append(frame_idx)
    
    for gop_idx, gop_start in enumerate(i_frame_positions):
        if gop_idx + 1 < len(i_frame_positions):
            gop_end = i_frame_positions[gop_idx + 1]
        else:
            gop_end = len(frames)
        
        print(f"  å¤„ç†GOP {gop_idx}: å¸§ {gop_start} åˆ° {gop_end-1}")
        
        gop_codebooks[gop_start] = []
        
        for strip_idx in range(strip_count):
            all_blocks = []
            block_types_list = []
            all_detail_big_blocks = []
            
            for frame_idx in range(gop_start, gop_end):
                strip_blocks = frames[frame_idx][strip_idx]
                if strip_blocks.size > 0:
                    frame_blocks, block_types, detail_big_blocks = classify_4x4_blocks_with_big_block(
                        strip_blocks, variance_threshold)
                    all_blocks.extend(frame_blocks)
                    all_detail_big_blocks.extend(detail_big_blocks)
                    block_types_list.append((frame_idx, block_types))
            
            # ç”Ÿæˆç»Ÿä¸€ç æœ¬
            unified_codebook = generate_unified_codebook(all_blocks, codebook_size, kmeans_max_iter)
            
            # ç”Ÿæˆå¤§å—ç´¢å¼•ç è¡¨
            big_block_codebook, big_block_usage_all = generate_big_block_codebook(
                all_detail_big_blocks, unified_codebook, error_threshold, kmeans_max_iter)
            
            # ä¸ºæ¯ä¸€å¸§åˆ†é…å¤§å—ä½¿ç”¨ä¿¡æ¯
            frame_big_block_usage = {}
            usage_idx = 0
            
            for frame_idx, block_types in block_types_list:
                frame_usage = []
                # æŒ‰4x4å¤§å—çš„é¡ºåºæ”¶é›†usageä¿¡æ¯
                big_blocks_h = None
                big_blocks_w = None
                
                # ç¡®å®šå¤§å—ç½‘æ ¼å°ºå¯¸
                if block_types:
                    max_big_by = max(pos[0] for pos in block_types.keys()) + 1
                    max_big_bx = max(pos[1] for pos in block_types.keys()) + 1
                    big_blocks_h = max_big_by
                    big_blocks_w = max_big_bx
                
                if big_blocks_h and big_blocks_w:
                    for big_by in range(big_blocks_h):
                        for big_bx in range(big_blocks_w):
                            if (big_by, big_bx) in block_types:
                                block_type, _ = block_types[(big_by, big_bx)]
                                if block_type == 'detail':
                                    if usage_idx < len(big_block_usage_all):
                                        frame_usage.append(big_block_usage_all[usage_idx])
                                        usage_idx += 1
            
            gop_codebooks[gop_start].append({
                'unified_codebook': unified_codebook,
                'big_block_codebook': big_block_codebook,
                'block_types_list': block_types_list,
                'big_block_usage': frame_big_block_usage,
                'total_blocks_count': len(all_blocks),
                'detail_big_blocks_count': len(all_detail_big_blocks)
            })
            
            # ç»Ÿè®¡è‰²å—å’Œçº¹ç†å—æ•°é‡
            color_count = 0
            detail_count = 0
            for _, block_types in block_types_list:
                for (big_by, big_bx), (block_type, _) in block_types.items():
                    if block_type == 'color':
                        color_count += 1
                    else:
                        detail_count += 1
            
            # ç»Ÿè®¡å¤§å—ç´¢å¼•ä½¿ç”¨ç‡
            big_block_usage_count = sum(1 for usage in big_block_usage_all if usage[0])
            total_detail_blocks = len(big_block_usage_all)
            usage_rate = big_block_usage_count / total_detail_blocks * 100 if total_detail_blocks > 0 else 0
            
            print(f"    æ¡å¸¦{strip_idx}: æ€»å—æ•°{len(all_blocks)}, è‰²å—{color_count}, çº¹ç†å—{detail_count}")
            print(f"      å¤§å—ç´¢å¼•ä½¿ç”¨ç‡: {usage_rate:.1f}% ({big_block_usage_count}/{total_detail_blocks})")
    
    return gop_codebooks

class EncodingStats:
    """ç¼–ç ç»Ÿè®¡ç±»"""
    def __init__(self):
        # å¸§ç»Ÿè®¡
        self.total_frames_processed = 0  # å®é™…å¤„ç†çš„å¸§æ•°ï¼ˆæ¡å¸¦çº§åˆ«ï¼‰
        self.total_i_frames = 0
        self.forced_i_frames = 0  # å¼ºåˆ¶Iå¸§ï¼ˆGOPå¼€å§‹ï¼‰
        self.threshold_i_frames = 0  # è¶…é˜ˆå€¼Iå¸§
        self.total_p_frames = 0
        
        # å¤§å°ç»Ÿè®¡
        self.total_i_frame_bytes = 0
        self.total_p_frame_bytes = 0
        self.total_codebook_bytes = 0  # åªè®¡ç®—Iå¸§ä¸­çš„ç æœ¬æ•°æ®
        self.total_index_bytes = 0     # åªè®¡ç®—Iå¸§ä¸­çš„ç´¢å¼•æ•°æ®
        self.total_p_overhead_bytes = 0  # På¸§çš„å¼€é”€æ•°æ®ï¼ˆbitmapç­‰ï¼‰
        
        # På¸§å—æ›´æ–°ç»Ÿè®¡
        self.p_frame_updates = []  # æ¯ä¸ªPå¸§çš„æ›´æ–°å—æ•°
        self.zone_usage = defaultdict(int)  # åŒºåŸŸä½¿ç”¨æ¬¡æ•°
        
        # ç»†èŠ‚ç»Ÿè®¡
        self.color_block_bytes = 0
        self.detail_block_bytes = 0
        self.color_update_count = 0
        self.detail_update_count = 0
        
        # æ¡å¸¦ç»Ÿè®¡
        self.strip_stats = defaultdict(lambda: {
            'i_frames': 0, 'p_frames': 0, 
            'i_bytes': 0, 'p_bytes': 0
        })
    
    def add_i_frame(self, strip_idx, size_bytes, is_forced=True, codebook_size=0, index_size=0):
        self.total_frames_processed += 1
        self.total_i_frames += 1
        if is_forced:
            self.forced_i_frames += 1
        else:
            self.threshold_i_frames += 1
        
        self.total_i_frame_bytes += size_bytes
        self.total_codebook_bytes += codebook_size
        self.total_index_bytes += index_size
        
        self.strip_stats[strip_idx]['i_frames'] += 1
        self.strip_stats[strip_idx]['i_bytes'] += size_bytes
    
    def add_p_frame(self, strip_idx, size_bytes, updates_count, zone_count, 
                   color_updates=0, detail_updates=0):
        self.total_frames_processed += 1
        self.total_p_frames += 1
        self.total_p_frame_bytes += size_bytes
        self.p_frame_updates.append(updates_count)
        self.zone_usage[zone_count] += 1
        
        # På¸§å¼€é”€ï¼šå¸§ç±»å‹(1) + bitmap(1) + æ¯ä¸ªåŒºåŸŸçš„è®¡æ•°(2*zones)
        overhead = 2 + zone_count * 2  # å¤§è‡´ä¼°ç®—
        self.total_p_overhead_bytes += overhead
        
        self.color_update_count += color_updates
        self.detail_update_count += detail_updates
        
        self.strip_stats[strip_idx]['p_frames'] += 1
        self.strip_stats[strip_idx]['p_bytes'] += size_bytes
    
    def add_block_stats(self, color_bytes, detail_bytes):
        self.color_block_bytes += color_bytes
        self.detail_block_bytes += detail_bytes
    
    def print_summary(self, total_frames, total_bytes):
        print(f"\nğŸ“Š ç¼–ç ç»Ÿè®¡æŠ¥å‘Š")
        print(f"=" * 60)
        
        # è®¡ç®—æ¡å¸¦çº§åˆ«çš„ç»Ÿè®¡
        strip_count = len(self.strip_stats) if self.strip_stats else 1
        
        # åŸºæœ¬ç»Ÿè®¡
        print(f"ğŸ¬ å¸§ç»Ÿè®¡:")
        print(f"   è§†é¢‘å¸§æ•°: {total_frames}")
        print(f"   æ¡å¸¦æ€»æ•°: {strip_count}")
        print(f"   å¤„ç†çš„æ¡å¸¦å¸§: {self.total_frames_processed}")
        print(f"   Iå¸§æ¡å¸¦: {self.total_i_frames} ({self.total_i_frames/self.total_frames_processed*100:.1f}%)")
        print(f"     - å¼ºåˆ¶Iå¸§: {self.forced_i_frames}")
        print(f"     - è¶…é˜ˆå€¼Iå¸§: {self.threshold_i_frames}")
        print(f"   På¸§æ¡å¸¦: {self.total_p_frames} ({self.total_p_frames/self.total_frames_processed*100:.1f}%)")
        
        # å¤§å°ç»Ÿè®¡
        print(f"\nğŸ’¾ ç©ºé—´å ç”¨:")
        print(f"   æ€»å¤§å°: {total_bytes:,} bytes ({total_bytes/1024:.1f} KB)")
        print(f"   Iå¸§æ•°æ®: {self.total_i_frame_bytes:,} bytes ({self.total_i_frame_bytes/total_bytes*100:.1f}%)")
        print(f"   På¸§æ•°æ®: {self.total_p_frame_bytes:,} bytes ({self.total_p_frame_bytes/total_bytes*100:.1f}%)")
        
        if self.total_i_frames > 0:
            print(f"   å¹³å‡Iå¸§å¤§å°: {self.total_i_frame_bytes/self.total_i_frames:.1f} bytes")
        if self.total_p_frames > 0:
            print(f"   å¹³å‡På¸§å¤§å°: {self.total_p_frame_bytes/self.total_p_frames:.1f} bytes")
        
        # æ•°æ®æ„æˆç»Ÿè®¡ï¼ˆä¿®æ­£ï¼‰
        print(f"\nğŸ¨ æ•°æ®æ„æˆ:")
        print(f"   ç æœ¬æ•°æ®: {self.total_codebook_bytes:,} bytes ({self.total_codebook_bytes/total_bytes*100:.1f}%)")
        print(f"   Iå¸§ç´¢å¼•: {self.total_index_bytes:,} bytes ({self.total_index_bytes/total_bytes*100:.1f}%)")
        
        # På¸§æ•°æ®æ„æˆ
        p_frame_data_bytes = self.total_p_frame_bytes - self.total_p_overhead_bytes
        print(f"   På¸§æ›´æ–°æ•°æ®: {p_frame_data_bytes:,} bytes ({p_frame_data_bytes/total_bytes*100:.1f}%)")
        print(f"   På¸§å¼€é”€: {self.total_p_overhead_bytes:,} bytes ({self.total_p_overhead_bytes/total_bytes*100:.1f}%)")
        
        # å…¶ä»–æ•°æ®
        other_bytes = total_bytes - (self.total_codebook_bytes + self.total_index_bytes + self.total_p_frame_bytes)
        if other_bytes > 0:
            print(f"   å…¶ä»–æ•°æ®: {other_bytes:,} bytes ({other_bytes/total_bytes*100:.1f}%)")
        
        # å—ç±»å‹ç»Ÿè®¡
        print(f"\nğŸ§© å—ç±»å‹åˆ†å¸ƒ:")
        if self.color_block_bytes > 0 or self.detail_block_bytes > 0:
            total_block_data = self.color_block_bytes + self.detail_block_bytes
            print(f"   è‰²å—ç´¢å¼•: {self.color_block_bytes} ä¸ª ({self.color_block_bytes/total_block_data*100:.1f}%)")
            print(f"   çº¹ç†å—ç´¢å¼•: {self.detail_block_bytes} ä¸ª ({self.detail_block_bytes/total_block_data*100:.1f}%)")
        
        # På¸§æ›´æ–°ç»Ÿè®¡
        if self.p_frame_updates:
            avg_updates = statistics.mean(self.p_frame_updates)
            median_updates = statistics.median(self.p_frame_updates)
            max_updates = max(self.p_frame_updates)
            min_updates = min(self.p_frame_updates)
            
            print(f"\nâš¡ På¸§æ›´æ–°åˆ†æ:")
            print(f"   å¹³å‡æ›´æ–°å—æ•°: {avg_updates:.1f}")
            print(f"   ä¸­ä½æ•°æ›´æ–°å—æ•°: {median_updates}")
            print(f"   æœ€å¤§æ›´æ–°å—æ•°: {max_updates}")
            print(f"   æœ€å°æ›´æ–°å—æ•°: {min_updates}")
            print(f"   è‰²å—æ›´æ–°æ€»æ•°: {self.color_update_count:,}")
            print(f"   çº¹ç†å—æ›´æ–°æ€»æ•°: {self.detail_update_count:,}")
        
        # åŒºåŸŸä½¿ç”¨ç»Ÿè®¡
        if self.zone_usage:
            print(f"\nğŸ—ºï¸  åŒºåŸŸä½¿ç”¨åˆ†å¸ƒ:")
            for zone_count in sorted(self.zone_usage.keys()):
                frames_count = self.zone_usage[zone_count]
                if self.total_p_frames > 0:
                    print(f"   {zone_count}ä¸ªåŒºåŸŸ: {frames_count}æ¬¡ ({frames_count/self.total_p_frames*100:.1f}%)")
        
        # æ¡å¸¦ç»Ÿè®¡
        print(f"\nğŸ“ æ¡å¸¦ç»Ÿè®¡:")
        for strip_idx in sorted(self.strip_stats.keys()):
            stats = self.strip_stats[strip_idx]
            total_strip_frames = stats['i_frames'] + stats['p_frames']
            total_strip_bytes = stats['i_bytes'] + stats['p_bytes']
            if total_strip_frames > 0:
                print(f"   æ¡å¸¦{strip_idx}: {total_strip_frames}å¸§, {total_strip_bytes:,}bytes, "
                      f"å¹³å‡{total_strip_bytes/total_strip_frames:.1f}bytes/å¸§")
        
        # å‹ç¼©æ•ˆç‡
        raw_size = total_frames * WIDTH * HEIGHT * 2  # å‡è®¾16ä½åƒç´ 
        compression_ratio = raw_size / total_bytes if total_bytes > 0 else 0
        print(f"\nğŸ“ˆ å‹ç¼©æ•ˆç‡:")
        print(f"   åŸå§‹å¤§å°ä¼°ç®—: {raw_size:,} bytes ({raw_size/1024/1024:.1f} MB)")
        print(f"   å‹ç¼©æ¯”: {compression_ratio:.1f}:1")
        print(f"   å‹ç¼©ç‡: {(1-total_bytes/raw_size)*100:.1f}%")

# å…¨å±€ç»Ÿè®¡å¯¹è±¡
encoding_stats = EncodingStats()

def main():
    pa = argparse.ArgumentParser(description="Encode to GBA YUV9 with unified codebook")
    pa.add_argument("input")
    pa.add_argument("--duration", type=float, default=5.0)
    pa.add_argument("--full-duration", action="store_true")
    pa.add_argument("--fps", type=int, default=30)
    pa.add_argument("--out", default="video_data")
    pa.add_argument("--strip-count", type=int, default=DEFAULT_STRIP_COUNT)
    pa.add_argument("--i-frame-interval", type=int, default=60)
    pa.add_argument("--diff-threshold", type=float, default=2.0)
    pa.add_argument("--force-i-threshold", type=float, default=0.7)
    pa.add_argument("--variance-threshold", type=float, default=5.0,
                   help="æ–¹å·®é˜ˆå€¼ï¼Œç”¨äºåŒºåˆ†çº¯è‰²å—å’Œçº¹ç†å—ï¼ˆé»˜è®¤5.0ï¼‰")
    pa.add_argument("--codebook-size", type=int, default=DEFAULT_UNIFIED_CODEBOOK_SIZE,
                   help=f"ç»Ÿä¸€ç æœ¬å¤§å°ï¼ˆé»˜è®¤{DEFAULT_UNIFIED_CODEBOOK_SIZE}ï¼‰")
    pa.add_argument("--kmeans-max-iter", type=int, default=200)
    pa.add_argument("--threads", type=int, default=None)
    # æ·»åŠ å¤§å—ç´¢å¼•ç›¸å…³å‚æ•°
    pa.add_argument("--big-block-error-threshold", type=float, default=10.0,
                   help="å¤§å—ç´¢å¼•è¯¯å·®é˜ˆå€¼ï¼ˆé»˜è®¤10.0ï¼‰")
    args = pa.parse_args()

    cap = cv2.VideoCapture(args.input)
    if not cap.isOpened():
        raise SystemExit("âŒ æ‰“ä¸å¼€è¾“å…¥æ–‡ä»¶")

    src_fps = cap.get(cv2.CAP_PROP_FPS) or 30
    every = int(round(src_fps / args.fps))
    
    if args.full_duration:
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        grab_max = total_frames
        actual_duration = total_frames / src_fps
        print(f"ç¼–ç æ•´ä¸ªè§†é¢‘: {total_frames} å¸§ï¼Œæ—¶é•¿ {actual_duration:.2f} ç§’")
    else:
        grab_max = int(args.duration * src_fps)
        print(f"ç¼–ç æ—¶é•¿: {args.duration} ç§’ ({grab_max} å¸§)")

    strip_heights = calculate_strip_heights(HEIGHT, args.strip_count)
    print(f"æ¡å¸¦é…ç½®: {args.strip_count} ä¸ªæ¡å¸¦ï¼Œé«˜åº¦åˆ†åˆ«ä¸º: {strip_heights}")
    print(f"ç æœ¬é…ç½®: ç»Ÿä¸€ç æœ¬{args.codebook_size}é¡¹")

    frames = []
    idx = 0
    print("æ­£åœ¨æå–å¸§...")
    with concurrent.futures.ThreadPoolExecutor(max_workers=args.threads) as executor:
        while idx < grab_max:
            ret, frm = cap.read()
            if not ret:
                break
            if idx % every == 0:
                frm = cv2.resize(frm, (WIDTH, HEIGHT), cv2.INTER_AREA)
                strip_y_list = []
                y = 0
                for strip_height in strip_heights:
                    strip_y_list.append((frm, y, strip_height))
                    y += strip_height
                future_to_idx = {
                    executor.submit(pack_yuv420_strip, *args): i
                    for i, args in enumerate(strip_y_list)
                }
                frame_strips = [None] * len(strip_y_list)
                for future in concurrent.futures.as_completed(future_to_idx):
                    i = future_to_idx[future]
                    frame_strips[i] = future.result()
                frames.append(frame_strips)
                
                if len(frames) % 30 == 0:
                    print(f"  å·²æå– {len(frames)} å¸§")
            idx += 1
    cap.release()

    if not frames:
        raise SystemExit("âŒ æ²¡æœ‰ä»»ä½•å¸§è¢«é‡‡æ ·")

    print(f"æ€»å…±æå–äº† {len(frames)} å¸§")

    # ç”Ÿæˆç»Ÿä¸€ç æœ¬å’Œå¤§å—ç´¢å¼•ç è¡¨
    gop_codebooks = generate_gop_unified_codebooks_with_big_block(
        frames, args.strip_count, args.i_frame_interval, 
        args.variance_threshold, args.codebook_size, 
        args.big_block_error_threshold, args.kmeans_max_iter
    )

    # ç¼–ç æ‰€æœ‰å¸§
    print("æ­£åœ¨ç¼–ç å¸§...")
    encoded_frames = []
    frame_offsets = []
    current_offset = 0
    prev_strips = [None] * args.strip_count
    
    for frame_idx, current_strips in enumerate(frames):
        frame_offsets.append(current_offset)
        
        # æ‰¾åˆ°å½“å‰GOP
        gop_start = (frame_idx // args.i_frame_interval) * args.i_frame_interval
        gop_data = gop_codebooks[gop_start]
        
        frame_data = bytearray()
        
        for strip_idx, current_strip in enumerate(current_strips):
            strip_gop_data = gop_data[strip_idx]
            unified_codebook = strip_gop_data['unified_codebook']
            big_block_codebook = strip_gop_data['big_block_codebook']
            
            # æ‰¾åˆ°å½“å‰å¸§çš„block_typeså’Œbig_block_usage
            block_types = None
            big_block_usage = []
            for fid, bt in strip_gop_data['block_types_list']:
                if fid == frame_idx:
                    block_types = bt
                    big_block_usage = strip_gop_data['big_block_usage'].get(frame_idx, [])
                    break
            
            force_i_frame = (frame_idx % args.i_frame_interval == 0) or frame_idx == 0
            
            if force_i_frame or prev_strips[strip_idx] is None:
                strip_data = encode_strip_i_frame_unified_with_big_block(
                    current_strip, unified_codebook, big_block_codebook, block_types, big_block_usage
                )
                is_i_frame = True
                
                # è®¡ç®—ç æœ¬å’Œç´¢å¼•å¤§å°ï¼ˆåŒ…æ‹¬å¤§å—ç´¢å¼•è¡¨ï¼‰
                codebook_size = args.codebook_size * BYTES_PER_BLOCK + BIG_BLOCK_CODEBOOK_SIZE * 4
                index_size = len(strip_data) - 1 - codebook_size
                
                encoding_stats.add_i_frame(
                    strip_idx, len(strip_data), 
                    is_forced=force_i_frame,
                    codebook_size=codebook_size,
                    index_size=max(0, index_size)
                )
            else:
                strip_data, is_i_frame, used_zones, color_updates, detail_updates = encode_strip_differential_unified_with_big_block(
                    current_strip, prev_strips[strip_idx],
                    unified_codebook, big_block_codebook, block_types, big_block_usage,
                    args.diff_threshold, args.force_i_threshold
                )
                
                if is_i_frame:
                    codebook_size = args.codebook_size * BYTES_PER_BLOCK + BIG_BLOCK_CODEBOOK_SIZE * 4
                    index_size = len(strip_data) - 1 - codebook_size
                    
                    encoding_stats.add_i_frame(
                        strip_idx, len(strip_data), 
                        is_forced=False,
                        codebook_size=codebook_size,
                        index_size=max(0, index_size)
                    )
                else:
                    total_updates = color_updates + detail_updates
                    
                    encoding_stats.add_p_frame(
                        strip_idx, len(strip_data), total_updates, used_zones,
                        color_updates, detail_updates
                    )
            
            frame_data.extend(struct.pack('<H', len(strip_data)))
            frame_data.extend(strip_data)
            
            prev_strips[strip_idx] = current_strip.copy() if current_strip.size > 0 else None
        
        encoded_frames.append(bytes(frame_data))
        current_offset += len(frame_data)
        
        if frame_idx % 30 == 0 or frame_idx == len(frames) - 1:
            print(f"  å·²ç¼–ç  {frame_idx + 1}/{len(frames)} å¸§")
    
    all_data = b''.join(encoded_frames)
    
    write_header(pathlib.Path(args.out).with_suffix(".h"), len(frames), len(all_data), 
                args.strip_count, strip_heights, args.codebook_size)
    write_source(pathlib.Path(args.out).with_suffix(".c"), all_data, frame_offsets, strip_heights)
    
    # æ‰“å°è¯¦ç»†ç»Ÿè®¡
    encoding_stats.print_summary(len(frames), len(all_data))

def write_header(path_h: pathlib.Path, frame_cnt: int, total_bytes: int, strip_count: int, 
                strip_heights: list, codebook_size: int):
    guard = "VIDEO_DATA_H"
    
    with path_h.open("w", encoding="utf-8") as f:
        f.write(textwrap.dedent(f"""\
            #ifndef {guard}
            #define {guard}

            #define VIDEO_FRAME_COUNT   {frame_cnt}
            #define VIDEO_WIDTH         {WIDTH}
            #define VIDEO_HEIGHT        {HEIGHT}
            #define VIDEO_TOTAL_BYTES   {total_bytes}
            #define VIDEO_STRIP_COUNT   {strip_count}
            #define UNIFIED_CODEBOOK_SIZE {codebook_size}
            #define EFFECTIVE_UNIFIED_CODEBOOK_SIZE {EFFECTIVE_UNIFIED_CODEBOOK_SIZE}
            #define BIG_BLOCK_CODEBOOK_SIZE {BIG_BLOCK_CODEBOOK_SIZE}
            #define EFFECTIVE_BIG_BLOCK_CODEBOOK_SIZE {EFFECTIVE_BIG_BLOCK_CODEBOOK_SIZE}
            
            // å¸§ç±»å‹å®šä¹‰
            #define FRAME_TYPE_I        0x00
            #define FRAME_TYPE_P        0x01
            
            // ç‰¹æ®Šæ ‡è®°
            #define COLOR_BLOCK_MARKER  0xFF
            #define COMPLEX_TEXTURE_MARKER 0xFE
            
            // å—å‚æ•°
            #define BLOCK_WIDTH         2
            #define BLOCK_HEIGHT        2
            #define BYTES_PER_BLOCK     7

            // æ¡å¸¦é«˜åº¦æ•°ç»„
            extern const unsigned char strip_heights[VIDEO_STRIP_COUNT];
            
            extern const unsigned char video_data[VIDEO_TOTAL_BYTES];
            extern const unsigned int frame_offsets[VIDEO_FRAME_COUNT];

            #endif // {guard}
            """))

def write_source(path_c: pathlib.Path, data: bytes, frame_offsets: list, strip_heights: list):
    with path_c.open("w", encoding="utf-8") as f:
        f.write('#include "video_data.h"\n\n')
        
        f.write("const unsigned char strip_heights[] = {\n")
        f.write("    " + ', '.join(map(str, strip_heights)) + "\n")
        f.write("};\n\n")
        
        f.write("const unsigned int frame_offsets[] = {\n")
        for i in range(0, len(frame_offsets), 8):
            chunk = ', '.join(f"{offset}" for offset in frame_offsets[i:i+8])
            f.write("    " + chunk + ",\n")
        f.write("};\n\n")
        
        f.write("const unsigned char video_data[] = {\n")
        per_line = 16
        for i in range(0, len(data), per_line):
            chunk = ', '.join(f"0x{v:02X}" for v in data[i:i+per_line])
            f.write("    " + chunk + ",\n")
        f.write("};\n")

if __name__ == "__main__":
    main()